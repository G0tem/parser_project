{
  "GIMP Script-Fu Первый Дан. Первое приветствие": "Наш первый Hello World в GIMP!Наша цель нарисовать приветствие на изображении в GIMP. Но для того что бы вывести что то на изображение, это изображение у нас должно быть. Мы можем создать его в ручную. Просто средствами GIMP, или же создать его прямо из плагина Script-fu.Научим плагин создавать изображения, определив функцию создания изображений. Что надо знать, так это то что изображение состоит из слоев - layer, в нашем новом изображении будет изначально только один слой. Изображение будет создано и заполнено ТЕКУЩИМ цветом фона, установленного в Гимп, но мы могли бы вполне себе и задать цвет фона создаваемого изображения.(define (create-1-layer-img w h)\n   (let* ((i1  (car (gimp-image-new w h RGB)))\n         (l1  (car (gimp-layer-new i1 w h\n                                RGB \"layer 1\" 100 LAYER-MODE-NORMAL-LEGACY))))\n      (gimp-image-undo-disable i1)\n      (gimp-image-add-layer i1 l1 0)\n      ;;(gimp-palette-set-background '(0 127 50))\n      (gimp-edit-fill l1 FILL-BACKGROUND)\n      (gimp-display-new i1)\n      (gimp-image-undo-enable  i1)\n      i1))\nДа забыл сказать, этот код подходит для версии 2.10. А GIMP хотя и медленно, но постоянно меняется, и я работал в двух версиях, так что можете сравнить  код для версии 2.6:(define (create-1-layer-img w h)\n   (let* ((i1  (car (gimp-image-new w h RGB)))\n         (l1  (car (gimp-layer-new i1 w h\n                                RGB \"layer 1\" 100 NORMAL))))\n      (gimp-image-undo-disable i1)\n      (gimp-image-add-layer i1 l1 0)\n      ;;(gimp-palette-set-background '(0 127 50))\n      (gimp-edit-fill l1 BG-IMAGE-FILL)\n      (gimp-display-new i1)\n      (gimp-image-undo-enable  i1)\n      i1))Изменения - косметические, в именах констант, но именно такие вот мелкие изменения могут делать код не работоспособным при работе его в различных версиях гимпа.Используем эту функцию и создадим изображение:(define img (create-1-layer-img 640 480))\n;;можно получить img или ошибку:\n;;Error: eval: unbound variable: LAYER-MODE-NORMAL-LEGACY\n;;если применить код для версии 2.10 в gimp версии 2.6\nВ переменной img будет храниться handle(хендл), что можно перевести как ручка, нечто за что можно держаться и управлять этим объектом, представляющий собой некий числовой идентификатор, передавая который обратно в гимп с помощью функций гимп, мы можем работать с внутренним объектом гимпа. Типы этих объектов могут быть совершенно разными: изображение, слой или какой либо другой тип.Так же мы можем создать изображение с помощью средств гимпа или даже загрузить его из файла. Но для того чтобы работать с ним нам надо знать хендл этого изображения. Получить список всех изображений можно с помощью функции:(gimp-image-list)\n;;(1 #(1))\n\n(define img 1)\n;;img\nи просто присвоить известный числовой идентификатор переменной img и так же работать с ним как с хендлом.Далее нам надо определится с тем каким шрифтом выводить текст, список всех известных гимпу шрифтов, можно получить с помощью функции:;;посмотреть список фонтов присутствующих в системе\n(gimp-fonts-get-list \".*\")\n;;очень длинный вывод\n(gimp-fonts-get-list \"Sans\")\n;;у меня список из 218 элементов в Линукс и 23 в Видовс.\nфункция ищет по шаблону имени фонта и выдает все фонты имена которых соответствуют  шаблону, регулярному выражению. Из этого списка мы можем выбрать фонт, которым будем рисовать наш текст.(gimp-text-layer-new img \"Привет Мир!\" \"DejaVu Sans\" 1 1)\n;;(3)\nВведя эту команду в консоль скрипт-фу, мы к сожалению не увидим добавленного текста, но она вернет опять таки некий хендл(как список), созданного текстового слоя. Используя его можно получить некоторую информацию о созданном текстовом слое. К сожалению, до тех пор пока слой не будет добавлен в изображение, все функции работы с текстовым слоем выдают ошибку. Поэтому слой с текстом будем добавлять с помощью функции:(define (insert-text1 i str font size unit)\n  (let ((l (car (gimp-text-layer-new i str font size unit))))\n    (gimp-image-add-layer i l 0)\n\tl))\n\t\n(insert-text1 img \"Привет Мир!\" \"DejaVu Sans\" 1 1)\n;;4\nНе смотря на единички, а вернее благодаря последней единице(обозначающей вывод текста в неких юнитах), в линукс текст получился гигантским, в виндовс - вполне нормальным. Но мы попробуем второй заход. И да, если изображение \"подпорчено\", неудачной функцией в ГИМПе можно легко откатиться на предыдущую версию изображения воспользовавшись \"Историей действий\", это такой список изображений, выбрав то изображение которое вы считаете нормальным, у нас это пока единственное пустое изображение, выберем его.(insert-text1 img \"Привет Мир!\" \"DejaVu Sans\" 100 0)\n;;6\nПоследний 0 в вызове функции обозначает, размер будет указан в пикселах, а 100 обозначает условную высоту текста. Полученный слой приблизительно соответствует этой величине. \"Привет Мир!\" Из текстового слоя можно получить не мало интересной информации, помните хендл полученный нами для данного слоя 6, и именно его мы используем в указанных ниже функциях, если у вас хендл другой используйте его: (gimp-text-layer-get-text 6)\n;;(\"Привет Мир!\")\n(gimp-text-layer-get-language 6)\n;;(\"ru_RU\")\n(gimp-text-layer-get-font-size 6)\n;;(100.0 0)\n(gimp-text-layer-get-font 6)\n;;(\"DejaVu Sans\")\n(gimp-text-layer-get-base-direction 6)\n;;(0)\n(gimp-drawable-height 6)\n(gimp-drawable-width  6)\n;;(117)(668)\nРезультаты могут отличаться, например потому что в Виндовсе у меня нет такого фонта, но система не ругается, а молча пытается подобрать какой то фонт, близкий к заданному.Но что не менее важно, можно эту информацию и установить, т.е изменить сам текстовый слой.(gimp-text-layer-set-color 6 '(255 255 0))\n(#t)\n(gimp-text-layer-set-font 6  \"URW Gothic Oblique\")\n(gimp-text-layer-set-font 6   \"Terminus\")\n(#t)\n(gimp-text-layer-set-text 6   \"Хай, Бро!\\nКак живёшь?\")\n(#t)\n(gimp-text-layer-set-base-direction 6  4);;в 2.6 направления 4 нет, есть 0 или 1.\n(#t)\nИ вот результат:\"Докатились!\"Таким образом видно, Гимп предлагает очень богатый набор функций по выводу текста на изображение. И что не менее важно, слой создаваемый функцией gimp-text-layer-new является не просто картой пикселей, но объектом, динамически изменяющим свой вид, в зависимости от устанавливаемых для него параметров.Как запомнить результаты нашей работы? Надо объединить слои в один и дать ГИМПу команду сохранения:(gimp-image-flatten img)\n;;(7)\n(gimp-file-save 1 img (car (gimp-image-active-drawable img))\n                \"D:\\\\WORK\\\\gimp\\\\test-hello1.png\" \"test-hello1.png\" ) \n;;(#t)Путь указан для Виндовса, для Линкуса всё тоже самое, но с обычным слешем, конечно же без диска. И да, почему я всё время CARкаю? Потому что многие процедуры, типа gimp-image-active-drawable возвращают список, хотя казалось бы, почему список? активный слой может быть только один, но она возвращает список, а мы берём из него 1 элемент. Поэтому CAR.И на этом пока всё.",
  "А так ли важно планирование поставок в ритейле?": "Зачастую среди топ-менджмента или миддл-менджмента в ритейле бытует мнение, что система управления запасами — это задача вторичной важности. Есть более приоритетные задачи такие как обеспечение работы склада или процесс управления ценами. Либо же существует представление, что самое сложное это прогноз, а управление запасами это просто задача в стиле решения простой формулы A + B - C = X. В этой статье я попробую объяснить, почему это не всегда верное мнение и зачастую эта задача должна стоять первой.Кто ты, мальчик?В ритейл пришел в 2003 году в роли сотрудника рабочего зала, это простые процессы выкладка, смена ценников, консультации клиентов и работа с кассой. После этого начал заниматься оптом и госзаказами. Дальше карьерные амбиции меня привели к смене города проживания до регионального центра и пришлось идти на склад ритейлера простым кладовщиком. Я поучаствовал во всех типовых процессах склада в роли исполнителя и линейного менеджера. На этом мой опыт в области операционных задач закончился и начался опыт офисный.В рамках роста по карьерной лестнице меня ждал переход на роль продакт-менеджера, это была новая, на тот момент для рынка, профессия аналогичная категорийному менеджеру, который «и швец, и жнец ….», в том смысле, что в нем концентрировался весь набор задач по управлению продажами начиная от формирования матрицы, обеспечения поставок и заканчивая ценообразованием и промо мероприятиями. Объективно говоря это не самый эффективный способ управления продажами в розничном бизнесе, менеджер перегружен задачами и со всеми справляется не до конца эффективно даже на небольшом масштабе. Это я понял, когда позже перешел уже на «обычного» категорийного менеджера в другой компании, в круг задач которого входило ограниченное количество обязанностей, например ценообразование или управление логистикой было отделено от этой функции. Следующей ступенькой стал рост до начальника аналитической группы по анализу эффективности основного бизнеса, куда вошли задачи по анализу качества управления ассортиментом, эффективности логистики и ценообразования.Это привело меня к первому проектному опыту – разработке собственной системы ценообразования. Так начался мой путь движения от фронт-офиса к бэк-офису, где моя команда не принимает конечных бизнес-решений, но разрабатывает инструменты для их корректного принятия. Это был небольшой проект продолжительностью один год. По сути, концентрировался на простых задачах – создать «иллюзию низких цен» у наших покупателей, выполнить цели по марже, при этом не наращивая нагрузку на операционный блок (сотрудники зала могут поменять только ограниченное количество ценников в день).Следующий проект – это разработка собственной системы управления ассортиментом конечных магазинов уже в другой компании. Бизнес, в котором я работал в тот момент, насчитывал несколько тысяч торговых объектов разного размера, местоположения и окружения. К тому же этот бизнес быстро рос, а значит открывал 1-2 тысячи торговых объектов в год. Надо было создать умный алгоритм, позволяющий категорийному менеджеру управлять общими принципами образования матрицы товаров, а мелкие детали отдать системе. Начали мы с организации мастер данных, в масштабах компании это был крупный проект самостоятельной эффективности, но в рамках разработки решения по управлению ассортиментом это был первый шаг перед основными работами. Задачи операционного бизнеса в этом проекте тоже имели решающее значение. Система ежедневно высчитывает оптимальный ассортимент в каждом магазине, но логистика имеет инерцию, категорийные менеджеры работают над общей эффективностью матрицы, а сотрудники торгового зала вынуждены менять выкладку под каждую ротацию. Данный проект пытался подстроить процессы других служб под себя, но если процессы коммерческих служб легко принимали изменения, то логистика представляла собой очень сложный и негибкий процесс. На данном проекте продолжился мой карьерный рост на пару-тройку грейдов и в определенный момент времени в мое управление попала вся корпоративная аналитика, включая финансовую и управленческую. Под моим управлением команда аналитического блока систематизировала всю модель данных компании, для чего постепенно создавала корпоративное хранилище данных и впоследствии создала свой самостоятельный бизнес юнит. Выходцы из него сейчас работают на ключевых позициях в телекоме или финансовом секторе – там, где важны точные и полные данные в короткий период времени, но это отдельная история.Возвращаясь к истории с проектом по управлению ассортиментом – ключевым препятствием в разные отрезки времени становилась логистика, а менять ее всегда было сложнее чем любой другой процесс в компании. Со стороны казалось, что это управленческое противодействие или легкая диверсия против основного бизнеса, ведь что важнее чем определить какой ассортимент нужен покупателю. На деле же, и это стало понятно гораздо позже, задача логистики важнее всего в ритейловом бизнесе и по своей ценности превосходит даже такие операционные моменты как чистый пол в торговом зале.По успешному завершению проекта по управлению ассортиментом мою команду направили повышать эффективность работы логистики. В общем смысле мы должны были разработать такие системы, которые с минимальными затратами позволили бы обеспечить два простых принципа – необходимое и достаточное количество товара в торговом зале. Эти принципы я пытался выполнить в разное время в 5 компаниях, занимая разный уровень должностей от CEO минус три, до CEO минус один и работая с разными видами бизнеса, начиная от продуктов питания или бытовой электроники и заканчивая элитным нижним бельем, в компаниях с выручкой 200 млн долларов США в год и компаниях с выручкой 30 млрд долларов США в год. В моем управлении была как проектная деятельность по развитию и внедрению систем, так и вся операционная деятельность. В рамках разных конференций мне приходилось общаться с коллегами из других бизнесов или принимать к себе в штат людей из других областей, которые рассказывали о сложностях и задачах на их прежнем месте работы. Несмотря на разные цели, инструменты и методы реализации неизменность двух базовых принципов подтверждалась в каждой компании.Коротко про ритейл.Любой ритейловый бизнес характеризуется тем, что представляет собой разнесенную на большой площади «сборную солянку» из разных по площади и конфигурации торговых объектов, сложную сеть собственных складов, порой существенно отличающихся друг от друга, а также иерархию поставщиков, с не менее сложной сетью площадок отгрузки и, что более важно, со своим набором требований к клиентам. Я применил слово «иерархия» к поставщикам, потому что в мире бизнеса есть те, кто крупнее тебя, то есть ты зависишь от них и тебе надо считаться с их условиями и те, кто мельче тебя, иными словами, зависимы от тебя, то есть те, кому ежедневно ваш топ-менеджмент рассказывает о партнерстве, но в контексте «будьте добры, пересмотрите свои условия, иначе такие партнеры нам не партнеры».Задача логистики – обеспечить товар в магазине, то есть внутри себя понимать всю структуру бизнеса с ее особенностями и сделать так, чтобы несмотря на всю проблематику «физической реальности» гарантировать наличие товара. Это простая задача если мы просто пытаемся перевезти товар из точки А в точку Б. Всего-навсего нужно учесть, что в точке Б есть некая потребность для продажи. Предсказываем эту потребность и везем ее из точки А. Казалось бы, какие сложности могут быть, ну, вероятно, самое узкое место в такой системе – предсказать сколько нужно. Однако для многих компаний и это не составляет труда.Выше я уже говорил, что бизнесы отличаются между собой принципами и методами. Если объединить их в две большие группы, то существуют бизнесы:про-активные – там, где у покупателя есть потребность, но спрос не сформулирован. Ну, например, вам нужна зимняя куртка, обычная серая базовая, но будет это куртка от производителя «ИП Иванов» или «ООО «Хорошие куртки» для вас не имеет значения. Главное это наличие, приемлемое качество и цена. Компании ведущие такой бизнес управляют спросом опираясь на вашу потребность. То есть они привозят вам коллекцию курток от ИП Иванова по той цене, что смогли купить (подразумеваем, что она лучшая) сделали адекватную наценку оглядываясь на конкурентное окружение и вуаля, у вас есть спрос на куртку от Иванова. Дальнейшее управление спросом выражается в том, что компания «играет» ценой в течении сезона, меняет выкладку, дарит куртку в подарок к ботинкам и делает в общем-то все, чтобы привезенная под сезон партия была распродана к его концу, а цели по маржинальности этой партии привели к выполнению показателей компании.ре-активные компании живут на рынке, где бренд уже сам управляет рынком формируя именно спрос, а не потребность. То есть вам нужен не просто растворимый кофе, а кофе марки «Якобс», и вы не согласны на «Нескафе». Такие компании просто предсказывают сколько «Якобса» они смогут продать на базе прошлой статистики, ищут лучшую цену на этот продукт, оптимальную стоимость логистики и везут его. Они тоже играют в управление спросом, но значительно меньше. Потому, что и Якобс, и Нескафе всегда есть на рынке, не нужно мыслить партиями или их распродажами. Во главе угла всегда ассортимент, на который уже есть спрос.Важно сказать, что кристально выраженной компании одного типа практически не бывает, если это не моно-брендовый бутик. Я работал в бизнесе, где было 5% про-активного управления и 95% ре-активного, так и в компаниях где доля про-актива достигала – 85%. Глобально и про-активной, и ре-активной компании нужен прогноз. Оба вида компаний не хотят потерять продажи ни за сезон целиком, ни в конкретный день. Но и первая, и вторая компания могут позволить себе использовать простой метод пополнения – продал 2 куртки, привез 2 куртки. Продал две банки кофе, привез две банки. В этом случае ценность прогноза очень низка, угадать сколько курток или банок вы точно не продадите за один цикл поставки нужно всего разово, то есть, по сути, определить потолок. Допустим ваш потолок 10 курток, вы привозите 11-12-15 и дальше вообще не думаете о прогнозе их продаж.На деле методы управления запасами в этих компаниях отличались, но об этом позже. Ключевая мысль данного раздела заключается в том, что на простой модели «везем некоторый объем из точки А в точку В» нет никаких сложностей, в том числе и прогноз спроса не является проблемой.Небольшое отступление про прогнозЗачастую непосвященному человеку кажется, что прогнозирование спроса — это очень сложная задача, покрытая таинством предвиденья будущего, вплоть до хрустальных шаров и… ну вы поняли. Действительно прогноз высокой достоверности — это очень сложный алгоритм, учитывающий десятки или сотни факторов и событий, которые могут произойти и повлиять на будущее. Особенно усложняется этот алгоритм если вы пытаетесь предсказать даже самые мелкие детали – какой товар в каком количестве продастся в конкретный день в конкретном магазине, а еще и какого цвета, фасона или размера он будет. Системы, с которыми я имел дело, в пике учитывали более 200 факторов, включая вкус продукта, наличие конкурентов в 500-метровой зоне магазина или прогноз погоды в конкретном регионе.Однако, действительно ли такая достоверность прогноза важна? Начнем с простого пояснения – прогнозом является любая величина, которую вы себе «придумали» на завтра, будь то сто или один. Самым примитивным методом прогнозирования является метод – «завтра будет как вчера». То есть прогноз является простым транслированием вчерашних событий «в завтра», продали 5, значит и завтра будет 5. На практике такой прогноз имеет очень высокую достоверность, в ряде случаев он будет достигать точности в 80% и более, особенно на высоко-оборачиваемых товарах. Но если брать в среднем, это прогноз с точностью около 50%. Посвященный в прогноз меня поправит, что в оценке прогноза учитывается не точность, а ошибка, но мы упростим этот момент для понимания большинством читателей.Если в этот алгоритм добавить логику усреднения продаж за последние 5-10-30 дней (так называемая скользящая средняя, SMA), то мы уже получаем некое подобие тренда и к точности сразу прибавляется 10%, а если к этому добавить регулярную сезонность мы выйдем на среднюю сопоставимую с 65% точности. Большинство компаний останавливаются на 3-5 факторах, таких как годовая сезонность, недельная динамика, периоды отсутствия товара, цена и промо. Некоторые же идут дальше.Причина остановки на 3-5 факторах заключается в следующем: улучшение алгоритма прогнозирования и увеличение его точности — это затраты в его разработку, развитие и поддержку. И, как в любой массивной задаче, тут действует правило Парето, 20% усилий для 80% результата и наоборот, оставшиеся 20% результата стоят в 4 раза дороже. Чтобы оправдать такой мультипликатор усилий в прогноз высокой точности, надо понимать каким образом они будут окупаться. То есть нужна модель монетизации прогноза. Здесь становится понятно, что монетизировать прогноз можно только через рост представленности товара в магазине. Основные бизнес-риски неверного прогноза это вероятность опустошить магазин в середине между поставками (когда логистика еще не успевает довезти дефицит) и потерять продажи до следующей поставки. Если мы исходим из точности прогноза в 65 процентов, то, гипотетически, вы можете потерять 35% от выручки. А точно ли это так?Прогнозирование не единственная система в цепочке, влияющей на запас в торговой точке. В обычной торговой точке есть демонстрационный/презентационный остаток. Система пополнения закладывает страховые запасы. Могут включаться процессы и системы, балансирующие поток товаров в магазины и также формировать некие избыточные запасы, чтобы пройти некие проблемные периоды. Например, специалисты вручную вмешиваются в управление запасом перед волатильными периодами и формируют дополнительный сток для перекрытия рисков. В конце концов цепь поставок чаще всего возит товар не штучно, а некими минимальными квантами (упаковками) и на полке всегда образуется избыток, связанный с округлением заказа до кванта. Мой практический опыт показывает, что влияние ошибки прогноза на продажи всегда находился в пределах 2-4% доступности товара на полке и влиял менее чем на 0,5-1,0% продаж. Далеко не всегда экономический эффект от этих продаж перекрывает затраты на проект по развитию системы. Но это зависит от каждой компании индивидуально, потому что у нее уже есть своя статистическая база. Самый важный шаг перед тем, как инвестировать в точность прогноза оценить монетизацию и весь финансовый кейс в целом. То есть использовать рациональный подход к необходимости развивать это направление.Также важно отметить, что операционные проблемы, такие как отсутствие товара у поставщика или ошибки товарного учета, всегда влияли на представленность в 2-3 раза больше, чем проблемы неверного прогноза.Пример реального влияния ошибки прогноза на представленность в крупной компанииПример реального влияния ошибки прогноза на представленность в крупной компанииГлавная мысль данного раздела заключается в том, что непонимание масштаба проблемы и ее влияния на операционный бизнес в совокупности с заблуждениями относительно таинственного могущества прогноза зачастую формируют у топ-менеджеров иллюзию того, что будет «идеальный прогноз» и проблемы устранятся. Обязательной необходимостью является, перед стартом данной инициативы, собрать факторную аналитику доступности товара на полке и оценки общего товарного запаса. На первых порах это сложный отчет, собрать который потребует большого количества усилий продолжительностью от одной недели до десяти, но разработка и замена систем — это месяцы и годы, поэтому на этом фоне усилия всегда себя оправдывают. Он позволит топ-менеджеру избавиться от иллюзий и четко понимать точку приложения усилий. А если этот отчет поставить «на поток» с регулярным обновлением, то топ-менеджер может наблюдать за результатами своих усилий в реальном времени. Если вы не СЕО и ваша зона ответственности это не «выручка за год» или «LFL YoY», то только путем факторного анализа можно выявить, где именно вы повлияли на общий успех компании (ну или наоборот). Тем не менее прогноз — это отдельная и интересная тема, потому что есть кейсы, когда прогноз окупает себя и на 1% роста продаж за счет очень больших цифр, об этом я подробнее пишу в другой статье.Возвращаясь к темеВесь предыдущий раздел был посвящен значимости прогноза, но мы все еще говорим о том, какая система является ключевой в ритейле. И тут будет уместна аналогия, которую я часто привожу, давая пояснение к этой теме. Если компания — это единый организм, то вся инфраструктура вашей компании это составные части этого организма. То есть органы, которые надо снабдить жизненно важными элементами для их существования. Система, которая делает эту работу в человеческом организме это кровеносная система. Представьте себе, что ваша сложная логистическая сеть ритейловой компании — это кровеносная система вашего бизнеса. Используя эту аналогию, становится понятно, почему иногда возникают проблемы снабжения отдаленных конечностей вашей структуры, что происходит, когда возникают проблемы с сердцем и так далее.Сеть поставок как кровеносная система, а товародвижение как сердцеТак вот система планирования запасов и поставок это то, что заставляет ваше сердце биться. Именно заявка, которую создал человек или система на перемещение товара из точки А в точку В является сердечным тактом, который впоследствии приведет к снабжению необходимыми для выживания элементами любой конечный орган. Это нисколько не умаляет значимость работы каждого отдельного органа, но именно логистика и системы, инициирующие «сердечные такты», являются самыми критичными системами жизнеобеспечения. (Тут, конечно, будут уместны замечания про значимость мозга, но не хотелось бы уходить в этические дискуссии относительно того, что есть примеры жизни организмов и без его участия)Кстати, система планирования поставок и есть та самая система, которая инициирует «сердечный такт». Если в вашей компании это делает, например, автозаказ, то именно ежедневная генерация заявок на движение товаров, а также их последующее исполнение и есть процесс, вокруг которого строится весь бизнес. Ритейл живет пока внутри вашей сети движутся товары. «Качество жизни» вашего бизнеса является отражением качества движения товаров внутри вашей логистической инфраструктуры. Резюмируя, «основной бизнес» ритейла это движение товаров.Прочти я эту мысль лет 10-15 назад, когда только начинал работать в этом направлении, то смог бы придумать десятки аргументов, почему есть что-то более важное, ну давайте пофантазируем.«Хорошо, привез ты товар, но поставь на него высокую цену – и нет продаж», заявит специалист по управлению ценами. Это безусловно так, неразумное ценообразование может «убить» продажи. Если ваша себестоимость 100 при входе товара в магазин, ваши конкуренты продают с наценкой 30% за 130, а вы заявляете цену на полке в 200 это конечно губительно для любого бизнеса. Но мы исходим из ситуации «при прочих равных», когда и вы, и ваш конкурент разумны. И вот тут ваша система логистики дает вам преимущество. Сколько вы потратили на транзит товара определяет именно она и именно это дает вам на входе в магазин 100 или 90, или 110. И если ваш вход 130, а у конкурента 130 уже «на полке», ваша система ценообразования уже не будет справляться в войне с рынком.«Прежде чем везти товар, надо управлять ассортиментом – толку от перевозки никому не нужных товаров» заявит менеджер по управлению ассортиментом и взаимоотношениями с поставщиком. Это тоже верный тезис, но отчасти. Качественные системы планирования дадут вам нулевой план по товарам, не имеющим спроса. Даже если первая поставка будет совершена на неверных предпосылках, следующие поставки не состоятся если товар не востребован. Ваш магазин, конечно, пуст, но система снабжения не генерирует вам неверные движения. Но если тот же менеджер по управлению ассортиментом ввел востребованный товар, то система поставок сфокусирует все свои усилия именно на этом товаре. Таким образом система сама сгенерирует поток по тому товару, что позволит бизнесу развиваться.«Вежливый кассир и красиво выставленный на полку товар, вкупе с освещенным торговым залом и чистым полом – вот залог успеха» заявит ответственный за операционный бизнес. Здесь не буду спекулировать на тему, «а как же магазины без персонала, которые массово открываются сейчас в разных уголках мира?» или «а как же рыбные рынки восточной Азии где нет ничего из этого?». Скажу, возможно грубо и просто, но без цели кого-то задеть: уборщица необходимый элемент всех существующих «в кирпиче и бетоне» бизнесов, но вы же не будете заявлять, что уборщица — это бизнес-образующий элемент? Да, значим, как и любой другой элемент системы, но в ритейле продавец продает, если есть что продавать.Вероятно, вы сможете придумать более дельный аргумент как финансист, HR или IT специалист. Но еще раз подчеркну, у меня нет цели сказать, что в ритейле есть неважные элементы, также как и в организме неважные органы. Лишь прошу обратить внимание на понятие «основной бизнес». То есть чем вы занимаетесь, уместив это в одну фразу? Моя мысль такова:Вы доставляете товар к тому месту, где покупатель может его купить.Если бы вас не было, покупателю пришлось бы самого себя доставлять к месту производства, чтобы его купить на фабрике. То есть ваш бизнес – доставка/перемещение товаров с эффективностью выше, чем это делают ваши конкуренты, производители или мог бы сделать сам клиент.Данный раздел, вероятно, является самым спорным, потому что в нем делается однозначное утверждение, причем вывод довольно дискуссионный. Те, кто этой мысли не уделял много времени, могут сходу придумать несколько критичных замечаний и потребовать дописать в формулировку несколько нюансов. Да и мне самому приходилось работать в ритейловой компании, основной бизнес которого не движение товаров, а время. Я долго осмыслял природу этого, пока не пришел к мысли, что работаю не в ритейле, а децентрализованном производстве и тогда все стало на свои места. В целом же данный вывод это результат многолетней практики в разных областях ритейла и итог большого количества наблюдений за развитием разных компаний.Ритейловый бизнес — это логистический бизнесК этой части статьи нужно принять позицию, указанную в заголовке, чтобы дальнейшее ее прочтение не формировало новых противоречий. Теперь попробуем разобраться в самом логистическом бизнесе. Если на время забыть о существовании всех остальных частей ритейла, то сама логистика тоже имеет внутреннее дробление. Крупных блока можно выделить три:Soft-логистика – перечень задач и функций направленных на работу с данными, предшествующих физическим операциям с материальными ценностями. По сути, все это можно объединить в один большой термин – планирование, но под этим термином скрываются совершенно разные задачи в зависимости от того, какой горизонт и детализацию вы планируете. Если бы у нас была задача перевезти товар из точки А в точку В, то в этом месте появилось бы понимание, куда надо перевезти товар, когда и в каком количестве.Hard-логистика – физическое исполнение планов, то есть процесс состоящий из действий конкретных юнитов по погрузке, упаковке, перевозке, приемке и распаковке ценностей, включая все сопутствующие затраты. Очевидно, что это самый дорогой элемент в логистике, пропорции таковы, что на тысячу исполнителей может хватить одного планера из предыдущего блока (правда снабженного современными инструментами). А стоимость одного склада в разы превышает стоимость серверного оборудования необходимого для планирования.Support-логистика – бэк-офис помогающий бизнесу развиваться в правильном направлении, включающий топ-менеджмент, все аналитические элементы, такие как построение бюджетных планов и анализ затрат, повышение производительности труда и прочие непрофильные задачи. В некоторых компаниях у этой функции есть даже собственный HR, СБ или IT.Находящиеся в разных этапах своего эволюционного развития компании приходят к такой структуре постепенно. Если компания совсем молодая, то планирование обычно находится в коммерческой функции, порой это просто ассистент категорийного менеджера ответственный за заказы поставщикам. Чуть позже формируется подразделение ответственное за товародвижение, а в крупной компании это превращается в отдельный блок по планированию. Как правило в финальной части этот блок переходит в ведение лидера по hard-функции, именуемого, чаще всего на западный манер, «Директор по цепочкам поставок», он же «Supply-chain manager, SCM», или по-нашему «Директор по логистике». Этот менеджер (SCM) находится в блоке support (как и написано выше) и анализируя экономику своего направления он просматривает P\u0026L (отчет о прибыли и затратах), стремясь сократить стоимость своих затрат на обслуживание того потока, который он обязан через себя пропустить для обеспечения продаж. Обычно ее считают в деньгах на единицу перевозимого на км товара. Это некая квинтэссенция эффективности. Если вы и ваш конкурент возят одинаковый объем на одинаковое расстояние, но вы делаете это дешевле и с гарантированным качеством, то вы эффективнее чем конкурент, а значит ваши шансы на выживание значительно выше.Возвращаясь к основному инструменту этого менеджера (P\u0026L), он видит в своем бюджетном анализе огромные затраты на водителей, складских работников, обслуживание складов и автопарка, а также затраты на развитие и амортизацию инфраструктуры, фокусирует свое внимание на снижение стоимости всех этих статей, повышая эффективность производительности труда и снижая стоимость одной операции. Все это верные шаги, если мы исходим из формулы «перевезти товар из точки А в точку В». На этой простой модели действительно эффективнее тот бизнес, что сделает это быстрее и дешевле. Но в погоне за стоимостью порой присутствуют явные перегибы. К сожалению, часто наблюдал такую картину, что весь бюджет развития функции цепочек поставок эти менеджеры инвестировали в задачу эффективности Hard оставляя своему малому Soft спутнику небольшие крохи. И также часто видел, как достигая плато по эффективности эти же менеджеры верили в некие «святые супертехнологии» (например, роботизированные склады), которые имели окупаемость в десятки лет, но они готовы были идти в них лишь бы сократить стоимость одной операции на несколько процентов. Но всегда ли это верное направление?Перевезти товар из точки А в В.До текущего момента статьи мы отталкивались от позиции, что задачу, которую решает логистика в ритейле (а по сути, весь ритейл) это перевозка товара, сознательно упрощая модель до такого простого примера как в заголовке. Принцип упрощения сущностей для понимания — это стандартный подход во многих задачах, но теперь хочется разобраться, а так ли просто все в ритейле. Можем ли мы упрощать эту задачу чтобы оценить ее целиком? Здесь следует обратиться к ежедневным функциям soft-логистов и задать вопрос, чем они занимаются при планировании. Ответ будет примерно такой, в их задачах нет никакого «rocket science», они пытаются составить план поставок, соответствующий принципам необходимости и достаточности.Изображение MidjourneyДля этого план должен соответствовать ряду принципов:Важной вводной для любого плана является горизонт, то есть план содержит в себе несколько отрезков времени в будущее (например, план на неделю содержит дни, план на квартал может содержать или дни, или недели целиком и так далее) для каждого из которых план составляется на индивидуальных правилах с учетом прогноза продаж, изменений в структуре сети и потока, а также изменений параметров используемых в алгоритмах, отражающих пожелания к корректировке работы системы.План должен обеспечивать «необходимость», за этим простым принципом скрывается модель определения минимального порога остатков ниже которого риск недо-продажи становится критичным. Это означает, что план может включать в себя сознательно заложенный уровень недо-продаж, важный нюанс, который создает разницу между прогнозом продаж и планом. Вы ожидаете, что прогноз находится в вилке от 7 до 13 штук, на какой уровень вы будете ориентировать свою систему? Самый простой ответ – конечно 13 штук, ведь привези мы 12, есть риск потерять продажу одной штуки, но ответ не всегда так однозначен, по «другую сторону» складывается совершенно иная экономика и не всегда обеспечение 100% вероятных продаж это обеспечение самой эффективной затратной части.Напротив необходимости находится принцип «достаточности», который должен лимитировать верхнюю границу в вашей структуре остатков. Это тот принцип, который должен ограждать от роста затрат в рабочий (замороженный) капитал (стоимость всего товара внутри торговой сети, который по принципам банковского процента генерирует регулярные затраты). Плюс перегруженная сеть поставок приводит к необходимости открывать и поддерживать новые звенья цепи (склады и подсобные помещения), увеличивать площадь для хранения, где товар не продается, а просто лежит. В итоге при пересчете затрат на каждую единицу товара существенно растет стоимость транзита на километр, а мы помним, что это ключевая метрика директора по функции.Опциональным, но крайне желательным является общий баланс потока. Если мы построим план, соответствующий вышестоящим условиям, вы получите очень волатильную кривую. На простом примере: у вас торговая сеть по продаже алкоголя, очевидно, что пятничные продажи существенно выше продаж понедельника. Значит план остатков показывает «пик» в четверг вечером и «яму» утром понедельника (вспоминая сердечные такты логистики мы видим на плане кардиограмму). На языке логистики это означает, что ваша инфраструктура работает с перегрузкой в четверг и «простаивает» в начале недели. А что такое «простой»? Допустим у вас есть склад, где работает 10 человек в смену. В понедельник на него падают задачи загружающие 5 человек из 10, значит вы платите зарплату 5 сотрудникам обосновано, а 5 получают зарплату не «за работу», а «за выход», то есть впустую с точки зрения бизнеса. В четверг на склад падают задачи на 15 человек, ваши 10 человек работают с перегрузкой и возможно вам нужно вывести из резерва дополнительных людей. Это дополнительные затраты на вызов человека с выходного, да и вместит ли ваш склад 15 человек и не будут ли они «толкаться локтями»? А ваш погрузочный блок может обслужить в 1,5 раза больше машин? А у вас есть такое количество авто? А в магазине могут принять объем в 1,5 раза больше и успеть выложить? В общем и целом, затраты на единицу растут, ведь вы закладываете в него «простой» понедельника и двухкратную оплату выхода с выходного в четверг. Конечно, на небольшом складе можно особым образом спроектировать смены в течении недели, но, если у вас 50 складов в каждом из которых 200 человек, вы уже не решите эту задачу простым планированием смен. Но и на небольшом складе у вас половину недели хранится товар, а это дополнительное пространство, ведущее к росту аренды. Резюмируя данный пункт, эта часть задачи не связана напрямую с наличием товара, но очень важна для директора по функции и влияет на экономику логистики в целом.Описанные выше принципы несколько усложняют задачу от уровня «перевезти товар из точки А в В» до уровня «сформировать план перевозки всех товаров между всеми точками, обеспечивая минимальное и достаточное количество товара в каждой точке и балансируя общую нагрузку на логистическую сеть». Также из указанных принципов следует ключевые параметры эффективности (KPI) для оценки работы блока в целом. Как правило это:Доступность товара (OSA, ISA) – как сравнительная метрика «ожидаемое наличие к фактическому». То есть логистика принимает на себя обязательство обеспечить наличие определенного ассортимента. Тот ассортимент, что есть в наличии вероятнее всего меньше, чем тот, который ожидается. Плюс сам термин «наличие» имеет разный способ расчета. Если товара 1 штука – это достаточное наличие? Вероятно, для одних товаров да, а для других остаток не должен упасть ниже 3-5-10 штук. Отношение факта к ожиданию и есть оценочное состояние качества решения этой задачи.Товарный запас или оборачиваемость (inventory turnover) – это ограничительная метрика, следить за которой нужно в динамике. Она показывает отношение среднего остатка к средней скорости продаж в сравнимых единицах. Идеальный и, в то же время, недостижимый запас это 1 день продаж для всего остатка. А какой является оптимальным для конкретного ритейлера это результат сложных моделирований и вычислений. Те ритейлеры, которые не научились строить эти сложные модели обычно работают с динамикой этого параметра стремясь каждый год его улучшать. Можно было бы сказать, что так делать неразумно, но практика бизнеса такова, что 95% компаний работают с этой метрикой именно так и при этом лидируют на рынке, возможно это неплохой способ работы.Затраты логистики (costs) – способ оценки экономики логистики в сравнении с другими компаниями. Обычно считается как отношение затрат логистики к выручке всей компании в деньгах. Получается процент затрат, что позволяет использовать эту величину с бенчмарками рынка. Компания А сравнивается с компанией В путем сравнения этих величин из публичных отчетов. Это в целом верно для простых сравнений, но для стратегического целеполагания несет в себе ошибки, более того это опасный путь. Тут небольшое отступление.Отступление про затратыДело в том, что процент затрат — это не только затраты на ресурсы и персонал, но следствие той инфраструктуры, которой обладает ритейлер. Если мы имеем две абсолютно одинаковые по макропараметрам сети, то безусловно более экономная сеть выглядит выгоднее чем, та, что тратит больше. И именно так рынок оценивает две компании между собой. Затраты на логистику уже давно находятся в публичной отчетности любой компании с размещением акций. Но даже непубличные компании все равно сравниваются по этой метрике путем небольшого шпионажа.Важен тот факт, что термин эффективность — это сложный термин. Он включает в себя затраты и их соответствие стратегическим целям. И как правило о «соответствии стратегическим целям» забывают, вынося на первый план только фактические затраты. На практике более высокие затраты это не обязательно ошибки менеджмента в части управленческих решений, но и вполне разумные резервные вложения, направленные на исполнение стратегических решений:Например, устойчивость напрямую коррелирует с тем, какой у вас ресурс – собственный или аренда. Когда на рынке начинается нестабильность она мгновенно влияет на компанию, переложившую свои проблемы на 3PL и аут-стафф. Тут уже нет возможности оперировать термином «лояльность» если это не ваши сотрудники. Или нет возможности отложить рост затрат на ресурсы путем управления объемом сделки, 3PL это отдельный от вас бизнес, он живет в своей стратегической модели.Способность к гибкой реакции на точечные изменения зависит от ваших резервов, которые стоят дополнительных затрат. Компания со своим штатом всегда растит замены линейного персонала и менеджеров внутри себя, перекрывает отпуска и больничные своими собственными штатными сотрудниками, а HR постоянно имеет некое понимание о стоимости среднего специалиста на рынке и понимает, как нужно увеличить ФОТ чтобы закрыть вакансии или снизить текучку в конкретной части цепи поставок. Также понимает цену обучения собственных сотрудников.Любому росту предшествует рост затрат, сначала надо потратить деньги на строительство склада, его конфигурирование и последующее комплектование персоналом, а потом он начинает генерировать возврат вложений. Конечно, грамотный финансовый директор может немного схитрить и вынести затраты на строительство в инвест-вложения исключаемые из затрат до расчета прибыли и не включать их в стоимость логистики, но ФОТ нового персонала, который вы наняли заранее для обучения туда уже не отнесешь.Так как метрика стоимости логистики обычно фигурирует в публичной отчетности, директору по функции часто ставят в цели снизить эту величину, чтобы выглядеть привлекательным для инвестора, но в своей практике я наблюдал как цели по этой метрике ставились несвоевременно. Например, компании в быстром росте ставили задачу сократить инвестиции или компании, прошедшей предшествующий год с проблемами, на новый бюджетный год ставили задачу пройти еще дешевле, без учета того, что проблемы являлись следствием недофинансирования прошлых периодов. И это недофинансирование не покрывал даже рост компании или инфляция. Затраты считаются как относительная величина, поэтому рост выручки теоретически увеличивает доступный бюджет логистики каждый год, но рост может быть в районе 10%, а недофинансирование значительно превышать эти цифры. При этом от директора ждут что он сможет уместиться в часть этой суммы, то есть ему доступно из 10% роста только 3%, а остальное он должен сэкономить, показав рост эффективности.Циклы для таких компаний выглядели так – пришел новый директор по функции, получил задачу снизить затраты (costs), стремясь получить бонус срезал как можно больше, в том числе жизненно важные функции, в коротком периоде это не привело к остановке работы из-за инерции, получил бонус, ушел в другую компанию оставив своему наследнику невыполнимую задачу с большим количеством воспаленных участков, руководство компании ищет топ-менеджера готового еще больше срезать затраты. Дальше цикл повторяется до первого крупного кризиса. После чего компания делает дикие вливания средств в эти статьи, порой выше, чем если бы компания делала все своевременно. Я бы добавил в публичную отчетность пункт про то, сколько текущий директор по функции занимает свой пост или сколько их сменилось за последние 5 лет, причем речь не только про функцию логистики, но и про остальные публикуемые метрики.Я сознательно сделал такое большое отступление про затраты несмотря на то, что статья про Soft-логистику, а затраты генерирует в основном Hard-логистика. Забегая вперед – позже в статье мы рассмотрим, как Soft-логистика может помочь директору по логистической функции найти внутренние резервы без вливаний в покупку складов или транспорта.Решение задачи формирования планаВ самом начале этой статьи мы говорили о делении комплексной задачи на множество простых задач. Если у вас есть сложная задача, то иногда вы можете разложить ее на простые подзадачи и добившись улучшения на маленькой задаче получить общее влияние на результат. Для примера возьмем задачу из области логистики.Для Hard-логистики это, например задача пробега при комплектации заказа специалистами отборщиками на складе. Ваши сотрудники суммарно ежемесячно проходят расстояние до луны? Берем маршрут одного сотрудника при сборке одной строчки в заказе и работаем с ним. Меняем логику расстановки товара в зонах склада, логику WMS в размещении товара в местах хранения относительно мест отборки, формируем задания на спуск таким образом, чтобы не терять время на сборку одного заказа. Мелкими шагами уменьшаем сборку одной строки с трех минут до двух минут, пробег с 30 метров до 20 и этот рост производительности на 30% на подзадаче приводит к существенному улучшению общего решения.Пример сокращения пробега на базе зонирования складаК сожалению, далеко не все задачи могут быть разложены на простые элементы. Речь идет о задачах, имеющих зависимости друг от друга. В примере выше я описал сборку одной строчки заказа с точки зрения процесса отборки. Но когда сотрудник складывает заказ на паллет или в другую транспортную тару он уже решает задачу сборки груза целиком. И тут нельзя добиться оптимизации работая со складыванием одного товара. Всегда найдется товар с другими весогабаритными характеристиками и его надо принимать во внимание. Будет странно складывать тяжелые аккумуляторные батареи поверх пластиковых бутылок с горючей жидкостью. Решение этой задачи всегда происходит в комплексе, и система должна комбинировать большое количество вариантов чтобы найти наилучший. В математике эта задача имеет общепринятый пример, на базе которого ее пытаются решить наилучшим образом – это задача о рюкзаке.Картинка из Википедии к статье «Задача о рюкзаке».Сложность этой задачи заключается в трех моментах:1. Есть общая емкость рюкзака, которую нельзя никак превышать2. Каждый элемент имеет несколько измерений, такие как, например, вес, ценность, габариты и представлен ограниченным количеством (то есть нельзя взять 10 товаров одинаковой ценности и размеров, надо сложить из имеющихся).3. Загрузить рюкзак необходимо максимально возможным весом и максимально повысить итоговую стоимость груза.В логистике можно себе это представить загрузкой грузовика, в который надо сложить товар в одной партии. Ну например вы везете компьютерную технику, у вас есть коробки с клавиатурами, с мышками, мониторы, системные блоки, процессоры, блоки бесперебойного питания и прочие комплектующие. В один грузовик вся партия не влезет и вам надо сделать так, чтобы загрузка объема грузовика была максимальной, а ценность перевозимого груза наибольшей, в этом случае отношение цены перевозимого груза к цене рейса будет наивысшей. Допустим рейс стоит 500 000, а перевозимый груз, состоящий только из блоков бесперебойного питания, 5 000 000, цена вашей логистики 10%, если же мы повезем полный грузовик процессоров, то цена рейса практически не изменится (ну разве что подорожает из-за стоимости страховки), но перевозится уже 500 000 000 и логистика будет стоить 0,1%. В реальности отвезти надо сборный груз, идеального заказа, состоящего только из процессоров, скорее всего, не получится.Не буду останавливаться на методах решения этой задачи, повторю лишь, что все методы — это различные вариации перебора всех возможных вариантов. Некоторые методы решения сокращают количество перебираемых вариантов отсечением большой ветки «плохих» вариантов, некоторые останавливаются на приближенном решении. Очевидно, что идеальное решение можно найти только взвесив каждый вариант и сравнив с другим.Главная мысль данной главы заключается в том, что задача по расчету плана движения товаров — это комплексная задача и ее сложность выше сложности остальных задач, в силу того что она является не разбиваемой на простые подзадачи. В ритейле есть и другие задачи, которые могут решаться с использованием комплексного подхода. К ним относится:Управление ассортиментом – хороший ритейлер стремится сформировать ассортиментную матрицу по группе, учитывая каннибализацию продаж идентичных товаров между собой и стремясь расширить предложение на все группы покупателей.Ценообразование и промо – если ваши технологии достаточно развиты, то вы, делая скидку на один товар, учитываете цены аналога, чтобы не привести ситуацию к полной потере прибыли. Совершенно нет смысла два разных бренда в одно время продавать с одинаковой скидкой. Покупатель возьмет свой любимый бренд, а ваша скидка не привела к стимуляции спроса, то есть вы просто «слили» маржу. Конечно, есть нюансы, но учитывать в ценообразовании одного товара цену на другой это рабочий метод.Прогнозирование продаж – по мере усложнения алгоритма продаж, каннибализация должна стать значимым фактором прогноза в силу того, что действия с ассортиментом в рамках одной группы спроса влияет на группу целиком. Рост продаж одного производителя в нормальном сценарии может привести к замедлению продаж его аналога, просто по той причине, что в одинаковом трафике обычно наблюдается равномерный спрос на определенный вид продукции.Эти примеры отличаются от примера с планом движения товаров так как являются для этих процессов опциональными. Вводя новую ассортиментную позицию и не выводя ее полный аналог, вы на какое-то время переполните полку, но категорийный менеджер вправе рассчитывать на то, что эту ошибку исправит логистика. Просто через время вы увидите полное замедление заказов по позиции, на которую нет спроса. Неверное ценообразование по двум позициям это нереализованный потенциал прибыли, но не потеря её. А прогноз продаж, не учитывающий продажи аналога при вводе, учтет его с небольшой задержкой, когда более слабая позиция покажет статистически отрицательный тренд. Иными словами, это потенциальные точки роста в этих процессах, которые обычно есть в списке будущих задач, но чаще всего ближе к хвосту бэклога, потому что влияние этих задач на общий результат малозначимо. То есть те самые задачи по Парето входящие в 20% результата за 80% усилий. Для плана движения товаров (или плана пополнения) это является базовой частью задачи, так как она всегда должна решаться комплексно. Давайте разберем почему так.Из чего состоит план.В первую очередь план пополнения представляет собой шкалу времени, протянутую от сегодняшнего дня в будущее на несколько циклов. Если пополнение происходит с частотой раз в неделю или чаще, то план обычно строится на уровне дня и на горизонт в 30-90 дней. Если пополнение происходит раз в месяц, то план строится на уровне недель и на горизонт 15-50 недель. На самом деле это просто отраслевая практика, а не незыблемый принцип.Основной влияющий на это фактор это цикличность поставок, в плане должен быть расчет относительно 3-10 ближайших поставок. То есть план обязательно обладает признаком точки О – стартовая точка, в которую мы приступили к расчету, точка А – ближайшая возможная поставка, точка В, С и т.д. – следующая возможная поставка.Шкала времени для расчета планаСобственно, производя нужные вычисления в каждую из возможных точек мы принимаем решение нужно ли в ней производить пополнение отталкиваясь от локальных ограничений. Заказ нужно производить если есть риск обнулиться до следующей точки и, при этом, сумма заказа должна быть минимальной возможной. Нужно учесть ошибку прогнозирования и прочие параметры, которые заложил пользователь на будущее. Это простая формула вида А+В*С, решаемая несколько раз в зависимости от того, сколько дат у нас на плане. Результат расчета этой формулы называется «идеальный план», в отрасли он зовется «неограниченным» потому, что результат расчета не содержит в себе ограничения, задаваемые физическими лимитами объектов и лимитами в контрактах с поставщиками. Отправлять неограниченный план в исполнение нельзя, контрагент не выполнит заказы в которых не учтены ограничения, а заказы не учитывающие лимиты «физики» просто не будут выполнены в срок. Это к тому, что решение задачи А+В*С не является ценным решением ввиду того, что его не исполнят, а значит комплексное решение — это не опция в данном случае.Если мы определились по одному товару, что его заказ надо сделать в точке А, то следующим этапом мы формируем заказ по всем товарам, которые нам отправляет данный контрагент, просто группируя в заказ все товары, которые тоже нужны в точке А. В простом варианте этот заказ можно отправлять, но тут не учитываются ограничения контракта. Например, сумма заказа в точке А должна быть больше какой-то суммы в рублях, потому что в контракте написано «заказ меньше чем на Х контрагент не отгружает» (его рейсы тоже стоят денег и везти товара на 500 000 за 50 000 он не хочет). В этот момент к каждому товару в заказе должны быть применены эти ограничения. То есть каждый товар надо пересчитать так, чтобы суммарные условия были выполнены. Решение этой задачи это решение системы уравнений.Такая же система уравнений решается если контрагентом выступает внутренний склад, но в этой задаче уже нужно учесть, что на складе могут быть дефициты, выступающие верхней границей. А если принять во внимание, что склад — это тоже объект, пополняемый нашей системой, появляется сложная матричная система зависимостей между уравнениями. Одна система уравнений решает задачу поставки со склада на магазин, а вторая систему поставки от поставщика на этот самый склад. И эти поставки должны быть синхронны между собой. Если магазину товар нужен на складе 10го числа, то поставка на склад 11го числа становится малополезной. Именно внутренние склады существенно усложняют расчеты. Если предположить, что вы смогли решить задачу с одним эшелоном, то представьте себе реально существующие на практике 3-4 эшелона.Решенная задача в данном случае — это ограниченный план. То есть в нем, в отличии от предыдущего «идеального» плана уже учтены ограничения первого этапа.Таким образом решая задачу для одного товара, мы решаем простое уравнение.Если задача решается для заказа, то мы решаем систему из нескольких уравнений.Если задача решается для одного склада мы решаем матрицу систем уравнений.Может показаться, что это конечные шаги решения, но тут вступает фактор оптимизации. Это тот кейс, когда пятничные отгрузки кратно выше отгрузок в понедельник. Это накладывает еще один уровень усложнения решения, но следуя примерам из других сфер ритейла эта задача является опциональной, потому что с точки зрения своих задач (снабжение товарами магазинов и складов с учетом ограничений), мы ее выполнили, решив матрицу из систем уравнений.Подводя итог данного раздела, формируется следующий вывод: задача составления плана поставок (плана пополнения) — это комплексная задача, декомпозиция которой на более мелкие всегда снижает качество общего решения, а в некоторых случаях упрощение до простой логики «перевезти товар из точки А в точку Б» делает задачу нерешенной в принципе.И тем не менее.Если вспомнить эволюционный путь подразделения soft-логистики внутри компании (там, где оно зарождается в коммерции и в итоге приходит к SCM), то нужно понимать, что системы компании тоже проходят эволюцию в несколько этапов вместе с самой компанией и функцией управляющей запасами. Когда компания открывает свой первый магазин, то заказы на него считаются вручную человеком, не имеющим вообще никакого отношения к экспертизе по товародвижению. Зачастую это просто менеджер магазина, отвечающий «за все» в новом проекте. По мере роста и развития подразделения сначала появляются Excel файлы для расчета (именно расчета, учет делается в ERP), потом появляются вариации автоматизаций, либо это макросы, либо какие-то надстройки или сервисы к материнской учетной системе собственной разработки.Качественный переход происходит как всегда по Гегелю, после количественных изменений. То есть с открытием новых торговых и складских объектов, по мере появления новых категорий товаров и схем поставок сложность логистики растет, количество сотрудников ответственных за пополнение увеличивается и управление в Excel становится невозможным. Убытки от ошибок в простых инструментах становятся вполне ощутимыми за счет масштаба, сотрудники жалуются на невозможность решить какие-то задачи без улучшений. К этому моменту ответственный за этот блок понимает, что качественный переход надо производить. Именно в такие моменты принимаются решения о покупке профильных систем для целей товародвижения или их самостоятельной разработке «с нуля».Следующим шагом, как правило, является внедрение несложной системы позволяющей упростить рутину. То есть простой расчет заказов с учетом небольшого набора ограничений, как правило автоматизируется самый массовый вид документов – заказы на магазин. Крупнейшие ритейлеры начинали с «автозаказа магазинов с встроенным прогнозом на скользящей средней». Более редкие и сложные заказы на склады, в которых нужно учесть многофакторные ограничения продолжают делать эксперты с большой практикой и развитой интуицией. Они видят прогноз по скользящей средней идущий вниз, недоверчиво оценивают взглядом и экспертным образом закладывают по своей практике увеличенный объем заказа. В компании к этому моменту обычно введена измеряемая система ключевых показателей (KPI), а для сотрудников есть система штрафов и поощрений. Ему не хочется терять свой доход допустив ошибку со снабжением, он использует метод собственных перестраховок.Заказы «на склад» могут считаться вручную, но специалисты зачастую стремятся внедрить внутреннюю автоматизацию на самом раннем этапе, пускай и простыми формулами или макросами. В общем и целом, это действительно рабочий метод использовать Автозаказ для магазинов и Excel для расчета заказов на склады. Так может продолжаться несколько лет, пока измерение KPI и постоянных поиск эффективности не приводит логистику компании к тупику. Уменьшение «пробега по складу» не дает возможности существенно сократить стоимость логистики ежегодно. Эффективность, достигнутая за первые несколько лет приходит к своему «плато», и грамотный менеджер по функции логистики (SCM) обращает свой взор на специалистов по управлению запасами.На этом этапе, который обычно предшествует следующему качественному переходу, стандартный диалог руководителя с экспертами строится примерно следующим образом:- Что вам мешает делать качественные заказы на склады? Почему одной части товара критически мало, а другой части с существенным избытком. Операционный бизнес постоянно на это жалуется.- У нас очень плохой прогноз с большой ошибкой, товары по которым прогноз завышен мы пере-заказываем, а по которым занижен недо-заказываем. Из-за этого мы и получаем «плохие» запасы.- Что нужно сделать чтобы вы стали заказывать лучше и мы стали выполнять цели по KPI?Повысить точность прогноза, а дальше мы справимся!Этот ответ отчасти верен, но в целом неверный совершенно. Дело в том, что, задавая вопрос эксперту его спрашивают о том, как он будет делать лучше свою часть работы, но мысля своим фрагментом общей задачи, он не всегда способен рассказать, как повысить качество решения задачи в целом. Общаясь со своими коллегами, я часто слышал вопрос от топ-менеджеров в стиле «что надо сделать чтобы повысить точность прогноза для заказов?», который по сути являлся передачей их диалога с экспертами.Начнем с того, что постановка вопроса в целом некорректна. Задачу, которую надо решить – это повысить качество решения задачи управления запасами. А через какой подход это уже следующий вопрос. Надо понимать, что эксперту выгодно занимать позицию, в которой его ошибки очень небольшие, а вот внешние факторы влияют на его результат существенно, это не означает что он вредит компании своими действиям, просто он мыслит теми категориями, в которых он профессионал и выдвигает соответствующие гипотезы. Здесь самый правильный шаг – собрать необходимый срез данных и перепроверить эту гипотезу как я предлагал в начале статьи.С большой вероятностью повышение точности прогноза позволит вам решить только часть проблем и точно не самую большую (их масштаб есть на графике выше). Пусть в вашей компании могут быть другие пропорции, если в вашей компании ошибки это действительно прогноз, то тогда нечего думать, надо его улучшать как можно быстрее. Но если вы уперлись в проблему, генерируемую количественным накоплением ошибок, то в данной точке нужно совершить качественный переход.Если бы я спросил людей, чего они хотят, они бы попросили более быструю лошадь. Г. Форд               Цитата из интернета, которую часто приписывают Генри ФордуЗавершить данный раздел я бы хотел небольшим итогом: несмотря на то, что задача планирования запасов является комплексной, на разных этапах ее все еще можно дробить на подзадачи. Отдавать рутину различным математическим алгоритмам, а финальную сложность собрать в работе эксперта. Но такое дробление лишь отложит проблему на время, до тех пор, пока логистика показывает общий положительный результат (например, за счет улучшений в hard). Если у вашей компании все хорошо, и она продолжает развитие, то вы вынуждены будете вернуться к задаче soft, когда достигните предела в возможностях физических улучшений.Новые этапы эволюцииК этому моменту уже понятно как дальше должна развиваться компания, нужно сделать следующий количественный переход от конфигурации «простое алгоритмами, сложное человеку» к конфигурации «всё алгоритмами, человеку контроль». Это существенный переход так как меняется роль специалиста с ключевой экспертной, на простую ремесленную. Но и у этого периода тоже есть этапы. Если решение о замене системы или ее существенной переработке принято заранее, то в данной точке происходит внедрение мульти-эшелонной системы управления запасами. Которая позволяет решать систему из матрицы уравнений с максимально достижимой точностью. Если вспомнить то, о чем я писал выше, как правило решение этой задачи – это перебор вариантов. А значит система позволяющая на огромном массиве делать перебор всех вариантов не всегда достижима. Используются методы и системы позволяющие найти «приближенное» решение через упрощение комбинаций. Такой шаг ведет к сокращению роли эксперта, в задачи которого входит настройка системы таким образом, чтобы она выдала результат, соответствующий целям компании. Например, компания хочет снизить оборотный капитал, риски списаний и готова пойти на некие упущенные продажи – снижаются настройки представленности товара и цели по сервису, либо наоборот задача на максимальные продажи и тогда все страховочные механизмы выкручиваются на максимум.Такая гибкость дает не только плюсы в бизнес метриках, но для нашего менеджера по функции (SCM) она дает возможность существенно облегчить жизнь hard логистике, генерируя экономию по-своему. Если напомнить то, что написано выше«он просматривает P\u0026L, стремясь сократить стоимость своих затрат на обслуживание того потока, который он обязан через себя пропустить для обеспечения продаж»дальше шла речь о том, что он работает с самым атомарным состоянием данного потока – пробегом отборщика на складе. Но управление потоком в целом это тоже весьма большая опция. «Прикручивая» или «откручивая» посильнее движущийся поток в разных точках своей кровеносной системы, он может решать проблемы на глобальном уровне.К сожалению, такую возможность дает только полноценная система, не состоящая из экспертов. Практика в бизнесе такова, что используя метод «письменных распоряжений» управлять потоком нельзя, вы можете написать своим экспертам письмо «Ребята, у нас в таком-то регионе проблема, давайте подумаем как сократить там поток процентов на 30», но если ваша компания уже доросла до большого уровня сотрудники отвечающие за поток в этом регионе могут либо слишком затянуть гайки по всем направлениям, лишив вас и важного, и неважного товара. Либо вы дадите им свободу самим применять систему приоритетности и тогда они не будут сотрудничать. Есть известная в теории игр «дилемма заключенного», которая объясняет почему рациональные люди вольные делать выбор не всегда будут сотрудничать друг с другом в пользу других, даже если это в их интересах.Иллюстрация Midjourney по теме «Дилемма заключенного»При этом ни у кого из них не будет злого умысла, они искренне хотят помочь своему руководителю, также искренне они считают «свою часть» потока наиболее важной и считают, что высокие показатели именно по их части ведут к успеху всю компанию, пусть и «небольшим ущемлением» других категорий. С этим может даже попытаться побороться мидл-менеджмент, но за тысячами ежедневных решений очень сложно проследить. Зато беспристрастно принять те же тысячи решений основываясь на объективных показателях может система. Эксперту нужно лишь настроить базовые принципы этих ограничений в системе и алгоритмы сами найдут ближайшее решение, которое позволить исполнить волю руководителя. То есть внедрение системы это в первую очередь повышение управляемости.Если с моментом, когда мульти-эшелонную систему стоило внедрить менеджер затянул, оставив приоритет задачам soft ближе к тупику, то к системе управления запасами ставятся более амбициозные задачи. Кроме автоматизации процесса принятия решений на объективных метриках обычно от системы ждут и существенного улучшения в части экономики. Система должна уже не только принимать решения в рамках заданных ограничений, но и управлять самими ограничениями предлагая их наилучшее состояние. Это на несколько порядков повышает вариативность задачи, то есть существенно повышается количество комбинаций, которые нужно перебрать и выбрать решение с наилучшей производной функцией. При этом такой перебор нужно делать очень часто, в огромных компаниях несколько раз в день, а в небольших не реже чем раз в неделю.Заранее скажу, что в данный момент систем, которые в состоянии перебрать все варианты не существует просто в силу количества вариантов. Самые дорогие системы от ведущих вендоров работают над упрощением процесса перевода, разбивая задачу на этапы и в каждом этапе находя приближенное решение. Как правило работу с «экономикой» логистики решает даже отдельный модуль системы, который позволяет производить «Что, если?» анализ или модуль, который сам просчитывает синтетические варианты ограничений в виде симуляций. Но отсутствие точного решения не означает, что нельзя найти следующее решение, которое лучше текущего.Второй важной опцией данных модулей является возможность найти оптимум, а не жить в парадигме «лучше год к году», что, как правило, означает просто меньше тратить. После внедрения данной системы топ-менеджер отвечающий за данную функцию (SCM) может аргументировано показать, что текущее состояние затрат или другого KPI является обоснованным математически, указать какие именно товары или контрагенты приводят к росту и предложить работать над ограничениями уже в другом направлении, не стремиться их исполнить, а стремиться их пересмотреть в лучшую для компании сторону.В одном из примеров моей практике на этом этапе появился «логистический контракт» вкупе с «коммерческим контрактом». До этого с поставщиком продукции подписывался договор коммерческой дирекцией. Менеджеры, отвечающие за работу с поставщиками, работали в интересах своих показателей, поэтому соглашались на выгодную цену закупки, даже если за ней стояли невыгодные ограничения по логистике. Потому, что скидка в процент на выручке в сто миллионов — это ощутимая и оцифрованная экономия на один миллион, а тот факт, что этот миллион приводил к удорожанию логистики на три миллиона никто просчитать не мог. В итоге внедрения системы позволяющей моделировать, что будет при пересмотре условий, многие контракты стали подписываться по другой системе. В частности, без «визы», что данный сценарий не приводит к удорожанию затрат или что удорожание принято, контракт не двигался дальше по цепочке согласований. Работать это могло и в обратном направлении, когда поставщику предлагалось смягчить условия, а рост цены, который поставщик требовал взамен был заранее пре-согласован.А что там с прогнозом?В процессе прочтения уже должно быть понятно, что в многофакторной модели расчета приближенного варианта решения задачи по запасам, прогноз, безусловно, является одним из факторов влияющим на производный результат решения. Он имеет свой вес в общем результате довольно высокий если его сравнивать с факторами второго и третьего порядка, но в самой формуле, решение, которой надо найти заложена логика позволяющая балансировать ошибку прогноза за счет прочих переменных. Вернемся, к примеру с продажами:Вы ожидаете, что прогноз находится в вилке от 7 до 13 штук, на какой уровень вы будете ориентировать свою систему? Самый простой ответ – конечно 13 штук, ведь привези мы 12, есть риск потерять продажу одной штуки, но ответ не всегда так однозначен, по «другую сторону» складывается совершенно иная экономика и не всегда обеспечение 100% вероятных продаж это обеспечение самой эффективной затратной части.Что если фактические продажи по итогам данного прогноза будут 15 штук? Очевидно, что имея в своей основе неверные предпосылки решение сложной системы уравнений, которое мы выбрали не позволило нам обеспечить необходимый нам товар. Если мы берем только этот частный случай, то в действительности вы получили очень существенный объем недо-продаж (2 из 15 которые вы не привезли – это 13%, а если не был сделан страховой запас, то целых 30%). Однако в реальности эта разница скорее всего будет нулевой или в несколько раз меньше. Во-первых, прогноз всегда ошибается в обе стороны, поэтому недо-проданные куртки вполне себе компенсируют случайно пере-проданные, из-за зеркальной ошибки прогноза, пальто. Во-вторых, на товарный запас кроме прогноза влияют десятки других факторов, самый частый из них – округление потребности до уровня упаковки. Самым частым явлением решения задачи является округление вверх до упаковки, упаковок до паллета, паллеты до авто, авто до минимальной суммы заказа поставщику и так далее. 60-80% товарного ассортимента продается с такой маленькой скоростью, что генерируют огромный товарный запас. Когда ваша упаковка 10 штук, а скорость продаж 0,5 штуки в день ваша ошибка прогноза на 50% совершенно неважна на общих цифрах.Выше я уже приводил пример влияния ошибки прогноза на провал в доступности товара на полке в 2% от общего результата. Важно понимать, что это не упущенные продажи, а именно провал в доступности, который в ряде случаев просто приводит к замене продажи одного вида продукции на его полный аналог с точки зрения покупателя. В некоторых отраслях ритейла 2% доступности — это эквивалент 0,5% упущенных продаж, выведенных на статистике. А сколько ваша компания потеряла на том, что выбрала неверный ассортимент? А установила некорректную цену в промо? А на том, что вам не хватило сотрудников на отдельном складе? А на том, что ваш поставщик привез товар с опозданием на два дня по своей вине или по причине очереди на вашем складе? А завал паллеты, допущенный в отгрузке со склада на магазин.Это не означает, что прогноз не имеет значения, а означает что 0,5% упущенных в год продаж это конкретная сумма. Ну, например, ваша выручка 100 миллионов в год, 500 тысяч вы теряете в выручке. Если ваша прибыль 8%, то 40 тысяч чистого влияния на вашу прибыль. Рассматривайте эти 40 тысяч как эффект от проекта по улучшению прогноза. Ожидаемо ваши затраты на сам проект не должны превысить 60 тысяч, чтобы окупить за 1,5 года вложения и начать генерировать положительный эффект. Много можно улучшить на 60 тысяч? Каждый ответит для себя сам.И здесь вероятно стоит сказать о положительных факторах влияния прогноза. На самом деле прогноз позволяет не только избежать недо-продаж. Есть его влияние на избыточные запасы и на просрочку продукции, которая имеет небольшие сроки годности (фарма, продукты питания) тоже довольно велико. Они тоже должны стать частью модели расчета экономического эффекта от внедрения этой системы. Математика расчета этого эффекта похожа на ту, что выше, нужно лишь подставить нужные цифры, но с запасами и рисками просрочки все сложнее. Дело в том, что ваш целевой уровень запаса должен быть результатом моделирования всей системы уравнений. То есть имея оборачиваемость запаса в 30 дней вы не знаете много это или мало. Отраслевой бенчмарк тоже не даст много информации, например, у вашего конкурента 25 дней, а насколько сравнимы все ваши ограничения и насколько похожа ваша инфраструктура? Возможно, у него существенно отличается ассортимент, входная цена от поставщика хуже вашей, но зато нет ограничительных мер в договоре по типу «каждый заказ должен быть кратен автомобилю». Без внедрения системы позволяющей делать расчет плана на базе всех предпосылок вы не получите оценку целевого запаса и не сможете сравнить свой потенциал с текущим фактом.Попытка внедрить улучшенный прогноз на эмпирической оценке его значимости от эксперта приведет к тому, что все улучшения от внедрения более сложной математики будут просто теряться на выполнении других расчетов с учетом лимитов.Системы другого порядкаВажно сказать, что планирование запасов в структуре самой развитой компании ритейлера это лишь одна система из комплекса. Ее глобальная суть быть частью «правильного» для любого бизнеса процесса интегрированного бизнес-планирования (IBP). В этих процессах через системы планирования каждая функция декларирует свои намерения в виде оцифрованных показателей внутри этого комплекса, а система планирования запасов является частью отвечающий за операционную реализацию этого планирования просто как один из шагов.На примере это выглядит как-то так: сначала СЕО компании ставит цели на следующие годы, финансы их декомпозируют до всех бизнес метрик на ближайший год, маркетинг подгоняет стратегию на ближайшие кварталы, коммерция планирует ассортимент на пару сезонов вперед и так далее. В конце этой цепочки стоит управление запасами. Выстраивается такой состав, где каждый новый вагон — это отдельная функция, добавляющая в план свою ценность.Вольная фантазия на тему паровозика IBPРазрыв в процессе создания и исполнения плана приводит к тому, что последующие вагоны начинает разбрасывать в разные стороны и компания страдает от лишних шагов или убытков. Но если план изначально составлен гармонично и с участием всех служб в его обслуживании, то управление запасами существенно упрощается. В такой модели практически исключены ситуации с волатильностью загрузки складов и транспорта, минимизированы ситуации с неверными ценами или неправильно подобранным ассортиментом. Локальная точность прогноза имеет гораздо меньшее значение и управление запасами в первую очередь ориентируется на утвержденный план, нежели на предсказание ближайших продаж, разве что, работая с оглядкой на них. Этот процесс там называют по-разному, где-то это товарно-финансовое планирование, где-то S\u0026OP/S\u0026OE/MIOE, где-то может быть просто совокупность связных процессов планирования, а где-то настоящий IBP.В других компаниях применяют метод «как исторически сложилось», все кто находятся в начале состава не особо переживают, за тех, кто находится в «хвосте» поезда считая их проблемы не своими. Обычной проблемой для таких компаний является ввод неэффективных товарных позиций, подписание невыгодных контрактов, перегрузка магазинов акционными товарами и прочие проблемы разнонаправленных векторов, которые шатают состав из стороны в сторону. Последние вагоны, в числе которых находится управление запасами страдают от того, что к моменту выполнения своих задач получили все ошибки на предыдущих этапах и пытаются создать сверхсложные инструменты исправления ошибок «за других» или встраивают разного рода защиты «от входящих ошибок». Особенно такой проблемой страдают в компаниях, где доля «ре-активных» товаров очень высока, больше, чем 60 процентов, но это не обязательно так, есть разные примеры. Само собой работа такой компании будет постоянно сбиваться из-за отсутствия этого подхода внутри нее, а в некоторых отраслях без этого подхода компания просто не сможет выжить на конкурентном рынке. Пример отрасли, где про-активный подход жизненно необходим ввиду особенности продаваемого товара fashion, строительные материалы или мебель, но не только в них.Мы раньше выяснили что у директора по функции есть как минимум два инструмента, которые ему может дать подразделение Soft:управление потоком в разных точках позволяющих управлять экономикой за счет объемафакторное выделение вклада каждого товара и контракта в модель запасов, чтобы работать не внутри ограничений, а над их изменениямиТретьим инструментом директора по функции (SCM) является балансировка данного состава. Любой новый для компании процесс должен обладать внутренним спонсором чтобы запуститься и начать работать. Как правило внедрение нового процесса, тем более такого как совместное планирование функций, это большое количество ресурсов и времени, а также сложных внутренних согласований с учетом всей политики присущей крупной компании. Но лидировать внедрение этого процесса кто-то должен, а самыми заинтересованным лицами для этого являются те, кто находятся в «последних вагонах», потому что их трясет больше всего. Нужно убедить владельца каждой функции, включая СЕО, что именно от внедрения этого процесса компания приобретает наилучший эффект на логистику. Нужно создать или внедрить готовые инструменты по работе с ним, поддерживать процедурную часть процесса (такую как регулярные встречи).В моей практике такая работа заканчивалась успехом только в половине случаев. Довольно долго руководителю просто не хватает настойчивости и упорства во внедрении такой методики, и он опускает руки. Комитеты по планированию быстро становятся площадками куда не приходят лица принимающие решения, а значит на них не фиксируются нужные действия и не выполняются шаги по улучшению. Но если процесс внедрился, то эффект от этого невозможно недооценить, согласованные действия разных служб давали огромный эффект на коротком периоде. Важно понимать, что эффект планирования достигается не за счет того, что вы смогли решить задачу с огромным количеством вариантов к перебору – быстрее, а за счет того, что вы существенно сократили количество вариантов до небольшой группы приемлемых. То, что раньше казалось проблемой (завышенный запас в каком-то участке бизнеса) уже не проблема работы логистики, а незначительное отклонение от совместного плана компании, у которого уже есть факт фиксации в рамках аналитики, принятое решение об исправлении разными средствами и вы находитесь на пути реализации этого решения в ближайшем будущем.Резюмируя данную главу, хочу отобразить тройку основных инструментов, которыми пользуется директор по функции SCM.Инструменты директора по цепочкам поставокИнструмент для планирования решения задачи по управлению запасами – по сути говоря это сложная математическая модель позволяющая решить задачу с учетом ограничений не на базе «писем экспертам», а на базе оцифрованных логичных правил, которые система превратит в решение. Зачастую такую модель называют системой «Автозаказ», не совсем верно, но отражает суть.Результатом данной модели является возможность проанализировать решение, увидеть вклад каждого ограничения в общий результат, а также влияние на затраты и сформулировать правильные приоритеты – факторная аналитика запасов и доступности и инструменты для анализа «Что, если?». Обычно в своей основе они нуждаются в данных из предшествующего блокаИнструмент коммуникации для трансляции этих приоритетов внутри компании (или вовне) – портал или другой инструмент для совместного обсуждения плана и внесения в него правок направленных на достижение приоритетных целей. Прежде чем предложить принять решение коллегам из соседних функций, следует узнать какие ограничения являются лучшими и почему. Результатом работы данного инструмента или метода является новая постановка задач для системы из первого пункта, чем образуется некая цикличность в использовании этих инструментов.Как правило принятие совместных решений существенно упрощает саму задачу, которую надо решить. Гораздо проще сбалансировать входящий поток, если о проблемах в конкретном регионе задумываются все службы, а не только логистика в «хвосте» поезда.ПослесловиеВ рамках данной статьи я ставил себе конкретную цель, ответить на самый частый вопрос, который я слышу от CEO, IT-директоров или Директоров по функции SCM в процессе диалога об их проблемах с завышенными стоками или недо-продажами, который они формулируют так как я написал выше:Что надо сделать, чтобы повысить точность прогноза для заказов?Бывают и другие вариации, которые могут иметь отличия в формулировке, но их суть сводится к тому, что задачу все видят, как простое повышение точности прогноза. К сожалению, сбор всей информации об ответе на этот вопрос занял очень большой объем статьи и потребовал сведения всей доказательной базы в одну точку. Но тут уместен вопрос – что же ты за эксперт, если не можешь дать простой ответ на сложный вопрос? Это не так, простой ответ на этот вопрос возможен – необходимо внедрить три ключевых инструмента Soft логистики в ежедневную работу.Однако специфика ответа такова, что он подразумевает под собой большие инвестиции времени и ресурсов в решение. Такой дорогой в реализации ответ как правило сбивает с толку тех, кто его слышит и им кажется, что нет очевидной взаимосвязи между вопросом и ответом. Тому, кто его задает, вероятно, хочется услышать свой ответ, что-то в стиле «надо внедрить умную модель на машинном обучении» и эта волшебная таблетка решит все проблемы. При этом задающие вопросы это опытные топ менеджеры, которые понимают, что волшебных таблеток не существует. Таблетка, в виде умной модели на машинном обучении, частично снимет симптомы, но очаги воспалений в организме вашей ритейловой компании не исчезнут.Попробуйте задать вопрос врачу в стиле «у меня систематически болит желудок, какую таблетку выпить чтобы он никогда не болел?». Если вы понимаете суть ответа на вопрос, то плохой врач скажет выпейте обезболивающее, а хороший врач скажет:пройдите диагностику, завершите назначенный курс лечения, исправьте привычки питания.Сложность диалога с топ-менеджерами еще в том, что они говорят не с врачом, которым люди склонны доверять в силу отсутствия медицинских знаний и практики, а с другим менеджером. Это хороший повод поставить его компетенцию под сомнение, так как весь практический опыт, полученный участниками диалога, развивался в аналогичных ролях и похожих компаниях. Когда общаются два эксперта с незначительными отличиями в опыте позицию каждого надо выражать не короткими рекомендациями, а развернуто и с большим количеством промежуточных выводов. Промежуточные выводы так же нужны, чтобы применить все методы, рассмотренные в статье не полностью и сразу, а применимо к той стадии, на которой уже находится компаний, то есть рассматривать статью не как конкретный ответ, а как руководство для самооценки текущей ситуации и выработки плана ближайших действий. В конкретный момент времени может потребоваться срочно внедрять новую систему, а где-то обойтись простым использованием Excel и эксперта.В любой точке принятия решений стоит в первую очередь сформулировать вопрос корректно, а согласно известной истине, правильно сформулированный вопрос содержит в себе большую часть ответа.",
  "Англофикация корейского видеорегистратора": "Всем привет! Попал ко мне в руки на постоянное использование корейский видеорегистратор, язык интерфейса в нем — Корейский, и никак поменять его нельзя. Не то что там нужно постоянно что то читать и нажимать, но хотелось чтобы интерфейс стал понятен. Итак, имеем видеорегистратор FineVu LX5000 power. В поисках русской или английской прошивки перерыл некоторое количество сайтов, но все безуспешно. На сайте производителя мне попадалась информация что для продуктов для рынка Кореи, прошивки с другими языками не предоставляются.Скачал с офф сайта Корейскую прошивку. Потом заметил что у офф сайта есть US версия, подумал что там тоже могут быть подобные видеорегистраторы. Нашел видеорегистратор который выглядит примерно так же как мой, это был GX5000. Скачал прошивку от него и начал изучать.Прошивка с расширением файла .bin легко открылась архиватором 7-zip, внутри была структура папок Linux системы. Структура папок в прошивкеНачал я искать папку с программой, которая рулит видеорегистратором. Нашел в /mnt/blackbox. В этой директории мне попался файл git-revision в котором был указан автор коммитов, попытался написать ему, но ответа нет по сей день. Ладно, продолжаем, в этой же директории нашел картинки, очень много картинок, прошивка занимает 80Мб, и львиная часть этого объема это картинки и звуки. Я так понял что инженеры компании производителя вообще не парились, и весь интерфейс нарезали картинками, даже информационные сообщения в картинках, вот пример:Как вы помните, до этого я же скачал английскую прошивку от видеорегистратора GX5000, в этой прошивке оказались почти такие же картинки, но уже с англ. надписями. Вытащил картинки и звуки, из каждого файла, и начал сравнивать, оказалось что большая часть картинок переведена, однако были и особенные пункты меню и кнопки, которые остались не переведенные, с этим помог переводчик. С помощью других сервисов определил что шрифт на картинках очень поход на Roboto Medium. Благодаря этому удалось сделать наиболее похожие к родным картинки с кнопками и пр.Дальше встал вопрос, как собрать прошивку с обновленными файлами, пробовал разные варианты с форматом iso, но все безуспешно. Решил что обратно все соберу просто заменой файлов в исходном файле. Попросил ChatGPT написать мне простенький скрипт на Python, который ищет последовательность байт в исходном файле, и заменяет нужной последовательностью байт. Если проще, то ищет в.bin файле картинку и заменяет ее английской версией картинки. Скрипт работал плохо, и пришлось его немного допилить. Тут же я увидел что некоторые картинки получились больше весом, поэтому они перетирают байты следующих файлов. Сначала сделал так: если файл с англ. текстом в байтах больше, чем файл с родным, корейским текстом, то просто сдвину байты в.bin файле. Но потом отказался от этой идеи, из за того что в прошивке есть симлинки, и даже если для симлинков это не страшно, есть еще причины почему лучше было по-максимуму сохранить структуру файла, но об этом позже. Как обходной вариант, придумал что могу ужать png и wav а разницу заменить нулями. Так и сделал. Пришлось повозиться конечно чтобы все заменяемые файлы были меньше, сложнее всего было с wav, пришлось немного порезать продолжительность дорожки.import os\n\ndef replace_and_adjust_bytes(file1_path, file2_path, file3_path):\n    try:\n        # Read bytes from file1\n        with open(file1_path, 'rb') as file1:\n            file1_bytes = file1.read()\n\n        # Read bytes from file2\n        with open(file2_path, 'rb') as file2:\n            file2_bytes = file2.read()\n\n        # Read bytes from file3\n        with open(file3_path, 'rb') as file3:\n            file3_bytes = bytearray(file3.read())\n\n        # Find the starting index of the file1 bytes in file3\n        start_index = file3_bytes.find(file1_bytes)\n\n        if start_index == -1:\n            print(f\"Error: File 1 bytes not found in {file3_path}.\")\n            return\n\n        # If file2 is larger than file1, we need to shift the bytes in file3\n        if len(file2_bytes) \u003e len(file1_bytes):\n            extra_bytes = len(file2_bytes) - len(file1_bytes)\n            # Shift bytes in file3 after the found sequence to accommodate extra bytes from file2\n            file3_bytes = file3_bytes[:start_index] + file2_bytes + file3_bytes[start_index + len(file1_bytes):]\n            print(f\"WARNING! file2 {file2_path} more than file1 {file1_path} make shift!\")\n        else:\n            # If file2 is smaller, replace and pad the difference with zeros\n            padding = len(file1_bytes) - len(file2_bytes)\n            file3_bytes[start_index:start_index + len(file1_bytes)] = file2_bytes + b'\\x00' * padding\n            #print(f\"File2 {file2_path} less than file1 {file1_path} make zeroes!\")\n\n        # Write the modified content back to file3\n        with open(file3_path, 'wb') as file3:\n            file3.write(file3_bytes)\n\n        #print(f\"Successfully replaced bytes of {file1_path} with {file2_path} in {file3_path}\")\n\n    except Exception as e:\n        print(f\"An error occurred while processing {file1_path}: {e}\")\n\ndef recursive_file_process(folder1, folder2, file3):\n    for dirpath1, _, filenames1 in os.walk(folder1):\n        # Generate corresponding folder2 path\n        relative_path = os.path.relpath(dirpath1, folder1)\n        dirpath2 = os.path.join(folder2, relative_path)\n\n        for filename in filenames1:\n            # Full paths for file1 and file2\n            file1_path = os.path.join(dirpath1, filename)\n            file2_path = os.path.join(dirpath2, filename)\n\n            # Only proceed if the file exists in both folder1 and folder2\n            if os.path.exists(file2_path):\n                replace_and_adjust_bytes(file1_path, file2_path, file3)\n            else:\n                print(f\"Skipping {filename}: File not found in {dirpath2}.\")\n\n# Example usage\nfolder1 = 'C:\\\\temp\\\\blackbox lx5000 orig' #Директория с родными Корейскими картинками\nfolder2 = 'C:\\\\temp\\\\blackbox lx5000' #Директория с англ. картинками\nfile3 = 'LX5000PWR_FW.bin'  # This is the single file in the folder with the script\n\nrecursive_file_process(folder1, folder2, file3)Дальше нужно было понять что в файле с прошивкой отвечает за версию и есть ли подписи, CRC или что-то такое. Открыл .bin файл в hex-editor и обнаружил в первых байтах название регистратора, и версию прошивки, сразу поднял версию прошивки. Начало файла оригинальной прошивкиПопробовал закинуть прошивку в видеорегистратор, он отказался принимать прошивку. Предположил что скорее всего какая то контрольная сумма поменялась, или я в процессе замены картинок что то еще зацепил. Продолжил изучение дальше, ушел в конец файла, там были 10 байт чем то заняты, возможно подпись или CRC подумал я. Как оно вычисляется я тогда не знал. Но файлы внутри прошивки то мне видны, начал искать что-то вроде updater или upgrader. Нашел несколько файлов sh и исполняемых, по sh вышел на файл bbupgrade, закинул его в ida, декомпилировал в псевдокод, и начал смотреть. Методы которые представляли интересУвидел там и проверку подписи и проверку файла.Благодаря выводам в консоль, намного проще разобраться что происходит на каждом этапе В очередной раз обратился к ChatGPT, попросил переписать этот псевдокод в c++, он переписал, но программа не работала. Как и сказал сам ChatGPT, псевдокод был для POSIX систем - там были методы которые есть только в linux. Не стал я искать чем заменить эти вызовы, просто перешел в linux, запустил код и подсунул ему оригинальный файл с прошивкой, код не сработал, потому что ChatGPT не учел некоторые касты, ну и вообще перемудрил. В общем немного посидел с напильником, код успешно запустился и выдал сообщение, что файл валидный. #include \u003ccstdio\u003e\n#include \u003ccstring\u003e\n#include \u003ccstdlib\u003e\n#include \u003ccerrno\u003e\n#include \u003csys/stat.h\u003e\n#include \u003cunistd.h\u003e\n#include \u003cfcntl.h\u003e\n#include \u003cregex.h\u003e\n#include \u003cstring.h\u003e\nusing namespace std;\n\nint FWUpgradeFileAnalyze(const char *a1, const char *a2)\n{\n    int fd;\n    ssize_t bytesRead;\n    struct stat buf;\n    unsigned char *ptr = nullptr;\n    int versionNew;\n    int s2[] = {-953237502, 1437226410, 44717482};\n    __int16_t v22 = 11968;\n    char s[128] = {0};\n    char name[128] = {0};\n    int currentVersion = 100000;\n    unsigned char checksum;\n\n    // Create the file path\n\n    // Check if the file exists\n    if (stat(a1, \u0026buf) \u003e= 0)\n    {\n        printf(\"[Firmware Check] %s - %ld\\n\", a1, buf.st_size);\n\n        // Allocate memory for the file content\n        ptr = (unsigned char *)malloc(buf.st_size);\n        if (ptr)\n        {\n            memset(ptr, 0, buf.st_size);\n\n            // Open the file\n            fd = open(a1, O_RDONLY);\n            if (fd \u003e= 0)\n            {\n                // Read the file\n                bytesRead = read(fd, ptr, buf.st_size);\n                if (bytesRead == buf.st_size)\n                {\n                    close(fd);\n\n                    // Validate the file content\n                    if (strlen((const char *)ptr) == 9 \u0026\u0026 strcmp((const char *)ptr, \"LX5000PWR\") == 0)\n                    {\n                        checksum = 255;\n                        for (int i = 0; i \u003c buf.st_size - 1; ++i)\n                        {\n                            checksum += ptr[i];\n                        }\n\n                        printf(\"[Firmware Check] checksum=%d checkF=%d\\n\", (unsigned char)~checksum, ptr[buf.st_size - 1]);\n\n                        if ((unsigned char)~checksum == ptr[buf.st_size - 1])\n                        {\n                            // Calculate the new version from the file content\n                            versionNew = 10 * (10 * (10 * (10 * (10 * (ptr[16] - '0') + ptr[18] - '0') + ptr[19] - '0') + ptr[21] - '0') + ptr[22] - '0') + ptr[23] - '0';\n\n                            printf(\"[Firmware Check] versionNew=%02x\\n\", versionNew);\n\n                            if (versionNew \u003e= currentVersion)\n                            {\n                                printf(\"[Firmware Check] OK(%s, %s(new:%d, curr:%d), %02x)\\n\",\n                                       (const char *)ptr, (const char *)ptr + 16, versionNew, currentVersion,\n                                       ptr[buf.st_blksize - 1]);\n                                free(ptr);\n                                return 1;\n                            }\n                            else\n                            {\n                                printf(\"[Firmware Check] Fail(%s, %s(new:%d, curr:%d), %02x)\\n\",\n                                       (const char *)ptr, (const char *)ptr + 16, versionNew, currentVersion,\n                                       ptr[buf.st_blksize - 1]);\n                            }\n                        }\n                        else\n                        {\n                            printf(\"[Firmware Check] Checksum Fail(%s, %s, %02x)\\n\",\n                                   (const char *)ptr, (const char *)ptr + 16, ptr[buf.st_blksize - 1]);\n                        }\n                    }\n                    else\n                    {\n                        printf(\"[Firmware Check] Sig Fail:\");\n                        for (int j = 0; j \u003c= 14; ++j)\n                            printf(\"%02x:%02x \", ((unsigned char *)s2)[j], ptr[buf.st_size - 16 + j]);\n                        putchar('\\n');\n                    }\n                    free(ptr);\n\n                    puts(\"[Firmware Check] Fail.\");\n                    sync();\n                    return 0;\n                }\n                else\n                {\n                    int err = errno;\n                    printf(\"[Firmware Check] read fail:%s, error:%d\\n\", s, err);\n                    free(ptr);\n                    close(fd);\n                    return 0;\n                }\n            }\n            else\n            {\n                printf(\"[Firmware Check] open fail:%s\\n\", s);\n                free(ptr);\n                return 0;\n            }\n        }\n        else\n        {\n            int err = errno;\n            printf(\"[Firmware Check] malloc fail:%d\\n\", err);\n            return 0;\n        }\n    }\n    else\n    {\n        int err = errno;\n        printf(\"[Firmware Check] stat fail:%s, error:%d\\n\", s, err);\n        return 0;\n    }\n}\n\nint main(int argc, char *argv[])\n{\n\n    FWUpgradeFileAnalyze(\"/opt/fw/LX5000PWR_FW_my.bin\", \"\");\n    return 1;\n}\n\nint FWUpgradeFileCheck(int a1)\n{\n    return -1;\n}\n\nint sub_10FA4(char *a1, char *a2)\n{\n    size_t v3;\n    char *s;\n    char *pattern;\n    size_t v7;\n    regmatch_t pmatch;\n    regex_t preg;\n\n    pattern = a1;\n    s = a2;\n    v7 = strlen(a2);\n\n    if (regcomp(\u0026preg, pattern, REG_EXTENDED) != 0) // Using REG_EXTENDED for flag '3' in regcomp\n        return 0;\n\n    v3 = strlen(s);\n\n    if (regexec(\u0026preg, s, 1, \u0026pmatch, 0) != 0) // Match single occurrence\n    {\n        regfree(\u0026preg);\n        return 0;\n    }\n    else\n    {\n        regfree(\u0026preg);\n        return 1;\n    }\n}\nОтлично, подумал я, уже финиш в одном метре от меня. Подсунул модифицированный файл с прошивкой, внутри которого были английские картинки, программа выдала мне что файл не валидный, и отобразила какая контрольная сумма должна быть в последнем байте, поменял в прошивке байт на указанный, запустил снова с++ код, программа выдала сообщение, что файл валидный. Супер.Довольный я принялся к установке прошивки в видеорегистратор. Закинул файл на флешку, запустил регистратор, и начал ждать... Первым появилось сообщение о наличии новой прошивки(если не совпадает сумма, подпись, да даже название файла, регистратор просто удаляет прошивку, и никаких сообщений не появляется), это был хороший знак, через несколько секунд начинается процесс установки прошивки. 2%...4%...6%...8% и за секунду счетчик с 8% перепрыгивает на 100% и видеорегистратор перезагружается...Процесс обновления прошивки Через несколько секунд он включается, и.... Все осталось по-корейски. Не получилось.  Предупреждение при запуске видеорегистратораЯ расстроился, и на несколько дней закинул идею. Но периодически я вспоминал и думал а как же так, ведь я все сделал чтобы прошивка принялась, что я мог не учесть.Решил сравнить файл с родной прошивкой с модифицированным файлом, и заметил что по всему файлу идет сдвиг на один байт. Этот сдвиг идет сразу после версии прошивки, т.е. я не аккуратно отредактировал версию.Разница в оригинальной и модифицированной прошивкеТут я уже понимал что 99% что проблема в этом сдвиге. Исправляю эту нелепую ошибку, закидываю прошивку на флэшку и включаю видеорегистратор. Начался процесс прошивки 2%...4%...6%...8%...10%...12%.... доходит до 100%, видеорегистратор перезагружается, иии...Предупреждение при запуске видеорегистратора на англ. языкеВсе получилось, теперь у видеорегистратора интерфейс англофицирован.Меню на английском языкеСпасибо за внимание!P.S. Никого не призываю к реверс инжинирингу, и проделыванию подобных манипуляций с техникой. Все что вы делаете, вы делаете на свой страх и риск.",
  "Внедрение электронной цифровой подписи в мобильное приложение на Android": "Привет, Хабр! Сегодня мы хотим поделиться решением интересной и новой для нас задачи: нужно встроить поддержу ЭЦП в мобильное приложение заказчика.Основные принципы и тезисыЭлектронная цифровая подпись — это криптографический механизм, который обеспечивает:Подлинность: позволяет удостовериться, что отправитель является именно тем, за кого себя выдает. В мире физических документов это аналогично проверке подписи      на бумаге, которую может поставить только конкретный человек.Целостность: гарантирует, что данные не были изменены после подписания. Если документ был изменен после подписания, подпись станет недействительной.Безотказность: автор подписи не может впоследствии отказаться от своих действий, утверждая, что он не подписывал документ.Всё это достигается благодаря использованию парных ключей: открытого и закрытого. Закрытый ключ хранится на защищенных носителях, а открытый распространяется вместе с данными, которые он подписывает. Закрытый или Приватный ключ равносилен личной подписи от руки.На этапе подписи данные хэшируются (то есть превращаются в уникальную \"цифровую подпись\"), которая шифруется закрытым ключом отправителя. Получатель может использовать открытый ключ отправителя, чтобы расшифровать подпись и сверить хэш с вновь сгенерированным из полученных данных. Несовпадение укажет на изменение данных.Простой пример: Вы отправляете коробку с вложенным списком содержимого и запечатанную лентой с уникальным рисунком. Получатель открывает коробку, сверяет содержимое со списком и убеждается, что лента не повреждена.Поскольку только отправитель может использовать свой закрытый ключ, чтобы создать подпись, он не может отказаться от авторства подписанного сообщения или транзакции; подписанная информация связывается именно с их идентичностью через уникальный ключ.Простой пример: Ваша подпись на бумажном документе в силе до тех пор, пока никто другой не смог её подделать, что крайне маловероятно, как и кража закрытого ключа при правильном его хранении.Ключи и сертификаты:Закрытый ключ — это секретная часть пары ключей, которым подпись создается и который должен храниться в максимально защищенных условиях.Открытый  ключ — это общедоступная информация, необходимая для проверки      подписи, связанная с вашим закрытым ключом.Сертификат — это документ, удостоверяющий связь открытого ключа с именно тем, кому он принадлежит, подтвержденный надежным удостоверяющим центром (ЦС).      Сертификат помогает другим доверять вашему открытому ключу.Эта система позволяет поддерживать безопасность и доверие в цифровой коммуникации, что критически важно для мобильных приложений, обрабатывающих чувствительные и юридически значимые данные.Внедрение ЭЦП в приложениеТеперь, когда мы вспомнили основные термины и принципы, давайте перейдем к рассмотрению нашего кейса более детально. Бизнес-процесс подразумевает формирование неких документов в мобильном приложении с дальнейшей их отправкой на сервер и последующей обработкой. Поскольку документы без подписи не имеют юридической силы, пользователям заказчика приходилось распечатывать созданные документы для их подписания вручную. Вот этот процесс и было решено автоматизировать с целью экономии времени и ресурсов. После выяснения всех обстоятельств, средством доставки сертификатов и приватного ключа был выбран носитель Рутокен Lite. Более детально со всем разнообразием подобных носителей и их отличий вы можете ознакомится на официальной странице техподдержки Рутокен. Провайдером же был выбран КриптоПро CSP. Такой выбор был сделан потому, что именно это сочетание обладает достаточной безопасностью, а также обеспечивает юридическую силу документов на территории Российской Федерации.Для начала нам нужно организовать подключение к Рутокену. Делается это несложно. Сперва добавим библиотеку rtpcscbridge. Для этого пропишем такую зависимость в нашем build.gradle файле:implementation 'ru.rutoken.rtpcscbridge:rtpcscbridge:1.2.0'Также нам нужно проинициализировать зависимость. Для этого добавим следующие строки в метод onCreate() нашего Application,передадим контекст приложения и присоединим его к жизненному циклу:RtPcscBridge.setAppContext(this)\nRtPcscBridge.getTransportExtension().attachToLifecycle(this, true)Далее нам необходимо подключить и настроить криптопровайдер для осуществления криптографических действий. Конкретно в нашем случае, это было копирование контейнера с ЭЦП на устройство, чтобы пользователям не приходилось постоянно держать Рутокен подключенным к устройству. Главная цель — подписание pdf-файлов. Разомнем пальцы и приступим.Для начала настоятельно рекомендуем внимательно ознакомится с информацией по ссылке. Если кратко, то мы скачиваем с официального сайта упомянутые в руководстве инструменты и примеры. В принципе, демонстрационное приложение от КриптоПро достаточно хорошо показывает широкий функционал возможностей для реализации. Однако, на наш взгляд, оно уже серьезно устарело, да и написано на Java, а у нас весь проект на Kotlin. Поэтому в данной статье мы будем приводить примеры обновленных нами функций, а оригиналы вы  знаете, где найти.Наша первая задача после сборки и настройки проекта — это правильно проинициализировать библиотеки криптошифрования. После успешной инициализации проверим наличие уже сохраненных контейнеров на устройстве. Согласно архитектуре нашего приложения, осуществить это было удобнее всего в UseCase.init {\n    initCSPProviders()\n}\n\n\nclass InitError @JvmOverloads constructor(\n    val errorCode: Int,\n    val errorMessage: String? = null\n) {\n    companion object {\n        const val INIT_JAVA_PROVIDER_ERROR: Int = 0xff\n    }\n}\n\n// Инициализация CSP провайдеров\nprivate fun initCSPProviders() {\n    initCSPProvidersFuture = CompletableFuture.runAsync {\n        val initCode = CSPConfig.init(context)\n        if (initCode == CSPConfig.CSP_INIT_OK) {\n            initJavaProviders(context, false)\n        }\n        initResult.postValue(InitError(initCode))\n    }.thenRun {\n        val mainHandler = Handler(Looper.getMainLooper())\n        mainHandler.post {\n            if (initResult.value?.errorCode == 0) {\n                checkContainersOnDevice()\n            }\n        }\n    }.exceptionally { throwable: Throwable -\u003e\n        Timber.e(\n            throwable,\n            \"InitError(code = %s ,message = %s)\",\n            InitError.INIT_JAVA_PROVIDER_ERROR,\n            throwable.message\n        )\n        initResult.postValue(\n            InitError(\n                InitError.INIT_JAVA_PROVIDER_ERROR,\n                throwable.message\n            )\n        )\n        null\n    }\n}\n\nprivate fun initJavaProviders(context: Context, useSSPITlsProvider: Boolean) {\n    if (Security.getProvider(JCSP.PROVIDER_NAME) == null)\n        Security.addProvider(JCSP())\n\n    Security.setProperty(\"ssl.KeyManagerFactory.algorithm\", \"GostX509\")\n    Security.setProperty(\"ssl.TrustManagerFactory.algorithm\", \"GostX509\")\n    Security.setProperty(\n        \"ssl.SocketFactory.provider\",\n        if (useSSPITlsProvider) \"ru.CryptoPro.sspiSSL.SSLSocketFactoryImpl\" else \"ru.CryptoPro.ssl.SSLSocketFactoryImpl\"\n    )\n    Security.setProperty(\n        \"ssl.ServerSocketFactory.provider\",\n        if (useSSPITlsProvider) \"ru.CryptoPro.sspiSSL.SSLServerSocketFactoryImpl\" else \"ru.CryptoPro.ssl.SSLServerSocketFactoryImpl\"\n    )\n    if (Security.getProvider(\"JTLS\") == null) {\n        if (useSSPITlsProvider) Security.addProvider(SSPISSL())\n        else Security.addProvider(ru.CryptoPro.ssl.Provider())\n    }\n\n    cpSSLConfig.setDefaultSSLProvider(JCSP.PROVIDER_NAME)\n\n    if (Security.getProvider(RevCheck.PROVIDER_NAME) == null)\n        Security.addProvider(RevCheck())\n\n    System.setProperty(\"ru.CryptoPro.CAdES.validate_tsp\", \"false\")\n    System.setProperty(\"com.sun.security.crl.timeout\", \"5\")\n    System.setProperty(\"ru.CryptoPro.crl.read_timeout\", \"5\")\n    AdESConfig.setDefaultProvider(JCSP.PROVIDER_NAME)\n    System.setProperty(\"xml_xxe_protected\", \"false\")\n    XmlInit.init()\n    ResourceResolver.registerAtStart(XmlInit.JCP_XML_DOCUMENT_ID_RESOLVER)\n    val xmlDSigRi: Provider = XMLDSigRI()\n    Security.addProvider(xmlDSigRi)\n    val provider = Security.getProvider(\"XMLDSig\")\n    if (provider != null) {\n        provider[\"XMLSignatureFactory.DOM\"] =\n            \"ru.CryptoPro.JCPxml.dsig.internal.dom.DOMXMLSignatureFactory\"\n        provider[\"KeyInfoFactory.DOM\"] =\n            \"ru.CryptoPro.JCPxml.dsig.internal.dom.DOMKeyInfoFactory\"\n    }\n    System.setProperty(\"com.sun.security.enableCRLDP\", \"true\")\n    System.setProperty(\"com.ibm.security.enableCRLDP\", \"true\")\n    System.setProperty(\"disable_default_context\", \"true\")\n    System.setProperty(\"ngate_set_jcsp_if_gost\", \"true\")\n    System.setProperty(\"ru.CryptoPro.key_agreement_validation\", \"false\")\n    val trustStorePath = getBksTrustStore(context)\n    val trustStorePassword = String(BKSTrustStore.STORAGE_PASSWORD)\n    System.setProperty(\"javax.net.ssl.trustStoreType\", BKSTrustStore.STORAGE_TYPE)\n    System.setProperty(\"javax.net.ssl.trustStore\", trustStorePath)\n    System.setProperty(\"javax.net.ssl.trustStorePassword\", trustStorePassword)\n}\n\nfun checkContainersOnDevice() {\n    if (initResult.value!!.errorCode == 0) {\n        val aliases =\n            getAliasesOnStore(HDIMAGE, AlgorithmSelector.DefaultProviderType.pt2012Short)\n        val updatedList = mutableListOf\u003cCertificateDetails\u003e()\n        aliases.forEach { alias -\u003e\n            getCertificateDetails(alias = alias, storeType = HDIMAGE)?.let { cert -\u003e\n                if (userName == cert.subjectFIO)\n                    updatedList.add(cert)\n            }\n        }\n        if (updatedList.size == 1 \u0026\u0026 updatedList[0].validTo.after(Date()))\n            setActiveCertificate(updatedList[0])\n        certsOnDevice.postValue(updatedList)\n\n        val activeAlias = sharedPref.getString(\"ACTIVE_SIGN_SP_$userId\", null)\n        if (activeAlias != null) {\n            getCertificateDetails(alias = activeAlias, storeType = HDIMAGE)?.let { cert -\u003e\n                activeCertificate.postValue(cert)\n            }\n        }\n    }\n}\nprivate fun getBksTrustStore(context: Context): String {\n    return context.applicationInfo.dataDir + File.separator + BKSTrustStore.STORAGE_DIRECTORY + File.separator + BKSTrustStore.STORAGE_FILE_TRUST\n}\n\nfun clear() {\n    initCSPProvidersFuture?.cancel(true)\n}После того, как провайдер готов к работе, добавляем инициализацию и слушателя на подключение носителя, чтобы мы могли считать и сохранить информацию с Рутокен:private lateinit var rtTransport: RtTransport\nprivate var readerObserver: RtTransport.PcscReaderObserver? = null\n\nfun initRutoken() {\n        try {\n            rtTransport = RtPcscBridge.getTransport()\n            rtTransport.initialize(context) \n            // Создаем и добавляем наблюдатель\n            readerObserver = object : RtTransport.PcscReaderObserver {\n                @RequiresApi(Build.VERSION_CODES.O)\n                override fun onReaderAdded(reader: RtTransport.PcscReader) {\n                    Timber.d(\"Reader added: ${reader.name}\")\n                    readRutoken(reader)\n                }\n\n                override fun onReaderRemoved(reader: RtTransport.PcscReader) {\n                    Timber.d(\"Reader removed: ${reader.name}\")\n                }\n            }\n            rtTransport.addPcscReaderObserver(readerObserver!!)\n        } catch (e: Exception) {\n            Timber.e(e, \"initRutoken Error\")\n        }\n    }\n\n\t//Функция считывающая все алиасы контейнеров на носителе\n    private fun readRutoken(reader: RtTransport.PcscReader) {\n        val aliases = getAliasesOnStore(\n            reader.name,\n            AlgorithmSelector.DefaultProviderType.pt2012Short\n        )\n        aliases.forEach { alias -\u003e\n            val certDetailedInfo =\n                getCertificateDetails(alias = alias, storeType = reader.name)\n            if (certDetailedInfo != null) {\n                activeCertificate.postValue(certDetailedInfo)\n\n                if (userName == certDetailedInfo.subjectFIO) {\n                    launch {\n                        try {\n                            onContainerCopyResult.postValue(\n                                createContainerOnDevice(\n                                    certDetailedInfo.alias,\n                                    certDetailedInfo.privateKey!!,\n                                    certDetailedInfo.certificate\n                                )\n                            )\n                        } catch (e: Exception) {\n                            Timber.e(e, \"Ошибка копирования сертификата\")\n                            onContainerCopyResult.postValue(\"Ошибка копирования сертификата: ${e.message}\")\n                        }\n                    }\n                } else {\n                    onContainerCopyResult.postValue(\n                        \"Нельзя сохранить сертификат. ФИО пользователя и владельца ЭЦП не совпадают.\\n\" +\n                            \"ФИО пользователя: ${inspectorService.cachedInspectorProfile?.user?.inspectorName}\\n\" +\n                            \"ФИО владельца ЭЦП: ${certDetailedInfo.subjectFIO}\"\n                    )\n                }\n            }\n        }\n    }\n\n    fun stopRutoken() {\n        readerObserver?.let { rtTransport.removePcscReaderObserver(it) }\n        readerObserver = null\n    }Здесь мы инициализируем слушатель подключения носителя, а также считываем данные при его подключении с помощью библиотек КрипПро CSP и нескольких утилитарных функций, которые будут приведены ниже. Функция stopRutoken() останавливает мониторинг подключения, так как данный функционал нужен исключительно в одном месте приложения.object KeyStoreUtil {\n\n    private const val STR_CMS_OID_SIGNED = \"1.2.840.113549.1.7.2\";\n    private const val STR_CMS_OID_DATA = \"1.2.840.113549.1.7.1\";\n\n    private val providerType = AlgorithmSelector.DefaultProviderType.pt2012Short\n\n    fun getAliasesOnStore(\n        storeType: String,\n        providerType: AlgorithmSelector.DefaultProviderType\n    ): List\u003cString\u003e {\n        val aliasesList = mutableListOf\u003cString\u003e()\n\n        try {\n            val keyStore = KeyStore.getInstance(storeType, JCSP.PROVIDER_NAME)\n            keyStore.load(null, null)\n            val aliases = keyStore.aliases()\n\n            while (aliases.hasMoreElements()) {\n                val alias = aliases.nextElement()\n                val cert = keyStore.getCertificate(alias) as? X509Certificate\n                val key = keyStore.getKey(alias, null)\n\n                if (cert != null) {\n                    val keyAlgorithm = cert.publicKey.algorithm\n\n                    if (providerType == AlgorithmSelector.DefaultProviderType.pt2001 \u0026\u0026\n                        keyAlgorithm.equals(JCP.GOST_EL_DEGREE_NAME, ignoreCase = true)\n                    ) aliasesList.add(alias)\n                    else if (providerType == AlgorithmSelector.DefaultProviderType.pt2012Short \u0026\u0026\n                        keyAlgorithm.equals(JCP.GOST_EL_2012_256_NAME, ignoreCase = true)\n                    ) aliasesList.add(alias)\n                    else if (providerType == AlgorithmSelector.DefaultProviderType.pt2012Long \u0026\u0026\n                        keyAlgorithm.equals(JCP.GOST_EL_2012_512_NAME, ignoreCase = true)\n                    ) aliasesList.add(alias)\n\n                }\n            }\n        } catch (e: Exception) {\n            Timber.e(e, \"getAliasesOnStore Error: ${e.message}\")\n        }\n\n        return aliasesList\n    }\n\n    // Создает контейнер на устройстве и копирует в него ключи с Рутокена\n    fun createContainerOnDevice(alias: String, privateKey: PrivateKey, certificate: Certificate):\n        String {\n        try {\n            // Пароль для контейнера\n            val password = \"\".toCharArray()\n            val storeType = HDIMAGE\n\n            if (checkAliasExists(alias, storeType)) {\n                Timber.e(\"Container $alias already exists !!! \")\n                return \"Контейнер $alias уже был скопирован ранее!\"\n            }\n\n            val keyStore = KeyStore.getInstance(storeType, JCSP.PROVIDER_NAME)\n            keyStore.load(null, null)\n            val entry = JCPPrivateKeyEntry(privateKey, arrayOf(certificate))\n            val protectedParam = JCPProtectionParameter(password)\n            keyStore.setEntry(alias, entry, protectedParam)\n            if (keyStore.containsAlias(alias)) {\n                Timber.i(\"Container created successfully with alias: $alias\")\n                getAliasesOnStore(storeType, providerType)\n                return \"Контейнер $alias успешно скопирован!\"\n            } else {\n                Timber.e(\"Failed to create container with alias: $alias\")\n                return \"Не удалось скопировать контейнер $alias!\\n\" +\n                    \"Пожалуйста, обратитесь к администратору!\"\n            }\n\n        } catch (e: Exception) {\n            Timber.e(e, \"Error creating container on device\")\n            return \"Не удалось скопировать контейнер $alias!\\n\" +\n                \"Ошибка: ${e.message}\"\n        }\n    }\n\n    private fun checkAliasExists(alias: String, storeType: String): Boolean {\n        return getAliasesOnStore(storeType, providerType).contains(alias)\n    }\n\n    private fun extractInn(input: String): String? {\n        // Регулярное выражение для поиска значения после \"1.2.643.3.131.1.1=\"\n        val regex = \"\"\"1\\.2\\.643\\.3\\.131\\.1\\.1=([^,/]+)\"\"\".toRegex()\n        val matchResult = regex.find(input)\n        return matchResult?.groups?.get(1)?.value\n    }\n\n    private fun extractSnils(input: String): String? {\n        // Регулярное выражение для поиска значения после \"1.2.643.100.3=\"\n        val regex = \"\"\"1\\.2\\.643\\.100\\.3=([^,/]+)\"\"\".toRegex()\n        val matchResult = regex.find(input)\n        return matchResult?.groups?.get(1)?.value\n    }\n\n    private fun extractValue(docType: String, searchIn: String): String? {\n        val regex =\n            when (docType) {\n                \"СНИЛС\" -\u003e \"\"\"1\\.2\\.643\\.100\\.3=([^,/]+)\"\"\".toRegex()\n                \"ИНН\" -\u003e \"\"\"1\\.2\\.643\\.3\\.131\\.1\\.1=([^,/]+)\"\"\".toRegex()\n                \"ОГРН\" -\u003e \"\"\"1\\.2\\.643\\.100\\.1=([^,/]+)\"\"\".toRegex()\n                \"ИННЮЛ\" -\u003e \"\"\"1\\.2\\.643\\.100\\.4=([^,/]+)\"\"\".toRegex()\n                \"EMAILADDRESS\" -\u003e \"\"\"EMAILADDRESS=([^,/]+)\"\"\".toRegex()\n                else -\u003e return null\n            }\n        val matchResult = regex.find(searchIn)\n        return matchResult?.groups?.get(1)?.value\n    }\n\n    private fun getDocPattern(docType: String): Regex {\n        return if (docType == \"EMAILADDRESS\")\n            \"\"\"EMAILADDRESS=#16[0-9A-Fa-f]+(?=,|$)\"\"\".toRegex()\n        else\n            \"\"\"$docType=#12[0-9A-Fa-f]+(?=,|$)\"\"\".toRegex()\n    }\n\n    fun getCertificateDetails(alias: String, storeType: String): CertificateDetails? {\n        try {\n            val keyStore = KeyStore.getInstance(storeType, JCSP.PROVIDER_NAME)\n            keyStore.load(null, null)\n            val certificate = keyStore.getCertificate(alias) as? X509Certificate ?: return null\n            val privateKey = keyStore.getKey(alias, null) as? PrivateKey ?: return null\n\n            val certificateString = certificate.toString().trimIndent()\n\n            val snils = \"СНИЛС=${extractSnils(certificateString)}\"\n            val inn = \"ИНН=${extractInn(certificateString)}\"\n\n            val subject = certificate.subjectDN.toString()\n                .replace(\"OID.1.2.643.100.3\", \"СНИЛС\")\n                .replace(\"OID.1.2.643.3.131.1.1\", \"ИНН\")\n                .replace(getDocPattern(\"СНИЛС\")) { snils }\n                .replace(getDocPattern(\"ИНН\")) { inn }\n                .replace(\"EMAILADDRESS\", \"E\")\n\n            val ogrn = \"ОГРН=${extractValue(\"ОГРН\", certificateString)}\"\n            val innYl = \"ИННЮЛ=${extractValue(\"ИННЮЛ\", certificateString)}\"\n            val mail = \"E=${extractValue(\"EMAILADDRESS\", certificate.issuerDN.toString())}\"\n\n            val issuer = certificate.issuerDN.name.toString()\n                .replace(\"1.2.643.100.1\", \"ОГРН\")\n                .replace(\"1.2.643.100.4\", \"ИННЮЛ\")\n                .replace(\"1.2.840.113549.1.9.1\", \"EMAILADDRESS\")\n                .replace(getDocPattern(\"ОГРН\")) { ogrn }\n                .replace(getDocPattern(\"ИННЮЛ\")) { innYl }\n                .replace(getDocPattern(\"EMAILADDRESS\")) { mail }\n                .replace(\",\", \", \")\n                .replace(\"  \", \" \")\n\n            val serialNumber = certificate.serialNumber.toString(16).uppercase().padStart(34, '0')\n            val signatureAlgorithm = certificate.sigAlgName\n            val validFrom = certificate.notBefore\n            val validTo = certificate.notAfter\n            val publicKeyAlgorithm = certificate.publicKey.algorithm\n\n            return CertificateDetails(\n                alias = alias,\n                certificate = certificate,\n                privateKey = privateKey,\n                issuer = issuer,\n                subject = subject,\n                subjectFIO = extractCommonName(subject),\n                serialNumber = serialNumber,\n                signatureAlgorithm = signatureAlgorithm,\n                validFrom = validFrom,\n                validTo = validTo,\n                publicKeyAlgorithm = publicKeyAlgorithm\n            )\n\n        } catch (e: Exception) {\n            Timber.e(e, \"@@@ Error extracting certificate details for alias: $alias\")\n        }\n        return null\n    }\n\n    private fun extractCommonName(subject: String): String {\n        val regex = Regex(\"CN=([^,]+)\")\n        val matchResult = regex.find(subject)\n        return matchResult?.groupValues?.get(1) ?: \"Неизвестно\"\n    }\n\n    @Throws(java.lang.Exception::class)\n    fun createSign(\n        dataForSign: ByteArray,\n        keys: Array\u003cPrivateKey\u003e,\n        certs: Array\u003cCertificate\u003e,\n        providerType: AlgorithmSelector.DefaultProviderType\n    ): ByteArray {\n\n        val all = ContentInfo()\n        all.contentType = Asn1ObjectIdentifier(\n            OID(STR_CMS_OID_SIGNED).value\n        )\n\n        val cms = SignedData()\n        all.content = cms\n        cms.version = CMSVersion(1)\n\n\n        val algorithmSelector = AlgorithmSelector.getInstance(providerType)\n        cms.digestAlgorithms = DigestAlgorithmIdentifiers(1)\n        val a = DigestAlgorithmIdentifier(\n            OID(algorithmSelector.digestAlgorithmOid).value\n        )\n        a.parameters = Asn1Null()\n        cms.digestAlgorithms.elements[0] = a\n\n        cms.encapContentInfo = EncapsulatedContentInfo(\n            Asn1ObjectIdentifier(\n                OID(STR_CMS_OID_DATA).value\n            ), null\n        )\n\n        // Сертификаты.\n\n        val nCerts = certs.size\n        cms.certificates = CertificateSet(nCerts)\n        cms.certificates.elements = arrayOfNulls(nCerts)\n\n        for (i in cms.certificates.elements.indices) {\n            val certificate =\n                ru.CryptoPro.JCP.ASN.PKIX1Explicit88.Certificate()\n\n            val decodeBuffer =\n                Asn1BerDecodeBuffer(certs[i].encoded)\n\n            certificate.decode(decodeBuffer)\n            cms.certificates.elements[i] = CertificateChoices()\n            cms.certificates.elements[i].set_certificate(certificate)\n        }\n\n\n        val signature = Signature.getInstance(\n            algorithmSelector.signatureAlgorithmName\n        )\n\n        var sign: ByteArray?\n\n        // Подписанты (signerInfos).\n\n        val nSigners = keys.size\n        cms.signerInfos = SignerInfos(nSigners)\n        for (i in cms.signerInfos.elements.indices) {\n\n            cms.signerInfos.elements[i] = SignerInfo()\n            cms.signerInfos.elements[i].version = CMSVersion(1)\n            cms.signerInfos.elements[i].sid = SignerIdentifier()\n\n            val encodedName = (certs[i] as X509Certificate)\n                .issuerX500Principal.encoded\n\n            val nameBuf =\n                Asn1BerDecodeBuffer(encodedName)\n\n            val name = Name()\n            name.decode(nameBuf)\n\n            val num = CertificateSerialNumber(\n                (certs[i] as X509Certificate).serialNumber\n            )\n            cms.signerInfos.elements[i].sid.set_issuerAndSerialNumber(\n                IssuerAndSerialNumber(name, num)\n            )\n\n            cms.signerInfos.elements[i].digestAlgorithm =\n                DigestAlgorithmIdentifier(\n                    OID(algorithmSelector.digestAlgorithmOid).value\n                )\n\n            cms.signerInfos.elements[i].digestAlgorithm.parameters = Asn1Null()\n\n            val keyAlgOid = AlgorithmUtility.keyAlgToKeyAlgorithmOid(\n                keys[i].algorithm\n            ) // алгоритм ключа подписи\n\n            cms.signerInfos.elements[i].signatureAlgorithm =\n                SignatureAlgorithmIdentifier(OID(keyAlgOid).value)\n\n            cms.signerInfos.elements[i].signatureAlgorithm.parameters = Asn1Null()\n\n            val data2hash: ByteArray = dataForSign\n\n            signature.initSign(keys[i])\n            signature.update(data2hash)\n            sign = signature.sign()\n\n            cms.signerInfos.elements[i].signature = SignatureValue(sign)\n        }\n\n\n        // CMS подпись.\n        val asnBuf = Asn1BerEncodeBuffer()\n        all.encode(asnBuf, true)\n        val sig = asnBuf.msgCopy\n        return sig\n    }\n}/**\n * Служебный класс AlgorithmSelector предназначен\n * для получения алгоритмов и свойств, соответствующих\n * заданному провайдеру.\n */\nopen class AlgorithmSelector protected constructor(\n    val providerType: DefaultProviderType,\n    val signatureAlgorithmName: String,\n    val digestAlgorithmName: String,\n    val digestAlgorithmOid: String\n) {\n    /**\n     * Возможные типы провайдеров.\n     */\n    enum class DefaultProviderType { pt2001, pt2012Short, pt2012Long }\n\n    companion object {\n        /**\n         * Получение списка алгоритмов для данного провайдера.\n         *\n         * @param pt Тип провайдера.\n         * @return настройки провайдера.\n         */\n        fun getInstance(pt: DefaultProviderType): AlgorithmSelector {\n            Timber.d(\"@@@ getInstance($pt)\")\n            return when (pt) {\n                DefaultProviderType.pt2001 -\u003e AlgorithmSelector_2011()\n                DefaultProviderType.pt2012Short -\u003e AlgorithmSelector_2012_256()\n                DefaultProviderType.pt2012Long -\u003e AlgorithmSelector_2012_512()\n            }\n        }\n\n        /**\n         * Получение типа провайдера по его строковому представлению.\n         *\n         * @param value Тип в виде числа.\n         * @return тип в виде значения из перечисления.\n         */\n        @JvmStatic\n        fun find(value: Int): DefaultProviderType {\n            Timber.d(\"@@@ find($value)\")\n            return when (value) {\n                0 -\u003e DefaultProviderType.pt2001\n                1 -\u003e DefaultProviderType.pt2012Short\n                2 -\u003e DefaultProviderType.pt2012Long\n                else -\u003e throw IllegalArgumentException(\"Unknown value\")\n            }\n        }\n    }\n}\n//------------------------------------------------------------------------------------------------------------------\n\n/**\n * Класс с алгоритмами ГОСТ 2001.\n */\nprivate class AlgorithmSelector_2011 : AlgorithmSelector(\n    DefaultProviderType.pt2001,\n    JCP.GOST_EL_SIGN_NAME,\n    JCP.GOST_DIGEST_NAME,\n    JCP.GOST_DIGEST_OID\n)\n\n/**\n * Класс с алгоритмами ГОСТ 2012 (256).\n */\nprivate class AlgorithmSelector_2012_256 : AlgorithmSelector(\n    DefaultProviderType.pt2012Short,\n    JCP.GOST_SIGN_2012_256_NAME,\n    JCP.GOST_DIGEST_2012_256_NAME,\n    JCP.GOST_DIGEST_2012_256_OID\n)\n\n/**\n * Класс с алгоритмами ГОСТ 2012 (512).\n */\nprivate class AlgorithmSelector_2012_512 : AlgorithmSelector(\n    DefaultProviderType.pt2012Long,\n    JCP.GOST_SIGN_2012_512_NAME,\n    JCP.GOST_DIGEST_2012_512_NAME,\n    JCP.GOST_DIGEST_2012_512_OID\n)На этом всё, ЭЦП успешно внедрена в мобильное приложение на Android. Надеемся, что данная статья будет полезна всем, кто столкнется с подобной задачей! Удачи в разработке!",
  "Гаджет-пластырь от супербактерий": "Бесконтрольный доступ к антибиотикам породил глобальную проблему для человечества, которая грозит отбросить медицину в эпоху средневековья. Все дело в том, что «недолеченные» пациенты становятся идеальной базой для созревания супербактерий. Бактерий, устойчивых к последующим порциям антибиотиков и заражающих все подряд. И спасения от них нет. По крайней мере, не наблюдалось в ближайшей перспективе. Суть инновации в борьбе с супербактериямиПоскольку все больше бактерий, вызывающих заболевания, становятся все более устойчивыми к антибиотикам, человечеству нужны принципиально другие способы борьбы. Одно из решений кроется в электрических разрядах, правильная подача которых мешает бактериям проникать через нашу кожу.Прорыв совершили исследователи из Чикагского университета и Калифорнийского университета в Сан-Диего. В статье, опубликованной в журнале Device на этой неделе, команда подробно описала, как устройство снижает активность вредоносных генов в инфекционных бактериях и сдерживает рост популяций.Исследование было успешно проведено с использованием бактерии Staphylococcus epidermidis на коже свиньи. Это типичный разносчик «больничных инфекций», бактерия, которая размножается именно в больницах, приобретая устойчивость к широкому спектру антисептиков. Поэтому, если новое решение безопасно для человека и воздействует на различные типы бактериальных инфекций, то это буквально мини-революция в области медицины.Более того, роль бактерий и микробиома в целом – опасно недооценивать. Наш образ жизни и питание напрямую могут влиять на то, как микрофлора кишечника провоцирует развитие депрессии.Принцип борьбы с супербактериямиВ ходе исследования ученые обнаружили, что некоторые бактерии были «избирательны» в отношении среды, в которой они проявляют возбудимость. То есть, поведение бактерий зависело от того, какая атмосфера царила вокруг них. В частности, Staphylococcus epidermidis становятся возбудимыми под здоровой кожей, которая представляет собой кислую среду.Вооружившись этими знаниями, ученые приступили к практическому изучению потенциала технологии, используя кожу свиней, гидрогель для воссоздания кислой среды и гибкий пластырь. Сам процесс получил название как: Биоэлектронная Локализованная Антимикробная Стимулирующая ТЕРапия, или — БЛАСТЕР. В этом исследовании биоэлектронный пластырь был прикреплен к свиной коже вместе с гидрогелем, который поддерживал кислую среду для возбудимости бактерий. Чикагский университет/Калифорнийский университет в Сан-ДиегоПринцип лечения прост: подача слабого электрического сигнала в 1,5 вольта — намного ниже безопасного предела в 15 вольт для человека — в течение 10 секунд каждые 10 минут в течение 18 часов через пластырь. Это остановило 99% скоплений бактерий, которые образуются для блокировки лекарств и вызывают инфекции. Также на обработанном образце кожи было почти в 10 раз меньше S. epidermidis по сравнению с необработанным образцом коже после 18-часового цикла.Перспективы технологииЭто может стать большой победой в лечении инфекций еще по нескольким причинам. Помимо работы с бактериями, устойчивыми к антибиотикам, технология может свести на нет необходимость в антибиотиках для обычных видов инфекций. Технологию можно использовать в отдаленных районах, куда сложно доставлять лекарства на регулярной основе. Кроме того, небольшой пластырь позволяет проводить целенаправленное лечение на небольшом участке кожи, что снижает вероятность потенциальных побочных эффектов. Как и следовало ожидать, в этой области предстоит еще много работы.Открытие этой избирательной возбудимости поможет нам узнать, как контролировать другие виды бактерий, рассматривая различные условия.Соавтор, Сэхён Ким из Чикагского университета.Старший соавтор Божи Тянь, также из Чикагского университета, объяснил, что есть возможность разработать пластырь с беспроводной цепью для контроля инфекций без лекарств, а также дополнительно оценить эффективность подобного лечения.По данным Всемирной организации здравоохранения, устойчивость бактерий к антибиотикам стала прямой причиной 1,27 миллиона смертей во всем мире в 2019 году. Данное лечение может потенциально помочь предотвратить серьезные заболевания, вызванные бактериями, устойчивыми к антибиотикам, и спасти жизни.Больше материалов про тонкую грань между наукой и потенциалом человечества, про наше сознание, мозг и перспективы слияния с технологиями, или про расширение возможностей тела и разума – читайте в материалах сообщества. Подписывайтесь, чтобы не пропустить свежие статьи!",
  "Дорожная карта теории вероятностей для собеседований, ШАД и олимпиад": "Теорию вероятностей спрашивают и на собеседованиях, и на экзаменах, также она является фундаментом для многих методов машинного обучения. По моим наблюдениям студентам явно не хватает того курса теор вера, который есть в ВУЗах, чтобы научиться решать основные задачи —  необходимы дополнительные материалы. В этой статье хотел бы поделиться моими самыми любимыми материалами и источниками для освоения теории вероятностей, имея за плечами крепкую школьную базу и скромные навыки в математическом анализе и линейной алгебре.   Первые шагиЛюбой курс по теории вероятностей начинается с освоения классической вероятности. Вспомним определение вероятности, которое еще давали в школе: То есть вероятность события А- это нужно количество удачных исходов (|A|) поделить на количество всех исходов (). Например, задача: подбрасывается игральный шестигранный кубик d6, какова вероятность, что выпадет честное число? Конечно, ответ 1/2, ибо всего вариантов , а удачных вариантов |A = {2, 4, 6}| = 3. Итого 3/6 = 1/2.В более сложных задачах на классическую вероятность вся сложность состоит в том, чтобы посчитать количество исходов. Здесь нужно знать комбинаторику. Для ее освоения советую Ленинградские математические кружки: главы Комбинаторика-1 и Комбинаторика-2. Пособие легендарное, написано для детей, понятным языком, очень много примеров и задач для самостоятельного решения. Кстати, материал классический и решения всех задач или их аналогов можно найти на форумах в интернете, чтобы себя проверить! Если хотите глубже разобрать какую-то тему из Комбинаторики или освоить такие важные темы как \"Формула включений и исключений\", которые очень часто встречаются на собеседовании, то советую Виленкина. Книга большая, очень много разных сюжетов, для первого прочтения после ЛКМ советую лишь главу 3 и главу 4. Сразу предупреждаю, что темы очень глубокие и осваивать их можно до бесконечности. Например, я как преподаватель сам возвращаюсь к ним и постоянно открываю для себя что-то новое. Мне кажется это просто бездонная бочка, поэтому советую вам действовать так: прочитать один параграф, понять общую идею, что-то порешать и переходить к следующему параграфу, при необходимости возвращаться к старым параграфам. После освоения комбинаторики советую освоиться с суммированием, которое часто встречается в дискретной вероятности: суммы в вероятностях и математических ожиданиях.Например, как посчитать, чему равно математическое ожидание геометрического распределения? Напомним, что случайная величина  геометрически распределена, если . Как такое сделать смотрите вот этот ролик Михаила Абрамовича. На самом деле кроме прошлого ролика и вот этого ролика, вам еще нужно вспомнить из школы чему равна сумма арифметическое и геометрической прогрессии. Далее самым оптимальным вариантом, как мне кажется, будет открыть задачник Алфутовой и решать задачи из 11 главы. Главное не стесняйтесь гуглить, товарищи! Предложенные задачи не рассчитаны на долгую медитацию. Если задач не получается в течение минут 5-15, то ее смело стоит гуглить, спрашивать у знакомых, спрашивать на формах и чатах, а потом просто решить аналогичную задачу на закрепление. Дискретная теория вероятностейЗдесь очень советую курс Сердобольской на физфаке МГУ: он фундаментален, при этом абстрактные понятия как мера, интеграл Лебега, которые сильно расстраивают новичков, встречаются очень редко. Лекции с теорией здесь, семинары с примерами решения задач здесь. По ссылкам есть как и печатный конспект в pdf, так и записи живых занятий. Конечно, без практики освоить материал невозможно, поэтому советую задачник Кочеткова, где много задач с интересным сюжетом (а не только ящики и шары). Также перед задачами приведена теоретическая справка и примеры решения стандартных задач. Для любителей пожестче советую задачник Севастьянова, особенно убойной мне кажется подборка задач по условной вероятности, после этой подборки проблем точно не возникнет. По моему опыту и опыту моих учеников для 95% карьерных собеседований хватает классической вероятности, условной вероятности, геометрической вероятности, что-то знать про дискретные и непрерывные случайные величины (как считать мат ожидание, дисперсию, что такое мода, медиана). Для экзаменов в магистратуру, ШАД и олимпиад нужно двигаться дальше!Непрерывная теория вероятностей Первым делом стоит освоить или повторить математический анализ, самое основное: как исследовать функцию с помощью производной, как взять интеграл. Для многомерных задач нужно освоить или повторить многомерные интегралы. Для этого советую замечательные семинары Скубачевского из МФТИ, где разобраны все стандартные задачи, а вся необходимая теория рассматривается на конкретных примеров. Для практики и отработки советую серию задачников Виноградовой, где кроме задач как всегда приведена необходимая теоретическая справка и примеры решения задач. Параллельно с этим можно осваивать уже упомянутые выше семинары и лекции Сердобольской по непрерывной вероятности. Конечно, материал непростой, очень много деталей, которые трудно уместить в курс на 13 семинаров. Наверняка, вам будет не хватать примеров решения задач, поэтому могу посоветовать \"Книжечку для экономистов\", где также есть и теория, и разборы задач, и задачи для самостоятельного решения. Какие-то моменты освещены подробней или под другими углами, нежели у Сердобольской. Также там есть классный раздел по математической статистики, которого с лихвой хватит для собеседований и понимания фундамента машинного обучения. Еще мне нравится курс от CS: часть 1 и часть 2, который отлично подходит для структурирования материала, чтобы привести все знания в порядок, а также расширить их новыми примерами и сюжетами, единственное там мало практики. Для расширения горизонтов также советую семинары Шкляева, которые ведутся на мехмате МГУ для будущих математиков, поэтому без предыдущей подготовки может быть нелегко! Задачники советую те же самые, которые советовал по дискретной вероятности. Олимпиадный уровеньЗдесь хотел бы посоветовать материалы для любителей нестандартных задач и для тех, кто готовиться к олимпиадам, магистратурам, ШАДам. Самое очевидное начать просто с разбора вариантов прошлых лет. Например, здесь собраны часть вариантов первого этапа в ШАД, а здесь часть вариантов второго этапа. Аналогично варианты олимпиад \"Я - профессионал\", \"Высшая лига\", \"Колмогоровская олимпиада\", \"Заочная олимпиада по теории вероятностей\" можно найти на официальных сайтах. Также олимпиады по математики проводит почти любая кафедра по математике, сборники прошлых лет здесь.Еще советую два пособия на английском для тех, кто готовится к собеседованиям на позицию Quantative Resercher: один и два. В эту тему есть всякие подборки задач, составленные энтузиастами, самая известная: культурный код. Или например сборная солянка задач с зачетов мехмата легендарного Кондратенко, то есть зачеты настолько интересные, что студенты не поленились затехать условия и решения. Мне же очень нравятся заметки с семинаров, очень качественно составлена подборка. ЗаключениеНадеюсь каждый читатель откроет для себя новый и интересный материал, а если не нашли среди предложенных свой любимый, то прошу вас поделиться им в комментариях: мне как практикующему преподавателю будет очень приятно добавить его в свою коллекцию. ",
  "Жаворонки не так продуктивны": "Привет, Хабр!Сегодня мы разберём исследование UK Biobank, которое, возможно, перевернёт твои представления о том, сколько нужно спать и как хронотип влияет на продуктивность. Если ты сова — не торопись завидовать жаворонкам, а если жаворонок — не торопись радоваться. Данные говорят сами за себя.Дисклеймер: Не принимай эту статью как руководство к действию — это всего лишь анализ данных и научные выводы. Если после прочтения захочется поспать лишний час или снова начать гордиться тем, что ты сова, — делай это на свой страх и риск!UK BiobankДля начала, что такое UK Biobank? Это одна из крупнейших баз данных о здоровье и образе жизни населения Великобритании. В исследовании приняли участие 501 718 человек, однако для анализа сна и когнитивных способностей было отобрано 26 820 участников, возраст которых составил от 53 до 86 лет.Этих участников разделили на две когорты: первая когорта включала 10 067 участников (56% женщины), которые прошли полный комплекс из четырёх когнитивных тестов, а во вторую когорту вошли 16 753 участника (также 56% женщины), выполнившие два теста. Для анализа использовалась регрессионная модель наименьших квадратов, чтобы выявить корреляции между характеристиками сна, хронотипом и когнитивными способностями.Какие когнитивные тесты использовались?В исследовании использовались следующие тесты:Fluid Intelligence Test — это 13 вопросов на логику и быстроту мышления. Каждое задание нужно выполнить за ограниченное время.Pairs Matching — игра на память. Участнику показывают пары картинок, которые затем скрываются, и нужно найти соответствующие пары.Reaction Time Test — нажатие кнопки при совпадении карт на экране. Это проверка скорости реакции.Prospective Memory Test — хитрый тест, где нужно сначала запомнить одну инструкцию, а затем выполнить совершенно другую. Типичный пример: сказали нажать на синюю кнопку, а нужно нажать на оранжевую.Параметры сна: сколько, когда и какТеперь к основным параметрам сна, которые рассматривались в исследовании:Продолжительность сна — участники отвечали, сколько часов они спят в сутки. Ответы разделили на три категории:Короткий сон: \u003c7 часов.Нормальный сон: 7–9 часов.Длинный сон: \u003e9 часов.Хронотип — это время суток, в которое человек наиболее активен:Жаворонки (активные утром).Совы (активные вечером).Промежуточные (ни то ни сё).Качество сна — наличие бессонницы. Участников спрашивали, как часто они страдают бессонницей или просыпаются среди ночи.РезультатыТеперь к делу. Какие выводы мы можем сделать из данных?Продолжительность снаПожалуй, самый важный показатель, который можно рассмотреть, это количество часов сна. Исследование подтвердило старую истину: 7–9 часов сна — это золотой стандарт для максимальной продуктивности. Вот как выглядят результаты на графиках:График продолжительности снаНа графике A для Когорты 1 чётко видно, что когнитивные показатели достигают пика при 6-9 часах сна. Это создаёт инвертированную U-образную кривую: когнитивные функции лучше всего работают именно в этом диапазоне. Когда ты спишь меньше 6 часов, когнитивная активность начинает падать — это факт . Слишком мало сна просто убивает продуктивность. Если ты думаешь, что можно просто выспаться побольше, то плохие новости: спать более 9 часов — тоже вредно для мозга. Видимо, слишком много отдыха может \"перегрузить\" твой мозг .На графике B для Когорты 2 всё немного интереснее. Здесь когнитивные функции остаются стабильными между 5 и 9 часами сна, но как только время сна переваливает за 9 часов, когнитивные способности резко падают. Видна прямая связь между количеством сна и тем, как быстро твой мозг думает и решает задачи.Хронотип и его влияние на мозгТеперь поговорим про хронотип. Исследование показало, что если ты \"сова\", не стоит переживать: у тебя, возможно, всё в порядке с мозгами! Наоборот, данные говорят, что \"совы\" и промежуточные типы демонстрируют лучшие когнитивные способности, чем жаворонки.График хронотипаНа графике A для Когорты 1 видно, что \"совы\" и промежуточные типы показывают более высокие когнитивные показатели, чем жаворонки. Это может быть связано с тем, что поздние хронотипы лучше приспособлены к выполнению когнитивных задач, которые требуют больше времени на обдумывание и креативность .В Когорте 2 (график B) видна схожая картина, хотя разница между хронотипами менее выражена.Хронотип напрямую влияет на то, как быстро и эффективно ты решаешь задачи. Чем ближе ты к \"сове\", тем лучше твои когнитивные способности.БессонницаТеперь поговорим про качество сна и бессонницу. Неожиданно, но бессонница оказала минимальное влияние на когнитивные способности. Хотя логика подсказывает, что бессонные ночи должны снижать производительность, исследования не показали больших различий между теми, кто страдает бессонницей, и теми, кто спит спокойно.График бессонницыНа графике A для Когорты 1 можно заметить, что разница в когнитивных показателях между людьми, которые страдают бессонницей и теми, кто нет, минимальна. Разница составляет всего несколько сотых долей балла по глобальному z-показателю (β = -0.05, p \u003e 0.05), что не является значимой.График B для Когорты 2 показывает похожие результаты. Даже если ты иногда страдаешь бессонницей, это не сильно бьёт по твоей продуктивности, как можно было бы ожидать.С бессонницей всё не так страшно, как могло показаться. Даже если ты не всегда можешь выспаться, это не катастрофа для твоих когнитивных функций. Однако это не означает, что нужно забивать на хороший сон.Влияние факторов здоровья и образа жизниЕщё один интересный аспект исследования — это влияние здоровья и образа жизни на когнитивные способности. Такие факторы, как возраст, пол, диабет и даже алкоголь могут значительно влиять на продуктивность мозга.Пол: У женщин когнитивные показатели оказались ниже, чем у мужчин в обеих когортах . Возможно, это связано с тем, что когнитивные задачи, используемые в исследовании, больше подходят мужчинам.Возраст: Старение — это предсказуемый фактор. Чем старше участники, тем ниже их когнитивные способности .Диабет: У участников с диабетом когнитивные способности были ниже .Алкоголь: Оказалось, что умеренное употребление алкоголя положительно сказывается на когнитивных способностях по сравнению с трезвенниками .Выводы: сколько спать, чтобы быть умнее?Подведём итог. Оказывается, быть умным — это не так сложно, как стать олимпийским чемпионом, но всё же кое-какие правила игры есть.7–9 часов сна — это золотой стандартЗабудь про попытки спать по 4 часа, как Илон Маск, и не думай, что ты станешь гением, если спишь по 10 часов в день. 6-9 часов — вот твоя зона комфорта для мозга. Всё, что меньше — это путь к зомби-режиму. Всё, что больше — это прямая дорога к \"пересыпанию\" и потере остроты ума.Хронотип важен: совы, вы победилиЕсли ты сова — расслабься. Не надо слушать этих жаворонков, которые утверждают, что только ранний подъём гарантирует успех. Нет, ты всё ещё можешь быть умнее их, если просто оставишь свои ночные бдения. Сосредоточься на том, чтобы спать свои 7-9 часов, и твой мозг не подведёт. Жаворонки, простите, но совы уже открыли второе дыхание, пока вы только зашнуровываете свои кроссовки на утреннюю пробежку.Бессонница не так страшна, как могла показатьсяОкей, если ты иногда ворочаешься и не можешь уснуть — не паникуй. Конечно, это не круто, но твой мозг не сразу превратится в кашу. Да, ты не станешь Шерлоком после бессонной ночи, но и не превратишься в Уотсона. Главное — не делай это привычкой.Настало время подумать о себеКороче, пора пересмотреть свои привычки. Ты уже знаешь, что сон — это сила, но теперь ты знаешь как и когда его использовать на максимум. Так что, вот тебе вопрос: сколько часов ты спишь? И самое главное: когда ты наконец ляжешь спать сегодня?И помни: если кто-то скажет тебе, что спать много — это для слабаков, просто покажи им эти графики.Sleep duration, chronotype, health and lifestyle factors affect cognition: a UK Biobank cross-sectional study by Raha West et al. in BMJ Public Health. 2024",
  "Импортозамещение Data Quality стека в нефтегазохимии: опыт СИБУРа": "В СИБУРе много данных, которые текут в режиме реального времени с многочисленных датчиков на разных производствах, эти данные нужно собирать, хранить, обрабатывать и анализировать, чтобы компания могла принимать правильные бизнес-решения. И от качества инфраструктуры для работы с данными зависит рентабельность производств и прибыль компании в целом, а это жизненно важные показатели.В небольшом цикле из двух статей мы разберём опыт СИБУРа в создании, поддержке и развитии DQ (Data Quality — качество данных) сервиса для DWH (Data Warehouse — хранилище данных) в условиях санкций и исчезающих вендоров проверенных и привычных решений.Рассказывать об этом опыте будет Александр Бергер, Lead DQ Analyst в Цифровом СИБУРе, которому посчастливилось лидить процесс создания DQ-сервиса на решениях вендора, который решил покинуть рынок РФ в разгар рабочего процесса.Что такое Data Quality?Если вкратце, то DQ-сервис — это набор инструментов для проверки качества данных, поступающих в хранилище, передающихся между слоями хранилища или уже хранящихся в DWH.Качество данных необходимо проверять, чтобы понимать, можно тем или иным данным доверять или нет — это критично, потому что на основе этих данных принимаются управленческие решения и цена ошибки в этом процессе очень высока, особенно в промышленном секторе, ярким представителем которого и является компания СИБУР.В производственных процессах в СИБУРе повсеместно задействованы современные информационные технологии, и в наших производственных контурах постоянно генерируются данные — оборудование, датчики, всевозможные автоматизации, IoT — отправляют данные 24/7, которые нужно передавать, собирать, обрабатывать, хранить.СИБУРу важно следить за качеством данных, от этого зависит рентабельность бизнеса. Поэтому компания выделяет ресурсы на развитие DQ-сервиса для DWH.Первые шаги после ухода вендора. Архитектура приложения DQ  Суть нашего DQ-сервиса можно увидеть на этой схеме, выкладываем её здесь в качестве тизера, а погрузиться в детали архитектуры можно будет в следующей статье.  В этой статье мы поговорим о животрепещущем вопросе импортозамещения решений, которые мы применяли в нашем DQ-сервисе до ухода вендора.Срочное импортозамещение До широко известных событий 2022 мы строили DQ на SAS Data Quality. На SAS у нас был выстроен процесс проверки качества данных, мы определили зоны ответственности и планировали начать процесс обучения коллег из бизнеса взаимодействию с сервисом. Проверки поступающих данных настраивались инженером качества над Vertica. И к февралю мы начали внедрение SAS Decision Management — инструмента для self-service проверок качества данных.Но что случилось, то случилось, и мы начали поиск замены. Нам нужно было найти то, что удовлетворяло бы нашим требованиям и при этом не исчезло бы в одночасье из-за санкций, не перешло бы на подписку, которую мы бы не смогли оплатить из России.Анализ рынка Первое, с чем мы столкнулись, — отсутствие готовых коммерческих отечественных решений на рынке РФ для нашего стека. Поэтому назвать то, что мы делали, «импортозамещением» можно с оговорками: мы замещали, да, но не отечественными решениями, а тем, чем могли.Ситуацию осложнял факт, что мы не понимали, в какие сроки сможем решить задачу в таких условиях. Воспользоваться опытом коллег из российских компаний, которые использовали OpenMetadata, мы не могли: там много чего нужно было допиливать, а у нас уже был свой работающий каталог.Первые шагиМы провели анализ рынка, и самым подходящим под наши потребности решением оказалась опенсорсная библиотека Great Expectations (GX), которая сделана на Python.Достоинства GX, важные для нас:Есть множество кастомных проверок.Высокая степень гибкости настроек и сценариев.Достаточно живое комьюнити.Бесплатный доступ.Взвесив все за и против, мы сделали выбор в пользу GX.И сразу на граблиУ нас уже была команда, которая разрабатывала каталог данных. И мы начали разработку DQ-инструмента как микросервиса для каталога данных. Но не своими силами, а привлекли подрядчиков.СИБУР, как большая компания, может себе позволить иметь отдельную команду, чтобы развивать собственные сервисы. Но в условиях хаоса мы приняли решение привлечь подрядчиков с релевантным опытом.С нашей стороны, как сейчас уже понятно, было ошибкой отдавать разработку микросервиса на аутсорс. Нет, они всё хорошо разработали, но подрядчики рано или поздно заканчиваются, и нам пришлось попрощаться с ними.В процессе передачи дел возникли проблемы, и в итоге мы потеряли часть экспертизы по разработке. Со временем удалось нарастить эту экспертизу, но на это потребовалось время.Однако неприятности на этом не закончились. И если историю с подрядчиками можно отнести в категорию рисков, которые мы осознавали и оценивали как «Возможные» при принятии решения, то события, которые развернулись дальше, мы отнесли в категорию «Вряд ли такое может произойти».Чистый Open Source или форкДело в том, что в работе с Open Source библиотеками есть нюансы, связанные с обновлениями. В целом это классическая история с опенсорсными решениями, что они тоже развиваются (обнаруживаются различные уязвимости в безопасности, архитектурные недочёты).И комьюнити, которые связаны с этими решениями, могут увести разработку в какую-нибудь сторону, которая нам не нужна. И возможна ситуация, что после очередного обновления какие-то шаблонные проверки или сервисы у нас перестанут работать. Так и произошло.Никогда такого не было, и вот опятьВ какой-то момент мы обнаружили, что в новых версиях GX отвалилась поддержка Vertica и Oracle, а это для нас критично. И на множество вопросов «Когда это всё закончится?» они перестали отвечать, потому что сфокусировались на развитии своего облачного продукта, который будет распространяться по подписке.То есть перед нами возникла дилемма:Пользоваться старой версией GX, в которой есть поддержка нужных нам решений.Делать форк и оптимизировать его под наши нужды.Искать замену.Проблема первого варианта заключается в том, что рано или поздно нам всё равно пришлось бы делать форк. Потому что мы хотим подключаться к большему количеству систем, хотим масштабировать, оптимизировать и так далее.А пайплайн с созданием форка — это нетривиальная история. Если даже разработчики оригинальной версии не настроили в новых версиях Vertica, у нас шансов на реализацию этого было явно меньше. Вдобавок у нас не было ресурсов, чтобы пилить такое решение самостоятельно, стандартизировать и поддерживать. СИБУР — большая компания, но не настолько.Выбор решенияМы начали анализировать, что есть на рынке. Снова. Пообщавшись с коллегами из других компаний, поняли, что мы не одиноки в истории с GX и проблемы у нас похожие.В России пользуется популярностью Arenadata Catalog, он в плане написания проверок качества данных тоже базируется на GX. И его разработчики пошли по второму пути — сделали форк. Но у них компания, которая заточена на разработку каталога и проверок качества данных. А у нас немного другой профиль.И так как в решениях на базе Great Expectations теперь нет поддержки Vertica и Oracle, нам они, как уже упоминалось выше, не подходят. Поэтому мы начали анализировать другие популярные решения.Поняли, что многие решения зачастую сфокусированы на проверках данных в режиме реального времени — во время загрузки, а мы в СИБУРе валидируем уже собранные данные, об этом мы расскажем в следующей статье. В некоторых случаях решения работают напрямую со Spark, что нам не подходит. Какие-то инструменты мы отмели, потому что там недостаточно живое комьюнити, а какие-то нас не устроили по множеству причин.Soda vs Great ExpectationsВ нашу выборку попал DQ-инструмент SODA — это Open Source. Мы проанализировали SODA, сравнили c GX и подумали, что он будет лучше. Потому что его проще поддерживать, он подключается к Vertica и Oracle — а это то, что нам нужно.В SODA нет проблем с подключениями, потому в GX идёт подключение с помощью SQLAlchemy, а в SODA по-другому — отдельные коннекторы написаны.  В общем, протестировали, проанализировали, посоветовались с коллегами в других компаниях и поняли, что многие переключаются на работу с SODA. Там активное комьюнити, люди пользуются, проблем нет.Вместо заключенияНаше текущее положение можно увидеть на этом графике. Он шуточный, но, кажется, в нём есть доля правды. Чем больше опыта ты набираешься в работе с данными, тем больше приходишь к тому, что проверки SQL-скриптами и разработка собственного DQ-инструмента с нуля выглядят более правильно.Риски использования Open Source есть и никуда не денутся, в чём мы убедились на собственном опыте, когда разработчик:убрал нужную нам функциональность;ушёл в сторону платной модели.Нам потребовался год, чтобы попробовать на себе всю эту специфику и понять, что нам не подходит и куда мы хотим двигаться дальше.Сейчас мы занимаемся миграцией с GX на SODA, так как этот вариант нам подошёл больше всего. Но и здесь сохраняются риски: разработчик может перестать поддерживать это решение, что-то может отвалиться либо переведено в какую-то платную подписку, а мы её купить не сможем, потому что мы в России.Также не стоит забывать, что SODA разрабатывает компания, которая находится на Западе. И мы снова можем оказаться в затруднительном положении. Если такое случится, будем искать что-то на нашем рынке либо разрабатывать своё решение.Но разработка собственного решения не выглядит привлекательной идеей для нас, как и для многих других компаний. Поэтому сейчас единственное, что остаётся, — это внедрять продукты с открытым исходным кодом.Подводя итоги по импортозамещению, можем сказать, что мы сделали всё возможное, чтобы не столкнуться с историей, когда у нас что-то отвалится и придётся всё резко переделывать. По крайней мере, старались организовать архитектуру именно таким образом. Мы выстраиваем многослойный DWH на Open Source стеке, внедряем туда Data Quality инструменты — тоже на базе опенсорсных решений.Концепцию и архитектуру нашего DQ-решения, которое можно применить в любой компании, обсудим в следующей статье. Если есть какие-то вопросы по этой теме — ждём в комментариях.",
  "Интернет тонет в спаме": "\nСпам в каталоге пакетов npm\n\nИнтернет уже не тот, что в 90-е. Тогда мы искали интересные сайты по тематическим каталогам Yahoo и Рамблера. Поисковых систем не существовало до появления AltaVista. Даже мысли не было создавать мусорные сайты для прокрутки рекламы, продажи ненужных товаров или обмана людей. Коммерция ещё не пришла в интернет.\n\nСейчас совсем другое дело. Почти никто уже не воспринимает интернет как технологическое чудо и научный инструмент. Для мошенников это просто ещё один способ обмануть окружающих. Когда знакомый бизнесмен в начале 2000-х узнал про существование электронной почты, его первый вопрос был — как разослать тысячи писем со своей рекламой? Факт аморальности рассылки спама его совершенно не смутил на фоне потенциальной прибыли. У коммерсантов просто другая система ценностей.\n\nИ не только электронная почта. То же самое с сайтами, блогами и остальным UGC: сегодня всё генерируется автоматически для поисковой оптимизации, облапошивания простых людей и выманивания денег любым путём.\n\nИногда кажется, что в интернете осталось только 5% полезного контента — и 95% спама.\n\nПоисковый спам\nЧёрные методы поисковой оптимизации (SEO) берут верх над алгоритмами поисковых систем, так что поисковая выдача Google безнадёжно погрязла в мусоре.\n\nПоиск Google сильно упал в качестве за последние годы. Сегодня вся первая страница — это реклама и спам.\n\n\n\nКомпания Google сама ухудшает ситуацию, продавая много позиций в результатах поиска, а ведь проплаченные ссылки по сути не отличаются от поискового спама, просто последний пытается пробиться в топ выдачи бесплатно. Если прокрутить ниже, сразу после рекламы начинается поисковый спам, то есть SEO-оптимизированный мусор:\n\n\n\nИ так по всем популярным запросам.\n\nВ последнее время ситуация ещё ухудшилась, потому что спамеры-оптимизаторы начали генерировать тексты в больших языковых моделях (LLM), так что генерация мусора сильно упростилась. Кроме того, они тупо воруют контент с информационных сайтов, у которых высокий PageRank и позиция в выдаче (Хабр, Википедия, Stack Overflow). По идее, поисковик должен штрафовать такие клоны и искусственно понижать их, но такое не всегда происходит.\n\nНедавнее исследование учёных из Лейпцигского университета и Веймарского университета «Баухаус» (Германия) показало большое количество спама в поисковой выдаче. На диаграммах ниже видно, что позиция в поисковой выдаче явно коррелирует с признаками SEO:\n\n\n\nИсследователи также заметили деградацию качества поиска в последнее время.\n\nСитуация настолько плачевная, что пользователям приходится устанавливать специальные расширения вроде uBlacklist, чтобы блокировать спамерские сайты в поисковой выдаче:\n\n\nРасширение uBlacklist\n\nРади партнёрских ссылок и поискового трафика спамеры заводят фейковые блоги от фейковых личностей с бессмысленными сгенерированными текстами, чтобы привлечь поисковый трафик.\n\n\nСовременный блогоспам. Фейковый блог от фейковой личности. Даже само лицо по всей видимости сгенерировано нейросетью This Person Does Not Exist\n\nСпамеры проникают на любые платформы, которые индексируются поисковыми системами.\n\nСпам в твиттере и соцсетях, каталогах приложений, пакетных менеджерах\nЕсть предположение, что в ближайшем будущем LLM-мусор заполнит все возможные пустоты, где выгодно использовать нейросети вместо людей: \n\n\nсценарии кинофильмов (уже сейчас значительная часть сценариев для малобюджетных фильмов генерируется LLM, в том числе простые диалоги);\n\nпоп-музыка;\n\nлитературные произведения;\n\nстатьи в СМИ;\n\nсообщения в блогах и соцсетях.\n\nголосовое общение по телефону (техподдержка, обслуживание клиентов).\n\nСпам добрался даже до пакетов NPM: по статистике, 25% новых пакетов во II кв. 2024 г. содержит спам. Особенно заметна рекламная кампания протокола Tea, который предлагает финансовое вознаграждение разработчикам опенсорса, стимулируя их в том числе клонировать чужие репозитории и пакеты, генерировать даже бессмысленный код. Там вознаграждение зависит от вклада. То есть чем больше репозиториев — тем выгоднее. Отсюда и тысячи бессмысленных клонов.\n\n\nРаспределение финансового вознаграждения среди опенсорс-разработчиков по протоколу Tea, с учётом зависимостей приложений, источник\n\nПлатформа сама подталкивает пользователей к такому поведению, предлагая вознаграждение (в поинтах) за выполнение разных заданий, в том числе за вирусное распространение информации в соцсетях:\n\n\n\nПохожая ситуация сложилась на Github, где количество спама вышло из-под контроля. Там в основном промышляют криптоспамеры и мошенники, которые публикуют пост с тегами множества других, реальных пользователей — а потом быстро удаляют его. Однако каждый «отмеченный» в посте пользователь Github получает по почте копию текста. Оригинальный способ инициировать почтовую рассылку. Выглядит это примерно так:\n\n\n\n\n\nЕсли пробить актуальные спамерские темы по поиску, то можно найти их в комментариях к пул-реквестам и багам, это сотни комментариев:\n\n\n\nК сожалению, на Github нет системы эффективной фильтрации спама, так что рекламные комментарии висят много дней и недель, а не удаляются сразу. Ещё больше мусора в каталоге приложений OpenAI (GPT Store) и других каталогах:\n\n\n\nСпам проникает буквально повсюду. Посторонний человек может даже добавлять события в ваш личный календарь Apple, Google или Microsoft. Для этого достаточно знать адрес электронной почты, привязанный к аккаунту, если включена опция «Добавлять приглашения от всех» в настройках:\n\n\n\nИронично, что даже на сайте независимой поисковой системы, которая ставит целью избавить поисковую выдачу от SEO-спама, 99% посетителей составляют спамерские SEO-боты.\n\nКак защититься от мусора? Никак. Способ простой: брать контент из проверенных источников. Платная подписка, личные блоги.\n\nКстати, корпорация Google недавно придумала технологию водяных знаков для LLM-текстов. Идея в том, что при генерации текста LLM выбирает токены частично основываясь на криптографическом ключе. И кто-то, знающий ключ, может определить и доказать использование ключа при создании конкретного текста. Исторически, простановка водяных знаков в текстах вызывает два затруднения:\n\n\nдля их детектирования требуется относительно большой объём текста;\n\nводяной знак не очень устойчив к редактированию после генерации.\n\nВерсия водяных знаков от Google выглядит неплохо: она обнаруживается даже в маленьких текстах от 200 токенов, то есть примерно три-четыре абзаца стандартного текста.\n\nСхема работы алгоритма под названием SynthID-Text (на иллюстрации внизу) состоит из трёх компонентов, которые выделены синим: генератор случайных сидов, алгоритм выборки и функция подсчёта баллов. Они используются при генерации текста и детектировании водяных знаков. \n\n\n\nПри генерации водяных знаков SynthID-Text используется алгоритм выборки Tournament, вот как он работает:\n\n\n\nКоличество спамерских текстов, сгенерированных LLM, растёт в геометрической прогрессии. И уже близок тот день, когда сгенерированного мусора в интернете будет больше, чем авторского контента, написанного человеком.\n\nЭксперимент на 20 млн сгенерированных текстов нейросети Google Gemini с водяными знаками не показал ухудшения качества выдачи. Специалисты прогнозируют, что все основные разработчики LLM внедрят функцию простановки водяных знаков в свою выдачу.\n\nМусорная реклама окружает нас повсюду в интернете, и не всегда спасают даже блокировщики рекламы, такие как uBlock Origin. Например, сайт YouTube пытается детектировать такие блокировщики и обходить их, а также внедряет «неотключаемую» рекламу в разные места видеоролика. Вдобавок к этому, сама компания Google готовит к выпуску новую версию браузера Chromium, в которой uBlock Origin \u003ca href='https://github.com/uBlockOrigin/uBlock-issues/wiki/About-Google-Chrome's-%22This-extension-may-soon-no-longer-be-supported%22'\u003eперестанет функционировать. А на движке Chromium основаны многие другие браузеры, кроме Firefox и Safari.\n\nТак что ради блокировки рекламы придётся возвращаться на Файрфокс или Оперу, которые обещает сохранить функциональность uBlock Origin в полном объёме.\n\nЗвонки по телефону. Обманывают самых умных\nГолосовая связь тоже засоряется робозвонками, спамом и мошенничеством. Такое чувство, что услышать живого человека по телефону скоро будет большой удачей: всегда и везде трубку снимают чатботы с распознаванием речи и прочие ИИ-агенты, которые могут переключить вас на живого оператора, а могут и не переключить. Говорят, что есть специальные ключевые слова (или нецензурная лексика), которые помогают быстрее отключить автоматизированную программу и привлечь живого оператора. \n\nПо оценке специалистов, в США за прошлый год произведено 55 млрд робозвонков на телефоны граждан. Это тот же спам, только по телефону, возможно даже интерактивный, с распознаванием речи и примитивным диалогом:\n\n\n\nВ Федеральную торговую комиссию США уже направлено предложение ввести ответственность и ограничить роботизированные звонки без согласия получателя. Есть идея ввести ответственность также за рекламные SMS. Кроме того, недавно в США ввели обязательную процедуру отписки от любых рекламных рассылок «в один клик». Вероятно, это распространяется и на рекламные звонки, SMS и прочие виды спама.\n\nКроме роботизированного спама, по телефону продолжает применяться традиционная социальная инженерия, чтобы выманить у человека персональные данные, получить доступ к его счёту или убедить перевести деньги. Популярны звонки якобы из милиции, в Беларуси — из КГБ. Мошенники манипулируют страхом граждан перед госорганами. В разговоре они не оставляют человеку времени подумать. Главное — скорость, жертву заставляют быстро реагировать и действовать. Например, в Турции по такой схеме работают целые колл-центры с десятками операторов, которые обманывают тысячи европейцев. Операции по выкачиванию денег идут в промышленном масштабе.\n\nИсследования показывают, что чаще всего жертвами мошенников становятся молодые люди в возрасте 34-х лет (об этом говорила клинический психолог Ксения Ягур в недавнем фильме с экспертами Яндекс 360 про кибермошенничество. Судя по всему, чем умнее человек — тем легче его обмануть, потому что он больше уверен в собственном интеллекте, чем более глупый сородич, который просто повесит трубку, не вступая в дискуссии с мошенником.\n\nМетоды борьбы с почтовым спамом\nСпам сейчас не только реклама, но и угроза. Соотношение мошенничества и мусора в почтовом спаме примерно 50 на 50.\n\nМошенники придумывают всё новые способы фишинга и социальной инженерии. Например, из последних изобретений:\n\n\nПисьма от госуслуг с просьбой уточнить данные\n\nПоддельный ответ на ваше письмо (несуществующее или реальное)\n\nChatGPT тоже помогает обойти антиспамерские фильтры почтовых провайдеров. Но ИИ используется с обеих сторон.\n\nПо словам технического специалиста Яндекс 360 из вышеупомянутого фильма, сейчас 90% всей работы по фильтрации спама выполняет ИИ. В целях безопасности маленькую часть работы по надзору оставили человеку-дежурному, который следит за почтовым трафиком. В случае пробива защиты, как на прошлый Новый год (30 млн спамерских писем) он оперативно созывает совещание из двух-четырёх программистов, чтобы отработать и внедрить изменения в алгоритмы фильтрации спама.\n\nПо технологии Спамообороны в Яндекс.Почте фильтрация спама происходит следующим образом, пошагово:\n\n\nПочтовое письмо приходит на сервер в формате .eml\n\nПроводится базовая проверка SPF и DKIM\n\nПроверка по списку доверенных IP-адресов\n\nПарсинг письма, анализ признаков (около 10 тыс.)\n\nПодача признаков на вход системы машинного обучения\n\nПолучение результата анализа ML в виде числового коэффициента, который определяет вероятность спама (в почтовом интерфейсе это отражается зелёным, жёлтым или красным индикатором «Спам» возле письма)\n\nПо словам представителя, с начала года через их сервис прошло свыше 66 млрд писем, из них более 16 млрд обозначено как спам. Вероятно, примерно такое же процентное соотношение и принципы работы у других эффективных сервисов по фильтрации спама, как в почте Gmail или Yahoo.\n\nSPF, DKIM и DMARC — основные методы защиты от спама в электронной почте.\n\nSPF, DKIM и DMARC\nSPF (Sender Policy Framework) представляет из себя текстовую запись в TXT-записи DNS-домена. В ней указан список серверов, которые имеют право отправлять письма от имени этого домена и механизм обработки писем, отправленных от других серверов. Это эффективная защита от фишинга. SPF-запись не позволит злоумышленнику отправлять письма от вашего домена, как это было популярно в 90-е годы. Один из видов фишинга по сути уничтожен на корню.\n\n\n\nDKIM (DomainKeys Identified Mail) — метод аутентификации почтовых отправлений, который защищает от подделки (спуфинга) адреса отправителя с помощью цифровой подписи.\n\n\n\nDMARC (Domain-based Message Authentication) — техническая спецификация для идентификации почтовых доменов отправителя на основании правил и признаков, заданных на почтовом сервере получателя. Это конкретно антиспамерская технология. DMARC предусматривает механизмы для обмена информацией между отправителем и получателем о качестве фильтрации спама и фишинговых атаках. Например, если вы представляете домен-отправитель почты и публикуете DMARC-запись с запросом информации, то можете получать от всех доменов-получателей, которые тоже поддерживают DMARC, статистику обо всех почтовых письмах, которые приходят с обратным адресом от вашего домена. Политики DMARC публикуются в системе доменных имён в виде ресурсных записей типа TXT и содержат инструкции по поводу того, что должен будет сделать узел, принимающий электронные сообщения, с полученными несоответствующими сообщениями.\n\n\n\nКстати, с февраля 2024 года Gmail и Yahoo ужесточили правила для входящих писем (объявление Gmail, объявление Yahoo).\n\nТри ключевых изменения для входящей почты, которые вступили в действие в 2024 году:\n\n\nПроверка подлинности электронной почты. Отправители должны подтверждать личность отправителя с помощью стандартных протоколов SPF, DKIM и DMARC.\n\n\nЛёгкая отписка. В случае массовых рассылок отправители должны внедрить в электронные письма ссылку для отказа от подписки в один клик, чтобы получатели могли легко отказаться от подписки.\n\n\nТолько письма, которые нужны пользователям. Gmail и Yahoo всерьёз взялись за мониторинг спама, и отправители должны следить за тем, чтобы количество рассылаемых писем не превышало установленного порога 0,3%.\n\nНа сайте Google опубликованы инструкции, как наилучшим образом реализовать аутентификацию DKIM для своего домена. Сегодня поддержка SPF и DKIM — это абсолютный минимум для исходящих писем, иначе Google отправит письма в папку «Спам». Для DMARC нужно реализовать хотя бы поле p=none.\n\n\nК сожалению, битва со спамом далека от победного конца. Изворотливые мошенники находят всё новые способы обойти любую защиту, заработать немного денег на партнёрских программах, продаже мусорных товаров или банальном мошенничестве. \n\nЖажда людей до денег просто неискоренима, так что с этой проблемой нам придётся жить ещё многие годы, а может и всегда, пока существует человеческая алчность.\n\nНо если ставить конкретные задачи, то их можно решить хотя бы частично. Например, мы можем эффективно бороться конкретно с почтовым спамом. Цифровая подпись для доменов, белые списки проверенных адресов, шифрование писем — это достаточно надёжный набор технологий, на которые можно положиться.\n\nКонечно, ужесточение защиты иногда приводит к ложным срабатываниям, когда добропорядочные домены попадают в чёрные списки. В наше время практически невозможно рассылать письма с собственного почтового сервера и домена, как это было в 90-е: крупные провайдеры просто заблокируют все ваши письма, независимо от их содержания и количества. Чтобы пробиться через все уровни спамообороны, нужно соблюдать все требования крупных почтовых провайдеров (см. выше) и постоянно проверять свой домен и IP-адрес по чёрным спискам:\n\n\n\nСейчас разрабатываются новые спецификации, которые помогут решить эти проблемы. Например, новый стандарт ARC (Authenticated Received Chain) в дополнение к DMARC обрабатывают ситуации, в которых последний не справляется. Например, он решает проблему DKIM-подписей в почтовых рассылках. \n\n\n\nИ логотип у него симпатичный.",
  "Как реализовывать IT услуги из России в ЕС/США, оставаясь незамеченным для санкций и активистов?": "Пример №1. Вы – владелец IT компании в России и оказываете услуги заказчикам/лицензиатам в ЕС или США.  Есть действующие заказчики и две проблемы:  (а) не проходят платежи в Россию и (б) есть желание скрыть владение российской компанией «на всякий случай».Пример №2. Вы – владелец компании, занимающейся автоматизацией производств. Поставщики оборудования из ЕС, а покупатели в России.  Вы не в сакционных списках, как и ваши покупатели, но неизвестные «активисты» пишут на мэйл и интересуется тем, как же вы закупаете из Европы оборудование и ввозите его в Россию. Такие «письма счастья» приходят по итогу анализа открытых данных из ЕГРЮЛ и купленных в даркнете баз таможенной службы. Цели учредителей в обоих кейса:  (а) продолжить оказывать услуги по поддержке ПО и перепродаже лицензий на рынки ЕС и США из России, продолжить закупку оборудования и (б) скрыть связь владения российской компанией. Первая цель решается либо через открытие транзитного счета юр. лица в Казахстане (описано здесь), либо через открытие филиала российского юр. лица в Казахстане. Вторая решается двумя путями Самурайский – вы создаете ТОО в Казахстане/ОООшку в Узбекистане и ставите ее главой номинала. Не самый надежный вариант, поскольку гражданское законодательство Казахстана даже не предусматривает понятия «номинальный директор» и не регулирует его ответственность перед бенефициаром. Номинал владеет «правом первой подписи», т.е. имеет доступ к банковским счетам и вправе распоряжаться денежными средствами. Проще говоря, ему ничто не мешает взять деньги и исчезнуть. А посредники, предоставляющие услуги номинальных директоров, никаких гарантий погашения убытков вам не дадут.  Разумный – вы выходите из состава своей ООО через корпоративный договор.  Корпдоговор – это договор между участниками ООО и/или третьими лицами о порядке управления компанией (см. ссылку). Он прекрасен тем, что является конфиденциальным и даже в налоговую его не сдают.  Вы выходите из состава соучредителей, оставляя на этой позиции номинала. Далее, заключаете с номиналом корпоративный договор и/или договор конвертируемого займа в пользу компании. В случае нарушения порядка управления компанией номиналу грозят штрафы по корпоративному договору, а конвертируемый займ позволит мгновенно вернуть права на долю в ООО, если «что-то пойдет не так». В ухом остатке, у вас фактический контроль над ООО при полном отсутствии данных о вас в ЕГРЮЛ.Рад помочь по вопросам сделок, контрактов, корпоративных отношений и судебных споров в РФ, Казахстане и международных арбитражах. Искренне Ваш, юридический консультантВасюков Иван@IvanLawyer1990\"Бизнес в Казахстане\", \"Московский юрист в Казахстане\", \"Правовое сопровождение\", \"Релокация бизнеса в Казахстан и Центральную Азию\". ",
  "Как создать личный кабинет B2B: разработка и усиление клиентского сервиса": "Личный кабинет для b2b-клиентов имеет особенности, отличающие его от интерфейса, предназначенного для розничных покупателей. В работе с оптовиками важны не только заказы, но и управление сопутствующей документацией, обработка объемных данных и постоянная связь с менеджерами отдела продаж. Чтобы оптимизировать взаимодействие с корпоративными клиентами, личный кабинет должен включать автоматизированные решения.Преимущества личного кабинетаСамообслуживание для клиентов. С помощью личного кабинета оптовики могут самостоятельно размещать заказы, запрашивать счета, отслеживать доставку и доступ к документам, что сокращает необходимость постоянного общения с менеджерами.Автоматизация документооборота. Оцифровка обмена документами (счета, накладные) снижает нагрузку на отдел продаж и минимизирует ошибки.Облегченная поддержка и рост эффективности. Интеграция личного кабинета с CRM, ERP и другими системами минимизирует необходимость ручного взаимодействия между отделами, автоматизируя рутинные процессы и облегчая поддержку. У менеджеров больше времени на построение долгосрочных отношений с клиентами.Прозрачность и снижение рисков. Стандартизация процессов уменьшает риски ошибок, а клиенты могут самостоятельно отслеживать статус заказов и наличие товара.Масштабируемая архитектура. Грамотная организация базы данных и использование контейнеризации позволяют B2B-решению гибко масштабироваться в зависимости от нагрузки. Это снижает риски возникновения \"узких мест\" и обеспечивает стабильную производительность даже при резких всплесках трафика.Основные функции личного кабинета для пользователейЗаказы и история покупок. Детализированный журнал покупок с фильтрами по статусам и этапам заказа.Программа лояльности. Специальные условия, индивидуальные цены и акции, накопительные баллы.Поддержка и обратная связь. Интерактивные опции для связи с техподдержкой и менеджерами.Контроль финансов. Доступ к финансовым отчетам, балансам и информации по задолженностям.Аналитика и отчеты. Возможность генерировать отчеты для анализа закупок и планирования, финансовые отчеты и контроль задолженностей.Управление профилем и настройками уведомлений. Возможность управления несколькими аккаунтами.Кастомная или типовая разработка: что выбратьСоздание личного кабинета для B2B-клиентов требует учета специфических требований, отличающих его от B2C-платформ. Однако здесь особенно эффективно применять лучшие практики и готовые модули, проверенные временем и способствующие удобству и эффективности. Кастомная разработка нужна только для нетиповых проектов со специфичными интерфейсом или бизнес-логикой. Многие компании не готовы к индивидуальной разработке или просто не нуждаются в ней, и экономия на качестве в таких случаях часто оборачивается более высокими затратами на поддержку и доработку сайта. Поэтому важна сборка по стандартам качества кода с проверенными решениями.Это влияет на:Скорость запуска. Использование готовых компонентов позволяет значительно сократить время на разработку и вывод продукта на рынок,  что особенно важно в условиях высокой конкуренции.Снижение рисков. Готовые решения основаны на лучших практиках и опыте, что уменьшает вероятность ошибок и проблем на этапе эксплуатации.Поддержку. Стандартизированные компоненты проще поддерживать и обновлять, что позволяет команде сосредоточиться на улучшении функционала вместо решения проблем, связанных с индивидуальной разработкой.Масштабируемость. Благодаря модульной архитектуре добавление новых функций и интеграций становится более простым, что позволяет бизнесу расти и адаптироваться к меняющимся условиям.Оптимизацию затрат. Использование типовых решений позволяет снизить затраты на разработку и поддержку, что особенно важно для малых и средних предприятий.Как разработать личный кабинет?1. Определите требованияИзучите потребности пользователей, чтобы понять их ожидания от личного кабинета. Зафиксируйте все функции, которые должны быть реализованы, включая обязательные и желаемые. Используйте методологии Agile, чтобы описать, как пользователи будут взаимодействовать с личным кабинетом. Это поможет команде понять приоритеты.2. Спроектируйте интерфейсПоскольку пользователи B2B-клиентов часто взаимодействуют с личным кабинетом на протяжении долгого времени, интерфейс должен быть интуитивно понятным и удобным. Дизайн личного кабинета должен быть согласован с дизайном сайта для целостного пользовательского опыта. Стартовую страницу кабинета лучше оформить в виде настраиваемого дашборда, отображающего статус заказов, персональные предложения и бонусы, чтобы пользователь мог настроить виджеты под свои нужды.Применяйте фреймворки, такие как React или Vue.js, для создания динамичных интерфейсов, а также инструменты прототипирования, такие как Figma или Adobe XD, для тестирования пользовательских интерфейсов на ранних этапах. Использование технологий lazy loading, кэширования и оптимизации запросов к базе данных поможет ускорить загрузку страниц.3. Выберите технологииПодбирайте технологии, которые обеспечат масштабируемость и безопасность, включая языки программирования, базы данных и интерфейсы API для интеграций с CRM, ERP и другими системами.4. Используйте модульную архитектуруИспользование гибкой архитектуры упрощает добавление новых функций. Так команда сможет легко добавлять новые модули, такие как управление документами, пользователями и анализ данных, без изменений в остальной части системы. Разработанные модули могут быть повторно использованы в других проектах. Использование проверенных модулей и шаблонов сокращает сроки и затраты на разработку, снижая риски ошибок и проблем в эксплуатации.Модульная структура облегчает переход к микросервисам и API, таким как RESTful или GraphQL, что позволяет обрабатывать больший объем данных и пользователей без потери производительности. Например, система может предоставлять API для создания, обновления и удаления заказов.5. Установите строгую систему ролей и прав доступаВ B2B-среде часто задействовано несколько пользователей с различными уровнями доступа. Четкое распределение ролей (менеджеры, бухгалтеры и т. д.) поможет обеспечить безопасность, а также четкое разделение зон ответственности пользователей.Используйте фреймворки для аутентификации и авторизации, такие как OAuth 2.0 которые поддерживают авторизацию через сторонние сервисы, или JWT (JSON Web Tokens), которые позволяют безопасно передавать информацию о пользователе между клиентом и сервером.Также необходимо разработать структуру базы данных, которая будет содержать информацию о пользователях, их ролях и правах. Это может быть реализовано через дополнительные таблицы в реляционной базе данных, где права доступа будут связаны с конкретными ролями и пользователями.6. Обеспечьте безопасность данныхДля B2B-портала безопасность данных особенно критична, так как он оперирует конфиденциальной информацией клиентов. Компрометация такой информации может нанести значительный ущерб как клиентам, так и компании, снижая доверие и лояльность.Важно понять, как управлять рисками, связанными с безопасностью, и планировать работу с разработчиками для их устранения. При этом высокий уровень безопасности может требовать дополнительных затрат на инфраструктуру и регулярное тестирование системы.Установите стандарты и процедуры для обработки данных клиентов.Логируйте действия пользователей все действия внутри системы для анализа и предотвращения подозрительных операций.Обновляйте инфраструктуру и используйте самые последние версии ПО и инфраструктурных компонентов, поскольку в них устранены известные уязвимости.Внедрите двухфакторную аутентификацию. Запланируйте и проводите регулярные проверки безопасности, чтобы выявлять уязвимости и принимать меры по их устранению.Используйте библиотеки для шифрования, такие как OpenSSL, и инструменты для анализа уязвимостей, такие как OWASP ZAP, чтобы обеспечить надежную защиту данных.7. Проводите тестирование на различных этапахТестирование от юнит-тестирования до интеграционного и пользовательского поможет выявить ошибки и улучшить качество продукта. После запуска личного кабинета активно собирайте данные о его использовании, чтобы выявить проблемные области и возможности для улучшения. Запланируйте регулярные обновления функционала на основе собранных данных и обратной связи от пользователей.8. Обучайте пользователей и обеспечьте поддержкуЧтобы новый пользователь быстро освоился с функционалом личного кабинета, внедрите онбординг — подсветите интерфейсные элементы с поясняющими подсказками. Это поможет избежать вопросов и сократит время на обучение работе с платформой.Подготовьте обучающие материалы: руководства, видеоуроки и FAQ для пользователей, чтобы помочь им освоить новый функционал личного кабинета.Организуйте службу поддержки для пользователей, чтобы они могли быстро получить помощь по возникающим вопросам.9. Настройте персонализациюЛичный кабинет должен предоставлять персонализированные предложения на основе истории покупок, специальные скидки и предложения для оптовиков.Важно создать гибкую систему скидок и условий, которая не будет замедлять работу личного кабинета при большом количестве клиентов и решить, как собирать и анализировать данные о клиентах для персонализации (например, сегментация на основе данных из CRM).10. Настройте интеграцию с внешними системамиОпределите процессы, которые требуют автоматизации, чтобы понять, как это может снизить рабочую нагрузку и улучшить клиентский сервис. B2B-клиенты обычно оформляют крупные и регулярные заказы, поэтому важно предоставить им полную картину по каждому этапу выполнения заказа: от подтверждения, упаковки, комплектации, отгрузки до доставки.Например, интеграция с ERP, CRM, платёжными системами, логистическими сервисами обеспечит клиентам доступ к актуальным данным о заказах, остатках, доставке и платежах, устраняя необходимость в ручном обновлении и снижая риск ошибок.Интеграции с ERP, CRM, платежными и логистическими системами можно реализовать через RESTful API, что обеспечит двустороннюю передачу данных и автоматизацию всех процессов. Протоколы SOAP и gRPC могут быть полезны для более сложных запросов.Apache Camel или MuleSoft обеспечивают адаптацию и трансформацию данных для разных систем, что особенно полезно при работе с устаревшими ERP.11. Управляйте данными Поскольку нужно обрабатывать большие объемы данных, важно учитывать способы хранения и анализа данных,  учесть соответствие бухгалтерским стандартам, чтобы обеспечить актуальность информации для клиентов. Раздел с аналитикой и возможностью генерации отчетов по покупкам, частоте заказов, категориям товаров и т.д. помогает клиентам анализировать свои закупки и планировать расходы. В разделе пользователь может выбирать параметры для отчетов и получать графики или таблицы с нужными данными, скачивать все необходимые документы по мере необходимости.Необходимо решить, как использовать данные аналитики для улучшения взаимодействия с пользователями (например, на основе отчётов о поведении клиентов в кабинете) и какие внедрить показатели эффективности для улучшения взаимодействия.Используйте базы данных NoSQL, такие как MongoDB или Cassandra, для гибкого хранения данных, и инструменты для анализа, такие как Apache Kafka или Elasticsearch, для обработки потоков данных в реальном времени.Обеспечьте настройку логов и систем мониторинга (например, ELK Stack, Prometheus) для анализа работы системы.Подключите аналитические сервисы для отслеживания поведения пользователей и дальнейшей оптимизации.12. Обеспечьте масштабируемостьЛичный кабинет должен поддерживать возможность масштабирования по мере роста бизнеса клиента. Настраивайте систему с возможностью горизонтального и вертикального масштабирования для обработки большого трафика.  Для этого можно использовать контейнеризацию с Docker и оркестрацию с Kubernetes, которые обеспечат оптимизацию ресурсов и гибкость при масштабировании, а также упростят развертывание обновлений без перерывов в работе.Также продумайте мобильное приложение или адаптацию для мобильных устройств. 13. Настройте уведомленияОперативные уведомления по email, в личном кабинете, SMS или через мессенджеры помогают клиентам быть в курсе статусов заказов, оплат и других важных событий. Настройте частоту уведомлений, чтобы они были полезными, а не раздражающими.***Разработка B2B-портала — это сложный проект, который требует комплексного подхода. Однако продуманный и тщательно спроектированный личный кабинет сможет стать ключевым инструментом для удержания и развития оптовых клиентов, обеспечивая простоту взаимодействия и доверие к компании.",
  "Миграция данных в Kubernetes: Всё, что нужно знать новичку": "Привет, Хабр! Сегодня у нас на повестке дня тема миграции данных в Kubernetes. Kubernetes — это как швейцарский нож в разработке: умеет всё и сразу. Он не только управляет контейнерами, но и отлично справляется с данными, благодаря таким объектам как PersistentVolume и StatefulSet. PersistentVolume — это механизм для подключения постоянного хранилища, чтобы данные не исчезли при перезапуске контейнера. StatefulSet — это контроллер, который позволяет управлять состоянием приложений и сохранять данные в стабильном виде даже при изменениях в кластере. Подготовка к миграцииПеред тем как начать, убедимся, что всё под рукой:Kubernetes‑кластер: локально с Minikube или в облаке.Kubectl: главный помощник для управления кластером.Helm: пакетный менеджер для Kubernetes, который упростит жизнь (опционально, но очень рекомендую). Ознакомиться можно здесь.Инструменты для миграции данных: pg_dump для PostgreSQL, mysqldump для MySQL и т. д.Хранилище данных: будь то облачное или локальное, поддерживающее PersistentVolumes.Настройка хранилища в KubernetesЧтобы данные были доступны приложениям, необходимо настроить хранилище. В Kubernetes для этого используются PersistentVolume и PersistentVolumeClaim. persistent‑volume.yaml:apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv\nspec:\n  # Указываем объем хранилища, который будет выделен для приложения\n  capacity:\n    storage: 20Gi\n  # Определяем режим доступа: только одно приложение может одновременно читать и писать данные\n  accessModes:\n    - ReadWriteOnce\n  # Задаем политику для PV: данные сохраняются даже после удаления PV\n  persistentVolumeReclaimPolicy: Retain\n  # Задаем путь, где физически будут храниться данные\n  hostPath:\n    path: \"/mnt/data\"persistent-volume-claim.yaml:apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv\nspec:\n  # Указываем объем хранилища, который будет выделен для приложения\n  capacity:\n    storage: 20Gi\n  # Определяем режим доступа: только одно приложение может одновременно читать и писать данные\n  accessModes:\n    - ReadWriteOnce\n  # Задаем политику для PV: данные сохраняются даже после удаления PV\n  persistentVolumeReclaimPolicy: Retain\n  # Задаем путь, где физически будут храниться данные\n  hostPath:\n    path: \"/mnt/data\"\nЗапустим манифестыkubectl apply -f persistent-volume.yaml\nkubectl apply -f persistent-volume-claim.yamlkubectl get pv\nkubectl get pvcЕсли всё прошло гладко, можно будет увидеть, что PVC привязан к PV. Развертываем StatefulSet для базы данныхТеперь пора заняться базой данных. Для этого отлично подходит StatefulSet — специальный ресурс Kubernetes для управления состоянием приложений.Пример манифеста StatefulSestatefulset‑postgres.yaml:apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres\nspec:\n  # Имя сервиса, через который будут обращаться к PostgreSQL\n  serviceName: \"postgres\"\n  replicas: 1  # Количество реплик базы данных (в данном случае одна)\n  selector:\n    matchLabels:\n      app: postgres  # Метка, по которой будет найден под для этого StatefulSet\n  template:\n    metadata:\n      labels:\n        app: postgres  # Метка, присвоенная каждому поду\n    spec:\n      containers:\n        - name: postgres\n          image: postgres:13  # Указываем образ PostgreSQL для использования\n          ports:\n            - containerPort: 5432  # Порт, на котором работает PostgreSQL\n          env:\n            # Переменные окружения для настройки PostgreSQL (логин, пароль, база данных)\n            - name: POSTGRES_USER\n              value: \"admin\"\n            - name: POSTGRES_PASSWORD\n              value: \"password\"\n            - name: POSTGRES_DB\n              value: \"mydatabase\"\n          volumeMounts:\n            # Указываем, куда будут монтироваться данные в контейнере\n            - name: postgres-data\n              mountPath: /var/lib/postgresql/data\n  # Определяем шаблон для динамического создания хранилищ для каждой реплики\n  volumeClaimTemplates:\n    - metadata:\n        name: postgres-data  # Название PVC для базы данных\n      spec:\n        accessModes: [\"ReadWriteOnce\"]  # Одновременный доступ для одного экземпляра\n        resources:\n          requests:\n            storage: 20Gi  # Размер запрашиваемого хранилища\n        storageClassName: standard  # Класс хранилища для PVCПрименяем:kubectl apply -f statefulset-postgres.yamlkubectl get statefulsets\nkubectl get pods\nkubectl get pvcЕсли всё прошло успешно, ваш под PostgreSQL запущен, и PVC привязан. Отлично!Миграция данныхТеперь самое интересное — переносим данные из старой базы в новую, развернутую в Kubernetes.Пример на PostgreSQLСначала на локальной машине выполняем команду для создания дампа базы данных:pg_dump -U postgres -h localhost -p 5432 mydatabase \u0026gt; dump.sqlЧтобы перенести дамп в кластер, создадим ConfigMap:configmap-pg-dump.yaml:apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: pg-dump\ndata:\n  dump.sql: |\n    --  содержимое  dump.sql сюдаЛучше использовать kubectl create configmap с файлом напрямую, но для примера показал вручную.kubectl create configmap pg-dump --from-file=dump.sqlТеперь создаем Job для восстановления данныхjob-pg-restore.yaml:apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pg-restore\nspec:\n  template:\n    spec:\n      containers:\n        - name: pg-restore\n          image: postgres:13  # Используем тот же образ PostgreSQL\n          env:\n            # Пароль для доступа к базе данных\n            - name: PGPASSWORD\n              value: \"password\"\n          # Команда для восстановления базы данных из дампа\n          command: [\"sh\", \"-c\", \"psql -U admin -d mydatabase \u003c /data/dump.sql\"]\n          volumeMounts:\n            # Монтируем ConfigMap с дампом базы данных\n            - name: dump-volume\n              mountPath: /data\n      restartPolicy: OnFailure  # Повторить задачу только в случае ошибки\n      volumes:\n        - name: dump-volume\n          configMap:\n            name: pg-dump  # Название ConfigMap, где хранится дампПрименяем:kubectl apply -f job-pg-restore.yamlkubectl get jobs\nkubectl logs job/pg-restoreЕсли всё прошло успешно, данные будут восстановлены в новой базе данных в Kubernetes. Мониторинг и резервное копированиеМиграция — это только начало. Данные должны быть всегда под контролем. Для этого отлично подходят Prometheus, Grafana и Velero.Установка Prometheus:helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install prometheus prometheus-community/prometheusУстановка Grafana:helm install grafana grafana/grafanaНастроим PersistentVolume для Grafana:persistent-volume-grafana.yaml:apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: grafana-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: standardПрименяем PVC:kubectl apply -f persistent-volume-grafana.yamlНастроим бэкапы с VeleroЕсли ты не хочешь обнаружить себя в 2 часа ночи с осознанием, что все данные потеряны, настроить резервное копирование с помощью Velero — это обязательный шаг. mongodump --db mydatabase --out /path/to/backupСоздаем бэкап:kubectl create configmap mongo-dump --from-file=/path/to/backupВосстановливаем:velero restore create --from-backup my-backupМожно создать CronJob для регулярного создания бэкапов. cronjob-velero-backup.yaml:apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: velero-backup\nspec:\n  # Расписание: бэкап будет запускаться каждый день в 2 часа ночи\n  schedule: \"0 2 * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n            - name: velero\n              image: velero/velero:v1.6.0  # Образ Velero для выполнения бэкапа\n              command:\n                # Команда для создания ежедневного бэкапа\n                - /velero\n                - backup\n                - create\n                - daily-backup\n                - --include-namespaces=default\n          restartPolicy: OnFailure  # Перезапуск только при неудачахkubectl apply -f job-mongo-restore.yamlТеперь бэкапы будут создаваться автоматически.Прежде чем мигрировать большие базы, попробуйте на маленьких. Это поможет выявить проблемы заранее.Можно интегрировать процесс миграции в конвейеры CI/CD. Автоматизация приводит к снижению вероятности ошибок.Ведите подробную документацию. Больше актуальных навыков по IT-инфраструктуре вы можете получить в рамках практических онлайн-курсов от экспертов отрасли. Также приглашаем на открытый урок 6 ноября.На уроке рассмотрим популярные инструменты для сбора логов, такие как Vector, Fluentbit и Promtail. Узнаем, в чём их преимущества, как они работают и где лучше применять. Подробно разберём, какой функционал предлагают эти решения для управления логами. Записаться на урок",
  "Нужно ли запретить ИТ-курсы?": "За что ругают курсыБудучи сторонником капиталистических отношений между акторами рынка, в том числе рынка труда, я всегда недоумевал, зачем кто-то ругает все эти бесчисленные курсы, если они существуют в конкурентной среде, предоставляют качественные услуги, о которых во времена юности миллениалов можно было только мечтать.Можно было бы попенять на то, что все эти проклятые курсы торгуют несбыточными ожиданиями людей, рассказывают красивые сказки о красивой жизни, несметных богатствах. Действительно, маркетинговые уловки создают впечатление, что можно легко и быстро изучить необходимые навыки и далее получать 300кк/нсек. Конечно это не так ни на стартовых, ни на старших позициях.Но давайте посмотрим правде в глаза - для очень многих позиций те навыки, которые требуют для прохождения собеседований, абсолютно избыточны. Механически заучить конструкции фреймворков и заучить критерии применения тех или иных конструкций, вполне можно за время курсов. Применения обычных человеческих навыков в виде здравого смысла, логики и способности уточнить у аналитика непонятные моменты - вполне достаточно для реализации достаточно сложной бизнес-логики. Способность погрузиться в предметную область зачастую важнее технических навыков. Кстати это та причина, по которой мы все еще не можем полноценно заменять людей ИИ и почему в последнее время растет значение софт-скиллов.Кроме того, я нередко слышу, что возможна оплата курсов уже после устройства на работу, что само по себе немного ослабляет аргумент о торговле несбыточными ожиданиями.Будут ли снижаться зарплаты?Возможно, что часть критиков курсов имеет глубоко личные причины недовольства, например страх снижения зарплат за счет насыщения спроса на специалистов. Эта причина на самом деле выглядит самой рациональной, но вместе с тем обоснованность такой позиции под вопросом, поскольку бум джунов в ИТ длится довольно долго, а спрос самый высокий на мидлов. Если бы самый высокий спрос был на сеньоров, можно было бы списать на лаг времени обучения, но спрос все-таки на мидлов.Следовательно либо фильтрация на реальных задачах отсеивает очень много джунов при стабильном спросе, либо потребность в рабочих руках растет все равно быстрее, чем нарастающий поток джунов.Из этого можно предположить, что размер зарплат будет меняться по причинам скорее связанным с общим состоянием экономики, нежели чем с потоком джунов с курсов.Кому в самом деле вредят курсыHR (мы часто их не любим, но все же)Первые, кто приходит в голову - это рекрутеры. Нагрузка на тех, кто вынужден закрывать начальные позиции серьезно возрастает. Сотни откликов на вакансии приводят к уменьшению вероятности найти талантливого соискателя, поскольку на первое место выходит навык правильного написания резюме, которое должно пройти первичный отбор. Не уверен, что есть корреляция между техническими навыками и навыками заказа услуги по написанию резюме.Технические специалисты (мы с вами)Кроме рекрутеров страдают и технические специалисты, которых привлекают на следующий этап отбора кандидатов. Поскольку количество кандидатов растет, то растет и время, необходимое на проведение собеседований. Происходит ожидаемая механизация собеседований - решение задачек на литкоде, вместо совместного решения более приближенных к реальности задачек, которое лучше раскроет способности и стиль мышления кандидата. Соответственно здесь проверяется скорее навык прохождения интервью и литкодинга, нежели реальная полезность кандидата. В дальнейшем лиды вынуждены больше возиться с джунами, мельче нарезать задачки, чаще консультировать.Архитекторы (тоже мы с вами)Очень опосредованно страдают даже архитекторы. Из-за того, что пирамида квалификации сильно утолщается в части джунов, а качество джунов часто страдает из-за причин перечисленных выше, архитекторы вынуждены учитывать снижение средней квалификации команды и применяют решения, которые могут усложнить систему в общем, ради упрощения составных частей. Самый очевидный пример - это попытки выстроить микросервисные архитектуры с их очевидными недостатками в тех случаях, когда можно было бы применить Service Based Architecture или даже монолит, при условии наличия высококвалифицированных разработчиков. В результате получаем проблемы несоразмерные получаемым преимуществам.КурсантыНу и больше всего курсы вредят самим курсантам. Несмотря на то, курсы не бросают курсантов после прохождения и заинтересованы в их трудоустройстве, ореол вокруг ИТ рисуется не до конца реалистичный. Преувеличиваются и зарплаты, и условия труда, и карьерные возможности. Это приводит к тому, что тратятся не только деньги, но и время и силы, которые можно было потратить, например, на повышение квалификации в своей области, либо на вкатывание в менее престижные, но не менее денежные области. Я без тени иронии мог бы многим посоветовать идти в сантехники, которые могут зарабатывать не хуже мидлов, если потратить время и силы на наработку клиентской базы и репутации.Кому выгодны курсыМотивированным курсантамЯ говорю прежде всего о тех, кто имеет не только денежную мотивацию, но и получает удовольствие от самого процесса, который нашел себя в профессии разработчика или DevOps. например.Такие люди рано или поздно найдут работу в ИТ, в то время как здравомыслящие курсанты, у которых душа не лежит к ИТ, в какой то момент времени выберут другой вектор.При этом курсы дают качественную базу, которую трудно развить без мотивации, но легко и приятно, когда каждый шажок подкрепляется естественным дофаминчиком.HRУ меня нет на руках никакой статистики, кроме собственного субъективного опыта. Но мне кажется, что из-за увеличения количества работы по поиску и отбору довольно значительно растет количество рекрутеров. Растет количество, растет спрос, растет вознаграждение.Менеджмент (на галерах)С большой очередью за забором, типичный галерный менеджмент может маневрировать с большей свободой на стадии пресейлов, например. Не обязательно иметь команду, ее всегда можно собрать после получения надежного контракта. И если это практиковалось в общем то всегда, но без очереди за забором приходится больше осторожничать. А сейчас можно набрать десяток другой джунов за неделю, выдернуть мидла из смежной команды и повесить ему лычку тимлида, а за время жизни проекта (год-два) плохие джуны уйдут, хорошие станут мидлами и затащат ценой бессонных ночей и работы по выходным.ВыводыК сожалению, несмотря на то, что курсы приносят некоторую пользу, они также приносят вред. Основной вред является производной от того, что разница между маркетинговой картинкой и реальной реальностью становится все больше.При этом, весь обсуждаемый процесс происходит в рамках рыночной парадигмы, а реклама. создающая несбыточные ожидания используется не только маркетологами курсов.Та же рыночная парадигма влияет на действия всех участников процесса - считать косвенные и отдаленные последствия трудно, зачастую мотивация линейного менеджмента не зависит от глубоко спрятанных последствий, решения принимаются так, как выгодно здесь и сейчас.Что можно сделать в сложившейся ситуации? А ничего делать не нужно.Сложившаяся ситуация на мой взгляд прекрасна. Это как раз положительный пример того, что «рыночек порешает». Рынок бурлит, развивается, люди приходят и уходят, конкуренция велика с обеих сторон, нет застоя. В том числе конкуренция курсов между собой, что приводит к созданию отличного продукта (хотя я бы добавил софт‑скиллов и предметных областей).А победит, как всегда, самый сильнейший приспособленный.",
  "Обучение ИИ-моделей на облачных серверах: совместный проект РБК и Рег.ру": "Привет, Хабр! С вами Сергей Рыжков, руководитель департамента хостинга и профессиональных сервисов Рег.ру, и Александр Михеев, ML-engineer РБК. В этой статье расскажем, как мы автоматизировали процесс тегирования редакционных материалов РБК с помощью нейросети в облаке Рег.ру. О первых результатах нашего эксперимента читайте под катом.Навигация по тексту:→ Зачем начали обучать нейросеть→ Как проверяли гипотезу и проводили эксперименты\t↪ 1. Выбор ИИ-модели\t↪ 2. Эксперименты с обучением\t↪ 3. Тестирование оборудования\t↪ 4. Проверка гипотезы→ Результаты проектаЗачем начали обучать нейросетьТегирование материалов — неотъемлемая часть работы любой медиакомпании. В РБК ее выполняли вручную: редакторы прописывали 2–3 тега для каждого материала. В связи с этим возникало несколько трудностей: Количество тегов неконтролируемо увеличивалось — в основном разделе РБК до внедрения системы их набралось десятки тысяч.Появились дубли и синонимичные теги. Например, тег «сыр» можно написать по-разному: использовать заглавные буквы или кавычки. С точки зрения системы «Сыр», «сыр» и «сыры» — разные теги. Из-за большого числа тегов стало сложнее выбирать релевантные, и оставался риск пропустить другие подходящие.Тогда решили проверить гипотезу, насколько нейросеть потенциально может нивелировать человеческий фактор и позволит организовать процесс тегирования в полуавтоматическом режиме. Редактору можно будет не отвлекаться на поиск тегов, а только утверждать предложенные ИИ. В этом случае рутины станет меньше, и у сотрудников редакции появится время на по-настоящему важные задачи. Как проверяли гипотезу и проводили эксперименты1. Выбор ИИ-моделиПри выборе ИИ-модели одним из главных критериев было понимание русского языка. В РБК редакторы работают со сложно структурированными материалами на разные тематики, и чем лучше модель будет «понимать» русский язык, тем выше качество ее работы. За основу выбрали SOTA Open Source решение — архитектура T5. Это разработка Google, которую использовали для обучения модели FRED. Она обучалась на русском языке 35 дней на 160 графических процессорах V100 и 5 дней на 80 процессорах A100. Во время реализации проекта тестировали обе модели, и впоследствии перешли на оригинальную архитектуру T5 — большую мультиязычную модель MT5 от Google с более современным токенайзером.2. Эксперименты с обучением Начали с того, что разработали отдельный сервис для подготовки «чистого», более компактного списка на основе тех самых десятков тысяч тегов. В дальнейшем этот white list пригодится для создания датасета («корректные» теги + размеченные ими материалы), на основе которого будет обучаться модель.Первичный цикл обучения проводился на локальной видеокарте на мощностях РБК и составил 8 дней непрерывной работы. Нейросеть мгновенно подбирала теги, а процент ошибок был незначительный. Было ясно, что дообучение ИИ потребует дополнительных вычислительных ресурсов. Поэтому процесс решили перенести в облако с более мощным железом. Следующим шагом необходимо было подобрать оптимальную конфигурацию инфраструктуры, которая с одной стороны позволит сократить время на дообучение модели, а с другой — не будет избыточной по соотношению затраты/производительность. 3. Тестирование оборудования Специальной для этой задачи инженеры Рег.ру подготовили тестовый стенд с видеокартой А5000 на базе облачных серверов с GPU. Производительность стенда сравнили с двумя другими конфигурациями: Т4 и RTX 3090. В таблице ниже представлены результаты тестирования. Значения it/s —- это количество обрабатываемых материалов за один шаг в рамках эпохи, этот показатель отражает скорость обучения. По вертикали указано количество батчей в процессе параллельного выполнения задач обучения.Out — означает ошибку out of memory, то есть объема вычислительной памяти не хватило для размещения задачи. Подпись (gen) в модели — что обучение было на задачу text2text generation, а (pred) —- sequence classification.По итогу проведенных тестов видеокарта А5000 в облаке Рег.ру показала лучшие результаты. Она оснащена графической памятью объемом 24 ГБ и позволяет обрабатывать бо́льшие объемы данных без замедления работы. К тому же видеокарта поддерживает APICUDA и DirectML и совместима с большинством нейросетевых библиотек и приложений. 4. Проверка гипотезыТестирование автоматической разметки материалов тегами проводили на разных проектах РБК, включая «РБК Тренды», «РБК Отрасли» и «РБК Life». Суммарно это более 25 тысяч материалов. Для перепроверки использовали уже существующие материалы, размеченные редакторами, — публикации загружали в нейросеть и сравнивали результаты машины и человека. На основе полученных результатов делали выводы о том, насколько релевантные теги подобрала модель. Проведенный тест подтвердил работоспособность подхода. После успешного первичного тестирования и до внедрения в админку для публикации статей, ИИ-модель интегрировали в редакционные процессы «РБК Трендов» с помощью промежуточного решения в виде телеграм-бота. Результаты проектаПроцесс обучения ИИ-модели в облаке удалось уместить всего в 14-15 часов за один цикл обучения. Точность подбора тегов составила  99%, а скорость тегирования одного материала экстремального сократилась до 0,03 секунды. При этом различалось число тегов. Например, редактор ставил 3 тега, а ИИ-модель выдавала 7 тегов, среди которых в 99% случаев были исходные 3.  В таблице ниже представлены временные значения запуска ИИ-модели в сравнении в разных конфигурациях.Init — время загрузки модели в RAM / VRAM.Pred — время выполнения задачи до получения результатов. Значение времени является усредненным для 1 000 запущенных задач тегирования. Для оценки использовались два материала «РБК Трендов»: первый длиной ~200 токенов (по GPT4 токенайзеру), второй ~2000 токенов.В качестве наглядного примера рассмотрим демо в «РБК Трендах». Редактор указал 4 тега для материала, мы попросили ИИ-модель предсказать 6 тегов по тексту. Ниже представлены результаты двух моделей, обученных на 18 и 30 эпох, — для сравнения. В результате все 4 редакционных тега вторая модель предсказала верно.Что касается возможных ошибок в работе нейросети, то безусловно такую вероятность нельзя не учитывать, поэтому мы говорим не про автоматическое, а про автоматизированное решение. То есть нейросеть подбирает теги, а редактор визирует и либо принимает, либо правит их. Это позволяет исключить возможные ошибки нейросети, но при этом сохранить все преимущества, которые дал проект.В ходе эксперимента мы поняли и перспективы дальнейшего развития. Нейросеть может позволить давать более персонифицированные рекомендации, улучшить пользовательский опыт и повысить вовлеченность аудитории. Но это нас ждет еще впереди.",
  "Отладка и мониторинг в MobX: trace, introspection и spy": "Привет, меня зовут Дмитрий, я Middle-React-разработчик с замашками сеньора, поднимающийся с самых низов без мам, пап и ипотек. В последнее время я частенько вижу ситуацию: при использовании MobX в больших проектах у людей появляются сложности с количеством перерисовок или наоборот не обновлением данных со стора. Также могут проявляться проблемы с производительностью в том числе и из-за этого. Я решил поделиться отладочными инструментами MobX, ведь это может кому пригодиться.Реактивное программирование и состояние в MobX    Немного справочной информации про концепцию реактивного программирования. Реактивное программирование — это концепция, где данные и действия синхронизируются автоматически. Если что-то меняется в одном месте, это изменение каскадно влияет на все связные элементы приложения. MobX помогает превратить объекты JS в структуры данных, которые отслеживают изменения и обновляют приложение в реальном времени. Основными инструментами MobX являются:Observable (наблюдаемые): свойства, которые реагируют на изменения данных.Computed (вычисляемые): зависимости, которые  пересчитываются только при необходимости.Reactions (реакции): побочные эффекты, выполняемые в ответ на изменения в наблюдаемых данных.Используя эти элементы, MobX создает реактивные цепочки. Изменения в observables автоматически вызывают пересчет зависимых computed и реакций, что позволяет минимизировать ручное управление состоянием.  Но когда зависимостей и переменных становится много, бывает тяжело понять, откуда что обновилось и что вызвало перерендер. Для решения такой проблемы существуют инструменты отладки, о которых я постараюсь рассказать.Под капотомОдной из ключевых особенностей MobX, начиная с версии 5, является использование прокси-объектов, которые позволяют эффективно перехватывать изменения в состоянии и управлять зависимостями между данными и реакциями.MobX создает реактивные объекты через прокси, что позволяет \"перехватывать\" каждое обращение к свойствам этих объектов. Этот механизм дает возможность MobX отслеживать зависимости между данными в реальном времени, как только мы обращаемся к какому-либо свойству. Функции MobX:Создание наблюдаемых свойств: MobX оборачивает объекты в прокси, отслеживая доступ ко всем их свойствам и регистрируя зависимости. Например, если в реакции используется свойство объекта, MobX автоматически добавляет это свойство в зависимости реакции.Инвалидация при изменениях: Прокси позволяют MobX сразу же узнавать, когда какое-то наблюдаемое значение изменяется. Если в MobX-объекте обновляется свойство, то через прокси MobX \"инвалидирует\" все вычисляемые значения и реакции, которые зависят от этого свойства, и автоматически пересчитывает их. Простота управления состоянием: Благодаря прокси, MobX не требует дополнительных обёрток для каждого свойства. Все новые свойства, которые добавляются к объектам, также сразу становятся реактивными, что делает MobX простым в использовании.Использование прокси позволило MobX упростить и оптимизировать реактивность, так как только зависимости, затронутые изменениями, пересчитываются. Это увеличивает производительность и снижает потребление ресурсов, обеспечивая плавное обновление интерфейса при изменении данных.Зачем нужны инструменты для отладки и мониторинга?MobX требует контроля со стороны разработчика: необходимо убедиться, что реактивные связи и состояния обновляются только тогда, когда это действительно необходимо. Если зависимости не настроены корректно, это может привести к ненужным пересчетам и обновлениям, что может негативно сказаться на производительности. Кроме того, ошибки, связанные с реактивностью, не всегда очевидны и могут проявляться в неожиданном поведении.С помощью таких инструментов, как trace, introspection и spy, разработчик получает возможность следить за всем, что происходит в реактивных недрах проекта, и использовать MobX максимально эффективно. Отладка может быть сложной задачей, т.к требуется не только понять, какие изменения происходят, но и выяснить, что именно инициирует их. Давайте начнем с инструмента trace().Что такое trace и как он работает?Trace() в MobX — это встроенный метод для отслеживания реактивных связей. Когда мы вызываем trace() внутри функции autorun или computed, MobX выводит детальную информацию о том, какие именно зависимости вызвали её пересчёт.  Также trace() можно вызвать и просто в компоненте и если запустим приложение, то в момент обновления переменной получим точку дебага в браузере. Это полезно для ситуаций, когда вы пытаетесь понять, почему какое-то вычисляемое значение или реакция обновляется чаще, чем ожидается.Trace() позволяет увидеть, какие observable свойства участвуют в вычислении. Это помогает идентифицировать потенциальные проблемы с лишними перерисовками. С помощью этой информации можно более эффективно управлять состоянием и устранять неочевидные ошибки в реактивной логике.Примеры использования trace    Рассмотрим пример, где trace помогает разобраться в работе autorun и computed. Предположим, у нас есть класс Store с наблюдаемыми свойствами a и b, а также вычисляемое свойство sum.import { autorun, trace, makeAutoObservable } from \"mobx\";\n\nclass Store0 {\n  a = 10;\n  b = 20;\n\n  constructor() {\n    makeAutoObservable(this); \n  }\n\n  // Сеттер для свойства a\n  setA(value) {\n    this.a = value;\n  }\n\n  // Вычисляемое свойство sum\n  get sum() {\n    trace(); // Включаем trace внутри computed\n    return this.a + this.b;\n  }\n}\n\nconst store0 = new Store0();\n\n// Создаем autorun для автоматического обновления при изменении sum\nautorun(() =\u003e {\n  trace(); // Включаем trace внутри autorun\n  console.log(\"Сумма:\", store0.sum);\n});\n\nexport default store0;\nА также я создал компонент для вывода и изменения переменных. import React from \"react\";\nimport { observer } from \"mobx-react-lite\";\nimport store0 from \"./Store0\";\n\nconst Store0Component = observer(() =\u003e {\n  const handleAChange = (event) =\u003e {\n    const newA = parseInt(event.target.value, 10);\n    store0.setA(isNaN(newA) ? 0 : newA);\n  };\n\n  return (\n    \u003cdiv\u003e\n      \u003ch2\u003eStore0 Variables\u003c/h2\u003e\n      \u003cp\u003eValue of a: {store0.a}\u003c/p\u003e\n      \u003cp\u003eValue of b: {store0.b}\u003c/p\u003e\n      \u003cp\u003eSum (a + b): {store0.sum}\u003c/p\u003e\n\n      \u003ch3\u003eUpdate Value of a\u003c/h3\u003e\n      \u003clabel\u003e\n        a:\n        \u003cinput type=\"number\" value={store0.a} onChange={handleAChange} /\u003e\n      \u003c/label\u003e\n    \u003c/div\u003e\n  );\n});\n\nexport default Store0Component;\nВ этом примере, когда значения a или b изменяются, computed sum- свойство пересчитывается, и autorun вызывается для вывода нового значения суммы. Благодаря trace() MobX будет выводить информацию о том, какие зависимости (в данном случае a и b) привели к пересчету.Теперь представим, что мы изменяем значение a:Вот что выведет в консоль.Сообщения, которые выводит spy и trace, помогают понять последовательность изменений и реакций в Store0. Давайте разберём каждое событие:Spy event - action:   Spy event: {type: 'action', name: 'setA', object: Store0, arguments: Array(1), spyReportStart: true}Это означает, что был вызван метод setA (названный \"action\" в MobX). Он изменяет значение a и запустит отслеживание, когда его вызвали. Сообщение spyReportStart: true указывает на начало выполнения setA.    Spy event - update (observable a):Spy event: {type: 'update', observableKind: 'object', debugObjectName: 'Store0@5', object: Store0, oldValue: 10, newValue: 11}После вызова setA, MobX зафиксировал изменение в a: старое значение (oldValue) было 10, а новое значение (newValue) стало 11. Это событие обновления наблюдаемого объекта a.    MobX trace - computed sum:    [mobx.trace] 'Store0@5.sum' is invalidated due to a change in: 'Store0@5.a'Здесь trace показывает, что вычисляемое значение sum стало \"невалидным\" из-за изменения a. Это означает, что MobX понимает, что sum нужно пересчитать, поскольку оно зависит от a.    Spy event - computed update (sum):    Spy event: {observableKind: 'computed', debugObjectName: 'Store0@5.sum', object: Store0, type: 'update', oldValue: 30, newValue: 31}После пересчета sum его значение изменилось с 30 на 31, и это зафиксировано как обновление вычисляемого свойства.    MobX trace - autorun:    [mobx.trace] 'Autorun@6' is invalidated due to a change in: 'Store0@5.sum'trace показывает, что autorun был запущен снова, поскольку sum обновился. autorun пересчитывается каждый раз, когда sum (или любое наблюдаемое свойство внутри него) изменяется.Spy event - reaction (autorun):Spy event: {name: 'Autorun@6', type: 'reaction', spyReportStart: true}MobX зафиксировал, что autorun запускается как реакция на обновление sum, и реактивный код выполняется снова.Console output - Updated sum:Сумма: 31Это вывод из autorun, который печатает новое значение sum в консоль.    Когда использовать trace?    trace полезен в следующих случаях:    Отладка неожиданных обновлений и улучшение производительности. Когда какое-то computed свойство или autorun обновляется чаще, чем нужно, trace помогает понять, какая      зависимость инициирует эти обновления.Анализ сложных зависимостей. В больших приложениях с множеством      взаимозависимых observable и computed свойств trace помогает визуализировать      связи, что упрощает понимание для разработчика.С помощью trace можно быстро выявлять и устранять лишние перерендеры и получать полное представление о том, как именно MobX управляет реактивными зависимостями в приложении.    Introspection в MobX    Есть еще один инструмент для анализа. Introspection дает разработчику возможность заглянуть внутрь реактивных объектов и получить информацию о структуре зависимостей, типах свойств и текущем состоянии каждого из них.Что такое introspection и какие методы предоставляет MobX?Introspection— это набор методов, которые позволяют анализировать и получать доступ к информации о реактивных объектах, таких как observable свойства и computed значения. Эти методы помогают выяснить, является ли объект наблюдаемым, получить список его зависимостей и определить, какие элементы участвуют в вычислениях. Возможность использовать introspection важна для более глубокого понимания структуры реактивных цепочек и диагностики проблем в больших приложениях.Основные методы introspection в MobX:isObservable — проверяет, является ли      объект или его свойство наблюдаемым.isComputedProp — определяет, является ли      свойство вычисляемым (computed).getDependencyTree — отображает структуру      зависимостей для реактивного объекта, что позволяет понять, какие      observable или computed свойства влияют на его значение.getObserverTree — показывает, кто «подписан»      на данный объект, т.е. какие реакции или вычисления зависят от него.Эти методы дают более полное представление о том, как устроены и взаимодействуют друг с другом различные элементы реактивного состояния.Примеры использования introspection в сложных структурахВ качестве примера я придумал более сложный стор, чем в предыдущем примере. У него есть вложенные объекты, несколько уровней наблюдаемых свойств и вычисляемых значений.  Версия MobX в данном примере \"mobx-react-lite\": \"^4.0.7\". Обращайте на это внимание, потому что синтаксис со старыми версиями, где были декораторы отличается.import { makeAutoObservable } from \"mobx\";\n\nclass Store {\n  user = {\n    name: \"Alice\",\n    age: 30,\n    settings: {\n      theme: \"dark\",\n      notifications: true,\n    },\n  };\n\n  activities = [\n    { title: \"Jogging\", duration: 30 },\n    { title: \"Coding\", duration: 120 },\n  ];\n\n  constructor() {\n    makeAutoObservable(this);\n  }\n\n  get totalActivityDuration() {\n    return this.activities.reduce(\n      (sum, activity) =\u003e sum + activity.duration,\n      0\n    );\n  }\n\n  get userInfo() {\n    return `${this.user.name}, Age: ${this.user.age}`;\n  }\n}\n\nconst store = new Store();\n\nexport default store;\nТакже я создал компонент MyComponent, в котором мы хотим проверить, что от чего зависит и наблюдается ли.import React, { useEffect } from \"react\";\nimport { observer } from \"mobx-react-lite\";\nimport { isObservable, isComputedProp, getDependencyTree } from \"mobx\";\nimport store from \"./Store\";\n\nconst MyComponent = observer(() =\u003e {\n  useEffect(() =\u003e {\n    // Проверка, является ли user наблюдаемым объектом\n    console.log(\"user is observable:\", isObservable(store.user)); // true\n    console.log(\n      \"user.settings is observable:\",\n      isObservable(store.user.settings)\n    ); // true\n\n    // Проверка, является ли totalActivityDuration вычисляемым\n    console.log(\n      \"totalActivityDuration is computed:\",\n      isComputedProp(store, \"totalActivityDuration\")\n    ); // true\n\n    // Получение дерева зависимостей для userInfo\n    console.log(\n      \"Dependency tree for userInfo:\",\n      getDependencyTree(store, \"userInfo\")\n    );\n\n    // Получение дерева зависимостей для totalActivityDuration\n    console.log(\n      \"Dependency tree for totalActivityDuration:\",\n      getDependencyTree(store, \"totalActivityDuration\")\n    );\n  }, []); \n\n  return (\n    \u003cdiv\u003e\n      \u003ch1\u003eUser Info: {store.userInfo}\u003c/h1\u003e\n      \u003cp\u003eTotal Activity Duration: {store.totalActivityDuration}\u003c/p\u003e\n    \u003c/div\u003e\n  );\n});\n\nexport default MyComponent;\nТогда при запуске MyComponent в консоль выведется следующее:    По выводу мы сразу понимаем, что наблюдаемо, а также видим структуру. Думаю, комментарии излишни.  Spy в MobXИногда для полноценного понимания работы реактивного состояния требуется видеть все события, происходящие в MobX, в реальном времени. Именно для этого и служит инструмент spy. Это отладочный инструмент, который отслеживает все изменения в MobX и выводит их в консоль. Использование spy позволяет буквально заглянуть «под капот» реактивной системы и увидеть, какие действия происходят в каждый момент времени, будь то изменение observable, запуск computed функции или выполнение эффекта. Что такое spy и как он работает?spy — это метод, который позволяет подписаться на все события, происходящие в MobX. С помощью spy можно отслеживать любые изменения состояния и действий, таких как обновления наблюдаемых значений, пересчёты вычисляемых свойств и срабатывание реакций.Каждое событие, отслеживаемое spy, включает тип события (например, \"update\" для изменения значения observable или \"compute\" для пересчета computed), а также дополнительную информацию, такую как старое и новое значения для observable свойств. Это позволяет видеть полную картину изменений в приложении и обнаруживать неожиданные действия или неэффективные пересчёты.Примеры использования spyИзменим наш предыдущий store, добавив геттеры и сеттеры, чтобы можно было менять значение из компонента.import { makeAutoObservable } from \"mobx\";\n\nclass Store2 {\n  user = {\n    name: \"Alice\",\n    points: 100,\n    status: \"active\",\n  };\n\n  levelMultiplier = 2;\n\n  constructor() {\n    makeAutoObservable(this);\n  }\n\n  // Сеттер для user.points\n  setUserPoints(points) {\n    this.user.points = points;\n  }\n\n  // Сеттер для levelMultiplier\n  setLevelMultiplier(multiplier) {\n    this.levelMultiplier = multiplier;\n  }\n\n  // Вычисляемое свойство: базовый уровень\n  get baseLevel() {\n    return Math.floor(this.user.points / 100);\n  }\n\n  // Вычисляемое свойство: уровень с учетом множителя\n  get adjustedLevel() {\n    return this.baseLevel * this.levelMultiplier;\n  }\n\n  // Вычисляемое свойство: статус игрока в зависимости от уровня\n  get userStatus() {\n    return this.adjustedLevel \u003e 5 ? \"VIP\" : this.user.status;\n  }\n}\n\nconst store2 = new Store2();\n\nexport default store2;\nСделаем новый компонент для примера StoreComponent. Который будет отображать и изменять наши переменные в Store2. И подключим spy, чтобы увидеть изменения в реактивной цепочке, которая включает наблюдаемые и вычисляемые свойства.    import React from \"react\";\nimport { observer } from \"mobx-react-lite\";\nimport store2 from \"./Store2\";\nimport { spy } from \"mobx\";\n\nconst StoreComponent = observer(() =\u003e {\n\n    spy((event) =\u003e {\n        console.log('Spy event:', event);\n    });\n      \n\n  const handlePointsChange = (event) =\u003e {\n    const points = parseInt(event.target.value, 10);\n    store2.setUserPoints(isNaN(points) ? 0 : points);\n  };\n\n  const handleMultiplierChange = (event) =\u003e {\n    const multiplier = parseInt(event.target.value, 10);\n    store2.setLevelMultiplier(isNaN(multiplier) ? 1 : multiplier);\n  };\n\n  return (\n    \u003cdiv\u003e\n      \u003ch2\u003eUser Info\u003c/h2\u003e\n      \u003cp\u003eName: {store2.user.name}\u003c/p\u003e\n      \u003cp\u003ePoints: {store2.user.points}\u003c/p\u003e\n      \u003cp\u003eStatus: {store2.user.status}\u003c/p\u003e\n\n      \u003ch2\u003eLevels\u003c/h2\u003e\n      \u003cp\u003eBase Level: {store2.baseLevel}\u003c/p\u003e\n      \u003cp\u003eAdjusted Level: {store2.adjustedLevel}\u003c/p\u003e\n      \u003cp\u003eUser Status: {store2.userStatus}\u003c/p\u003e\n\n      \u003ch2\u003eAdjust Points and Level Multiplier\u003c/h2\u003e\n      \u003cdiv\u003e\n        \u003clabel\u003e\n          Points:\n          \u003cinput\n            type=\"number\"\n            value={store2.user.points}\n            onChange={handlePointsChange}\n          /\u003e\n        \u003c/label\u003e\n      \u003c/div\u003e\n      \u003cdiv\u003e\n        \u003clabel\u003e\n          Level Multiplier:\n          \u003cinput\n            type=\"number\"\n            value={store2.levelMultiplier}\n            onChange={handleMultiplierChange}\n          /\u003e\n        \u003c/label\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n  );\n});\n\nexport default StoreComponent;\nЗапускаем и прям из интерфейса давайте поменяем значение points или levelMultiplier и посмотрим, какие события будут зафиксированы при изменении points на 101 и levelMultiplier, например на 3;В консоле мы увидим для store.user.points структуру логов вида: Строк много и поначалу, кажется, что ничего не понятно. Спешу вас разубедить:) В целом, по этим логам можно будет понять, что было событие update для user.points, которое, изменило значение с 100 на 101. Это изменение инициирует пересчет baseLevel, так как baseLevel зависит от user.points. Затем adjustedLevel также пересчитывается, поскольку он зависит от baseLevel и levelMultiplier. Наконец, userStatus пересчитывается, поскольку его значение зависит от adjustedLevel.И для store.levelMultiplier будет похожая структура, по которой будет видно изменение значения levelMultiplier с 2 на 3. Это изменение инициирует пересчет adjustedLevel, т.к. он зависит от levelMultiplier. После этого userStatus пересчитывается, так как его значение зависит от adjustedLevel.Но будьте осторожны, в консоль может высыпаться столько, что все повиснет.Когда использовать spy?Инструмент spy оказывается особенно полезен в следующих ситуациях:Поиск неожиданных изменений состояния. Оптимизация реактивных цепочек. Обнаружение побочных эффектов. Включение spy может дать разработчику полное представление о том, что происходит внутри MobX, позволяя не только находить ошибки, но и выявлять и устранять возможные проблемы с производительностью.Заключение    MobX предоставляет разработчикам инструменты для отладки и мониторинга, такие как trace, introspection и spy. Каждый из этих инструментов выполняет свою роль и помогает понять, как работает реактивное состояние и какие зависимости его формируют. Так чем же они отличаются и в каких ситуациях их использовать?Сравнение trace, introspection и spyTrace: фокусируется на реактивных зависимостях. Этот инструмент помогает видеть, какие observable свойства вызывают пересчёт computed значений и реакций. Лучше всего подходит для анализа и оптимизации конкретных цепочек реактивных обновлений, когда нужно понять, что вызывает пересчёт и почему.Introspection: служит для анализа структуры реактивных объектов. С его помощью можно определить, является ли свойство observable или computed, а также получить дерево зависимостей или наблюдателей для определённых значений. Инструмент особенно полезен для детального анализа состояния и выявления зависимостей в крупных проектах.Spy: глобальный инструмент для мониторинга всех событий в MobX. Отслеживает любые изменения в observable, computed пересчёты и реакции в реальном времени, выводя в консоль каждое событие. Полезен для получения полной картины состояния и поиска неожиданных изменений или избыточных пересчётов.Примеры, в каких случаях какой инструмент предпочтительнее использоватьЕсли вы хотите понять, почему конкретный computed значение пересчитывается, используйте trace. Он покажет, какие зависимости инициировали пересчёт и поможет устранить лишние вызовы. Если необходимо узнать структуру объекта и его зависимости, используйте introspection. Это поможет вам увидеть взаимосвязи внутри реактивного состояния и, возможно, оптимизировать его структуру.Если нужно отследить все изменения в приложении в реальном времени или найти источник неожиданных изменений, используйте spy. Он полезен для отладки сложных систем и помогает идентифицировать потенциальные узкие места или побочные эффекты.Из опыта:    Переходя на новый проект, в период знакомства и погружения я, периодически, использую trace, если проект несильно замудреный и реже, скорее, прям очень редко, приходится юзать spy, так быстрее погружаешься в проект и понимаешь все зависимости и цепочки. Сейчас в моей разработке большой и сложный проект с кучей зависимостей в виде табличных данных, табличных фильтров, общих фильтров и сортировок. К сожалению, я не могу привести вам конкретный пример, потому что это коммерческая тайна. По своему опыту я не видел, чтобы разработчики пользовались этими инструментами. Но мне кажется, что иметь в арсенале своих скиллов эти инструменты определенно будет плюсом. Всем работающего кода, пока! ",
  "Семантическая сегментация: самый полный гайд 2024": "Что общего между автономными автомобилями, медицинскими диагностическими системами и спутниковыми снимками Земли?Ответ прост: все они зависят от способности машин «видеть» и понимать окружающий мир. Чтобы компьютер мог распознать объекты на изображении и отличить небо от дороги, человека от автомобиля или лес от здания, необходимо использовать технологии сегментации изображений. Но как именно машины учатся такому зрению? Тут мы должны вспомнить о семантической сегментации.Когда и зачем используется семантическая сегментацияЕсли объяснять в двух словах, то это одна из ключевых задач в области компьютерного зрения, она помогает машинам отличать разные классы объектов и фоновые регионы на изображении.Для семантической классификации мы выделяем каждый пиксель изображения, при этом каждый сегмент изображения соотносится с определенным классом. Например, на снимке с городским пейзажем модель выделяет здания, дороги, деревья и небо, причисляя каждый пиксель к своему классу. Это помогает машине \"видеть\" изображение так, как это делает человек, идентифицируя отдельные объекты и области. В результате машина распознает важные контексты на цифровых изображениях: пейзажи, фотографии людей, животных.В общем и целом, задачи сегментации изображений делятся на три основные группы:Семантическая сегментация: Этот тип сегментации классифицирует каждый пиксель изображения, но не различает разные экземпляры одного и того же объекта. Например, все автомобили на изображении будут отнесены к одной категории \"автомобиль\", без деления на отдельные экземпляры.Instance Segmentation: Она не только определяет класс объекта, но и различает разные экземпляры одного класса. Например, каждый автомобиль на изображении будет выделен как отдельный объект, что особенно важно для задач, связанных с подсчётом и идентификацией объектов.Паноптическая сегментация: Метод Panoptic Segmentation объединяет семантическую и Instance сегментацию, предоставляя полное понимание изображения. Паноптическая сегментация распознаёт как отдельные экземпляры объектов, так и не подающиеся подсчету области, такие как небо, земля или океан. Это позволяет более точно и комплексно анализировать сцены.Зачем нужна качественная разметка для семантической сегментацииПравильно размеченные изображения позволяют моделям эффективно различать объекты и фоны, а также корректно определять границы и классы объектов на изображении. В результате задачи по семантической сегментации улучшают способность систем компьютерного зрения правильно воспринимать визуальную информацию. Это особенно важно в системах автопилота или медицинской диагностике, где малейшая ошибка может привести к трагическим последствиям.Разметка для семантической сегментации помогает не просто классифицировать объекты, но и точно определить, где именно находится каждый объект, его границы и форма. Это даёт возможность машине понимать, где начинается и заканчивается каждый элемент сцены, что важно для более сложных задач анализа изображения.Как семантическая сегментация применяется в разных индустриях?Семантическая сегментация уже сейчас активно используется в различных индустриях:Автономные автомобили: Технологии автопилота используют семантическую сегментацию для распознавания дорог, пешеходов, транспортных средств. Это помогает автомобилю \"видеть\" дорожную обстановку и принимать верные решения в реальном времени, минимизируя риски аварий. Медицина: Семантическая сегментация используется для обработки медицинских изображений, таких как МРТ или КТ. Она помогает разделить изображение на различные сегменты, соответствующие тканям или органам, что улучшает диагностику и повышает точность медицинских заключений.В случае рентгеновских или других медицинских снимков, модели сегментации помогают выделить области интереса, такие как опухоли. Анализ медицинских изображений имеет большое значение, так как автоматизирует работу клиницистов. Например, Kvasir-SEG — это открытый набор данных с изображениями полипов ЖКТ и соответствующими масками сегментации. Модели, такие как FCB-SwinV2 Transformer для сегментации, разработанные Керром Фицджеральдом и Богданом Матушевским, могут способствовать раннему обнаружению полипов, которые потенциально могут перерасти в рак.Геоинформационные системы: Анализ спутниковых снимков с помощью семантической сегментации позволяет выделить участки земли, водоемы, лесные массивы, городские территории. Это помогает при создании карт и в мониторинге изменений в окружающей среде.Агропромышленный комплекс: Дроны с камерами используют семантическую сегментацию для анализа сельскохозяйственных полей. Это помогает фермеру отслеживать состояние посевов, выявлять участки с болезнями или вредителями, тем самым повышая эффективность управления полями.Поиск изображений по виртуальной базе: Как уже упоминалось, семантическая сегментация — это задача разделения изображения на сегменты, имеющие общие признаки. Это понимание позволяет разрабатывать алгоритмы поиска, которые находят изображения, схожие с исходным, и открывает возможности для создания систем визуального поиска, которые находят изображения на основе содержимого, а не только метаданных или текстовых описаний.Например, если необходимо найти изображения зданий с определенными архитектурными элементами, алгоритм сегментации выделяет и идентифицирует эти особенности на других изображениях.Сельское хозяйство: В сельском хозяйстве семантическая сегментация позволяет анализировать состояние посевов на основе изображений с дронов или спутников. Сегментация помогает выделять здоровые и пораженные участки растений, что позволяет фермерам быстро реагировать на угрозы, болезни или вредителей. А еще с помощью сегментации можно оценивать площадь посевов и прогнозировать урожайность, чтобы повысить производительность.Датасеты и семантическая сегментация: как правильно их использоватьСоздание точных моделей для сегментации изображений невозможно без качественных датасетов, в которых каждый пиксель представляет определенный объект или класс. Однако такая сегментация требует намного более детализированных и объемных наборов данных по сравнению с обычными задачами машинного обучения. Здесь важна не только правильная разметка, но и разнообразие данных, особенно когда от модели зависит безопасность, как в случае с автопилотами. Давайте рассмотрим популярные открытые датасеты, используемые для этих целей, и их значение для обучения точных алгоритмов.Существуют многочисленные открытые датасеты для таких задач, они охватывают разнообразные семантические классы и предоставляющие тысячи примеров с детальными аннотациями. Например, в задаче обучения автопилота распознаванию пешеходов, велосипедов и автомобилей критически важно, чтобы система чётко различала все объекты, иначе возможны аварии или ложные срабатывания. Точность и надёжность здесь имеют решающее значение.Вот несколько популярных открытых датасетов для сегментации изображений:Pascal Visual Object Classes (Pascal VOC): Включает различные классы объектов, ограничивающие рамки и карты сегментации.MS COCO: Содержит около 330 000 изображений с аннотациями для задач детекции, сегментации и создания описаний.Cityscapes: Ориентирован на городские условия с 5 000 изображениями, 20 000 аннотациями и 30 классами.Модели семантической сегментацииДля отличных результатов в семантической сегментации архитектура модели играет ключевую роль. Не каждая нейронная сеть способна эффективно выполнять эту задачу. Модели вроде Fully Convolutional Networks (FCN), U-Net и DeepLab были специально разработаны для того, чтобы обрабатывать каждую деталь изображения на уровне пикселей. Давайте посмотрим, как эти модели работают, что отличает их друг от друга и почему они стали ключевыми инструментами для компьютерного зрения.Fully Convolutional Networks (FCNs)FCN – это нейронная сеть, разработанная для семантической сегментации, которая использует сверточные слои для извлечения информации о каждом пикселе. В отличие от стандартных CNN, которые используют плоские слои для выдачи единичных меток, FCN заменяет их на свёрточные блоки, извлекая больше данных о изображении.Увеличение и уменьшение изображения: По мере накопления свёрточных слоёв размер изображения уменьшается, теряя пространственную и пиксельную информацию. В конце процесса происходит \"восстановление\" изображения до исходного размера через апсемплинг.Макс-пулинг: Этот метод выбирает наибольшие элементы в анализируемой области, сохраняя самые важные признаки для создания карты признаков.U-NetsАрхитектура U-Net, предложенная в 2015 году, улучшает результаты сегментации по сравнению с FCN. Она включает два компонента: энкодер, который понижает разрешение изображения, и декодер, который восстанавливает его через деконволюцию. U-Net часто используется в медицине для распознавания опухолей.Прямые связи: Это важное новшество в U-Net, позволяющее соединять выходные данные одного слоя с другим, не смежным. Оно уменьшает потерю данных при понижении разрешения, повышая точность финального результата.DeepLabМодель DeepLab, разработанная Google в 2015 году, использует атриумные свёртки для повышения точности сегментации. Вместо стандартного уменьшения разрешения DeepLab сохраняет больше данных, улучшая результаты через алгоритм CRF, что приводит к более точным маскам сегментации.Pyramid Scene Parsing Network (PSPNet)PSPNet, представленная в 2017 году, использует модуль пирамидального парсинга для сбора контекстной информации с большей точностью. Архитектура сочетает подход энкодера-декодера с пирамидальным пулингом, что позволяет анализировать более широкий объём данных и улучшать результаты.Все правила разметки для семантической сегментацииМоделям глубокого обучения обычно требуется огромное количество входных изображений для обучения. Процесс создания датасета включает сбор и разметку этих изображений. При этом в процесс разметки для семантической сегментации обычно входят несколько ключевых этапов:Этапы работы:Сбор исходных данных (изображений)На первом этапе необходимо собрать изображения, которые будут использоваться для обучения модели. Источники и данные могут варьироваться в зависимости от задачи: это могут быть фотографии городских улиц для автопилотов, медицинские снимки или спутниковые фотографии. Качество и разнообразие исходных изображений играют ключевую роль, так как именно они формируют основу для обучающей выборки.Предварительная обработка данныхПеред тем, как приступить к разметке, изображения часто требуют предварительной обработки. Это может включать изменение размера, нормализацию цветовых параметров, устранение шумов, артефактов и преобразование форматов. Это помогает улучшить качество данных и сделать их подходящими для разметки и машинного обучения.Разметка изображенийСамый важный этап — это разметка каждого пикселя изображения. Исполнитель вручную выделяет области, которые соответствуют определённым классам (например, человек, автомобиль, здание). Это кропотливый процесс, требующий внимания к деталям, так как от точности аннотаций зависит качество работы будущей модели.В некоторых случаях для упрощения разметки применяются полуавтоматические инструменты или алгоритмы, ускоряющие процесс аннотирования. Часто имеет смысл использовать уже существующую модель сегментации, чтобы автоматически разметить изображения, а затем вручную исправить ошибки и дополнить недостающие области.Кроме использования предсказаний моделей, можно применять алгоритмы обнаружения границ и другие методы сегментации для предварительной разметки изображения. Далее вручную заполняются пиксели с правильными классами, что помогает сократить объем работы и повысить точность аннотаций.Используемые инструменты и программное обеспечениеДля облегчения процесса разметки изображений существует множество специализированных инструментов и программных решений, которые помогают аннотировать изображения с высокой точностью.Популярные инструменты для разметкиLabelMe: Это бесплатный и широко используемый инструмент с открытым исходным кодом для разметки изображений. Он позволяет легко аннотировать изображения вручную, создавая полигоны, линии и другие формы для выделения объектов.VGG Image Annotator (VIA): Ещё одно бесплатное программное решение, которое поддерживает создание аннотаций на уровне пикселей. VIA удобен для быстрой разметки изображений и поддерживает множество форматов.Программные решения для автоматизацииСовременные технологии позволяют автоматизировать часть процесса разметки, что особенно полезно при работе с большими объёмами данных. Программные решения на основе искусственного интеллекта могут предварительно размечать изображения, предлагая аннотации, которые затем проверяются и уточняются вручную.SuperAnnotate: Эта платформа сочетает возможности автоматической разметки с ручной корректировкой, что значительно ускоряет процесс.CVAT (Computer Vision Annotation Tool): Популярный инструмент для разметки видео и изображений, который позволяет автоматизировать процессы на основе предобученных моделей, сокращая время аннотирования и повышая точность.Проблемы семантической сегментацииВ случае с семантической сегментацией ее потенциал напрямую зависит от качества разметки данных. От аннотирования быстро изменяющихся сцен до необходимости учитывать культурное и географическое разнообразие данных — разметка представляет собой настоящую головоломку. Давайте посмотрим распространенные проблемы и обсудим, как их обойти.Аннотирование динамичных сценРазметка изображений, особенно в случае семантической сегментации, — это не просто выделение объектов, а создание детализированных карт, где каждый пиксель имеет значение. Но что происходит, когда сцена динамична, а объекты постоянно двигаются?Аннотирование видеопотоков с перемещающимися автомобилями, пешеходами или изменяющимися условиями освещения становится настоящим вызовом и требует высокой концентрации и точности, поскольку малейшая ошибка в разметке может снизить качество модели. Кроме того, различные ракурсы и сложные взаимодействия объектов усложняют процесс аннотации, делая его трудоемким и подверженным человеческим ошибкам.Диверсификация данныхВ мире семантической сегментации данные — это основа всего, и чем больше разнообразия, тем лучше обучаются модели. Однако сбор и аннотирование изображений из самых разных культурных и географических контекстов — задача не из легких.Например, как разметить пейзажи городов, отличающиеся архитектурой, дорожными знаками, или растениями? Чем более разнообразны данные, тем сложнее обеспечить постоянство разметки, ведь каждый новый набор может представлять уникальные проблемы — от недостатка четкости изображений до разной степени видимости объектов. Это требует не только опыта, но и глубокого понимания культурных и этнографических особенностей.Будущее разметки семантической сегментацииАвтоматизация, искусственный интеллект, и самообучающиеся модели меняют привычные подходы к разметке данных, делая её быстрее и эффективнее. Но как далеко мы продвинулись? И что нас ждет в будущем? Поговорим о том, как новые технологии влияют на разметку и как они могут преобразить целые индустрии, от медицины до сельского хозяйства.АвтоматизацияРазметка изображений всегда была трудоемкой задачей, но с развитием технологий на помощь приходят модели, обученные на основе искусственного интеллекта. В будущем алгоритмы смогут самостоятельно разметить изображения с минимальной корректировкой со стороны человека.Но не нужно бояться, это не исключает человека из процесса полностью. Роль специалистов меняется — они становятся скорее кураторами и валидаторами, готовыми вмешаться, если алгоритмы допустят ошибку. Это сэкономит время и ресурсы, а процесс разметки станет еще более эффективным.От ручной работы к интеллектуальной автоматизацииБудущее семантической сегментации — это полная автоматизация, где человек будет играть значимую роль только на финальных этапах проверки. Новые подходы, такие как использование самообучающихся систем, смогут распознавать и классифицировать объекты с минимальной зависимостью от обучающих данных. Искусственный интеллект не только ускорит разметку, но и выведет на новый уровень все направление компьютерного зрения с более умными и интуитивными моделями, которые смогут адаптироваться даже к самым сложным условиям.Эти технологии не просто помогают бизнесу адаптироваться к будущему, они делают это будущее реальностью. Хотите узнать, как меняются правила игры, прочитать больше про разметку и ее использование для бизнеса? Обратите внимание и на другие наши статьи!",
  "Солнечная энергия: собираем в космосе, отправляем на Землю. Как это работает?": "Привет, жители Хабра! Это Виктор Сергеев из команды спецпроектов МТС Диджитал. Идея получения солнечной энергии прямиком из космоса появилась примерно в середине XX века, возможно, даже раньше. Ее озвучил, например, Айзек Азимов в НФ-рассказе «Логика» в 42-м году. Что касается практики, а не фантастики, то один из ранних проектов NASA из 60-х предполагал размещение больших солнечных панелей на орбите Земли с целью бесперебойной подачи энергии на Землю. Но отсутствие необходимых технологий и огромные затраты, требующиеся для их поиска, привели к заморозке проекта.Сейчас компания Aetherflux, созданная Байджу Бхаттом (сооснователь платформы Robinhood), намерена превратить эту мечту в реальность. Миссия Aetherflux — использовать спутники на низкой околоземной орбите для сбора солнечной энергии и передачи ее на Землю через инфракрасный лазер. Идея Aetherflux: не мытьем, так катаньемОсновная идея космической энергетики проста: в космосе солнечные панели могут работать постоянно, без перерывов на ночь или помех погодных условий. Теоретически Земля может быть обеспечена чистой энергией круглосуточно при помощи геостационарных спутников или аппаратов на низкой орбите.На практике реализация проекта для покрытия хотя бы 30% энергетических потребностей Европы обошлась бы в миллиарды евро и потребовала масштабных инфраструктурных решений. Завершить строительство в лучшем случае можно к середине XXI века. А если будут задержки и переносы, то впору вспоминать анекдот про ишака, которого учили богословию, и султана.Aetherflux предлагает новый подход — вместо развертывания массивной сети в геостационарной орбите на расстоянии 36 000 км компания начнет с единственного спутника на высоте около 500 км. Он будет построен на базе коммерческой платформы Apex. Она разрабатывается одноименной американской компанией для быстрой сборки спутников различного размера и назначения. Они могут быть настроены под конкретные задачи и используются в основном для проектов на низкой (LEO) и геостационарной орбитах (GEO). Продуктовая линейка включает несколько моделей, среди которых Aries и Nova, подходящие для малых и средних полезных нагрузок (весом от 100 до 500 кг), и GEO Aries, предназначенная для геостационарных миссий. Спутник планируется запустить через 12–15 месяцев с помощью SpaceX. У него будут солнечные панели, способными генерировать около 1 киловатта энергии. При помощи инфракрасного лазера Aetherflux передаст собранную энергию на Землю, где она будет приниматься мобильной наземной станцией диаметром около 10 м.«Представить себе проект размером с небольшой город на геостационарной орбите — за гранью моего воображения. Я думаю, что масштабы — одна из причин, почему идея космической солнечной энергии долгое время оставалась невостребованной. Наш подход совсем иной», — отмечает глава Aetherflux. Тестовая миссия позволит проверить основные возможности технологии: безопасность и эффективность сбора солнечной энергии, ее передачу через атмосферу и получение уже на Земле.«Наша цель — превратить концепцию космической энергетики из научной фантастики в реальность, начав с демонстрационной миссии, чтобы доказать ее безопасность и эффективность для удовлетворения глобальных нужд в энергии», — говорит Бхатт. В случае успешного выполнения в будущем будут созданы сети спутников, обеспечивающих энергией отдаленные районы, где нет возможности получать ее иным способом в нужных объемах. Вот основные преимущества проекта: Постоянный источник энергии. Об этом уже говорилось выше. В отличие от земных солнечных электростанций, которые зависят от смены дня и ночи, а также погодных условий, космические спутники могут собирать и передавать энергию круглосуточно. «Чистая» энергия. Она потенциально может заменить дизельные генераторы или такие традиционные источники энергии, как угольные электростанции. Это привело бы к значительному сокращению выбросов CO₂. Компактные приемные станции. В отличие от традиционных подходов, которые предусматривают установку крупных солнечных панелей на геостационарной орбите и передачу энергии с помощью микроволн, Aetherflux предлагает развернуть сеть небольших спутников с инфракрасными лазерами. Этот метод позволяет передавать энергию компактным наземным приемникам, что делает систему более доступной и гибкой в масштабировании.Не все так гладкоРеализация проекта сопряжена с серьезными вызовами: Высокие начальные затраты. Космическая солнечная энергетика требует значительных инвестиций в разработку, запуск и обслуживание спутников. Поддержка их работы на орбите также потребует регулярных вложений. Космический «извоз», т. е. ракеты-носители, вещь недешевая. И здесь нужны деньги. Финансов на развертывание масштабной структуры нужно очень много. Технические сложности. Передача энергии через атмосферу — сложный процесс, связанный с потерями по пути. Не на последнем месте и вопросы обеспечения безопасности: лучи не должны представлять угрозу для людей, животных или авиации. Обслуживание спутников. Разработка спутников, способных стабильно работать на орбите, требует применения передовых технологий, в том числе робототехники, для сборки и обслуживания конструкций в космосе. Проекты подобного масштаба требуют сотен запусков, что может создать дополнительные проблемы — как технические, так и экологические.Угрозы безопасности. Всегда существует риск хакерских атак на спутники. С этим тоже надо что-то делать, поскольку перехват управления энергетическим спутником не игрушка. Когда мечты станут реальностью?Космическая солнечная энергетика рассматривается учеными, правительствами и компаниями как перспективное направление. Доступ человечества к постоянному «чистому» источнику энергии решит множество проблем на Земле.Компании вроде Aetherflux предлагают необычные идеи и пути их реализации. В будущем, при успешной реализации первых проектов, возможно, у нас появится еще один источник энергии. Как сейчас возник дополнительный канал связи — тоже спутниковый, от Starlink. Несмотря на высокую стоимость, сложности с разработкой технологий и вопросы безопасности, у космической энергетики есть потенциал стать частью глобальной энергетической инфраструктуры. Прорывы в области робототехники, материаловедения и снижения стоимости запусков приближают этот момент. Вероятно, в ближайшие 3–5 лет мы увидим и другие проекты, помимо пилота от Aetherflux. ",
  "Тестовое с Chat GPT и собеседование с наушником: работают ли чит-коды при трудоустройстве в IT-компанию": "Хитрость помогает добиться успеха так же, как и положительные навыки: трудолюбие, усердие или любознательность. Но использовать ее нужно с умом.Меня зовут Сергей Романов, я работаю в KODE тимлидом бэкенд-разработчиков. Недавно проверял тестовые задания кандидатов на менторскую стажировку и увидел в коде нестандартный для Go архитектурный паттерн. В нем не было ошибки, но обычно пишут не так. Он встретился мне еще в шести заданиях. На втором задании я подумал, что у меня дежавю. На третьем — что ребята списали из одного источника. На четвертом обнаружил пометку «GPT-4», которую автор забыл удалить, и все встало на свои места. Тогда я рассказал об этом коллегам, узнал, что, помимо тестовых, кандидаты пользуются ИИ во время собеседований, и решил написать статью. Объясню, почему считаю читерство при трудоустройстве не лайфхаком, а выстрелом себе в колено.Источник: к/ф «‎Гарри Поттер и Орден Феникса», режиссер Дэвид ЙейтсЧто выдает читера на собеседованииВнезапное озарениеКандидат не может решить задачу. Ему предлагают спокойно разобраться с ней попозже. Он соглашается и присылает ответ через пару минут после окончания собеседования. Это подозрительно, ведь только что в течение некоторого времени у него не получалось найти решение. Мы отправляем условие в Chat GPT и получаем дословно такой же текст.Chat GPT вообще ненадежный подельник, и при первой же возможности сдает с потрохамиРезкая уверенностьПеред ответом на вопрос происходит заминка на 5-10 секунд: кандидат чешет в затылке, хмыкает и как бы раздумывает, а затем быстро и уверенно отвечает. Были такие кейсы и во время лайвкодинга — курсор долго находится вне фокуса, то есть кандидата нет на страничке, а затем он возвращается и пишет правильное решение.Помехи видеосвязиСобеседование проводят двое: рекрутер и инженер по тестированию. Рекрутера кандидат слышит хорошо и отвечает ему быстро. Но после вопросов инженера по тестированию он жалуется на связь и по несколько раз переспрашивает незначительные детали. При этом рекрутер инженера отлично слышит. На некоторые вопросы кандидат говорит: «Я знаю, как это работает, но не могу объяснить своими словами, лучше скажу вам определение».А у вас стена белаяКандидат сидит в темном помещении на фоне белой стены. Когда ему задают вопрос, он делал вид, что думает, но в это время отсвет за его спиной меняется из-за переключения вкладок.МикронаушникОднажды рекрутер думал, что рядом с кандидатом сидит помощник и подсказывает ему — настолько громким был звук в микронаушнике.Как спалиться в тестовомИспользование ИИ в тестовых заданиях легко заметить:По идентичному стилю, формулировкам и ошибкам у разных людей.По разному стилю текста. Например, в абзацах, где кандидат рассказывает о себе в свободной форме, он изъясняется менее формально и сдержанно, чем в ответах на задания. Кажется, что первое писал человек, а второе робот.По низкому качеству перевода на русский язык. Например, «конечная точка API» вместо «API endpoint».По неверному решению задания в случае, когда правильный ответ очевиднее неправильного. Или по совсем неправильному ответу: например, нужен был код для сервиса, который должен работать асинхронно, а кандидат прислал синхронный — потому что асинхронный ИИ не потянул.По излишним комментариям в коде. В нашем простом тестовом с прозрачным решением нет сложной бизнес-логики, которую надо объяснять. А Chat GPT постоянно комментирует код сухим техническим языком, чтобы объяснить человеку, что он сгенерировал.Пример кода с комментариямиПочему читерство — не выходДумаю, вы согласитесь со мной в том, что каждый из сценариев выше выглядит заранее обреченным на провал. Конечно, можно сказать «Ты привел выборку только из тех, кто попался, а как насчет непойманных?». Ну так мы их и не поймали — значит, они были умнее или повезло.Рекрутеры часто проводят собеседования, ведущие специалисты проверяют тестовые задания одно за другим. Поэтому им нетрудно заметить подозрительные действия или решения. А когда обман раскрывается, у сотрудников компании нет возможности закрыть на него глаза и продолжить общение, как ни в чем ни бывало, и в 100% случаев кандидату отказывают.Понимаю, что глобально в IT есть проблема переусложненных технических собеседований, на которых кандидату предлагают, например, балансировать красно-чёрное дерево из толстенного тома Кормена. При том, что это совершенно не пригодится ему в работе и, вообще, о существовании деревьев он слышал только в вузе. Возможно, такое грех не списать. Понимаю, что помимо переусложненных собеседований есть и переусложненные тестовые задания. Или ситуации, в которых из 100 тестовых неплохого качества выбирают только два, и смотрят не на условия, указанные в задании изначально, а на работу «сверх».Понимаю, что на рынке периодически мелькают истории вроде «Я прошел собеседование в Google на высокую должность, полгода работал два часа в неделю и никто ничего не заметил». Из-за них возникает ощущение, что прорваться в IT можно любой ценой.Но мы в KODE — и это подход не только нашей IT-компании — проводим отбор на вакансию или стажировку для того, чтобы оценить уровень знаний кандидата и возможности его мышления. Мы занимаемся коммерческой разработкой и ищем специалистов под конкретные проекты. На собеседованиях и в тестовых задаем условия, максимально приближенные к реальности, проверяем глубину понимания инструментов, с которыми надо будет работать. Мы не ждем от кандидата умения работы с ИИ, потому что у нас нет процессов, в которых мы бы генерировали код через ИИ. У наших задач сложная бизнес-логика: пару раз мы пробовали сгенерировать код через ИИ, но затраты на приведение его в нормальный вид были сопоставимы с самостоятельным написанием. Возможно, в будущем ИИ сможет выдавать качественные решения, но пока это не так, мы его не трогаем и упрощаем себе жизнь только с помощью детерминированной кодогенерации.Поэтому мы не можем позволить себе нанять человека, который привык сильно полагаться на ИИ. Даже если он каким-то чудом пройдет техническое интервью, он не сможет выполнить повседневные задачи. Или будет делать их слишком долго, потому что у него ухудшился навык самостоятельного написания кода.Как использовать ИИ во благоОднажды кандидат сдал отличное тестовое задание, но документация в нем была слишком подробной и хорошо отформатированной. Я прямо спросил, и он честно ответил, что сгенерировал ее в Chat GPT. Это не составило проблемы, потому что он автоматизировал рутинную задачу, а в остальном все понимал.Пример не слишком качественно сгенерированной документацииНаши фронтендеры прибегают к помощи ИИ в рутинных задачах: например, генерируют моковые данные по схеме. Или тесты — потому, что ИИ часто придумывает неочевидные кейсы, до которых сложно догадаться.Как же быть в итогеКороткий ответ: либо списывать так, чтобы не поймали, либо не списывать вовсе. Я склоняюсь ко второму. На мой взгляд, у использования ИИ в тестовых заданиях три причины:Желание сэкономить время.Желание спрятать пробелы в знаниях.Неуверенность в себе и ощущение, что ИИ знает лучше.В первом случае лучше не торопиться. Обычно мы даем на тестовое достаточно времени — около недели, но понимаем, что большинство кандидатов ищет новую работу, не увольняясь с предыдущей, или выполняет несколько тестовых одновременно. При форс-мажорных обстоятельствах мы можем пойти навстречу и подвинуть дедлайн.Во втором случае считаю, что все тайное становится явным. Вместо того, чтобы идти на риск, лучше пополнить знания с помощью глубокого ресерча: почитать статьи или книги, посмотреть видео, покопаться в сообществах. Причем относительно глубокого — по опыту, в некоторых заданиях кажется, что кандидат мог бы найти верное решение после получасового ресерча в Google. Но вместо этого он прочел краткую выжимку из  Chat GPT по диагонали и не добрался до истины. Если на ресерч совсем нет времени, размышляйте самостоятельно и комментируйте ход мыслей. Например, «Не уверен, что здесь пригодится именно этот метод, но раз он может сделать то, то и то, он подходит с вероятностью 90%».В третьем случае уверенно заявляю: пока что ИИ не умнее человека, и польза от сэкономленного времени не компенсирует потенциальный вред. Он может упростить жизнь специалиста, который хорошо ориентируется в своей области. Но для менее квалифицированного кандидата ИИ — ненадежный рассказчик, на которого нельзя полагаться. Он может подкинуть неверное решение под видом правильного, не объяснить, в чем подвох, и просто подставить вас перед потенциальным работодателем.Поэтому лучше либо честно признаться в незнании — оно может быть не так и критично, либо выдать не идеальное, но самостоятельное решение.А вы что думаете?",
  "Учимся (не) проходить финалы на стажировку в Яндекс": "Лирическое отступлениеМне 32 года из которых около трех лет активно изучаю python. Я являюсь тем самым \"в IT`шником\", который решил что у него достаточно сил чтобы побороться со студентами за место на стажировку в Яндексе. Подготовка к Яндексу заняла более полу года. К тому времени у меня завершенная стажировка в стартапе, пара проектов на заказ, несколько прочитанных книг и несколько учебных проектов.Получив фидбек об успешном прохождении второй алгоритмической секции я точно не ожидал, что впереди 8 месяцев отказов и практически отсутствие интереса ко мне среди команд.Дисклеймер: вся информация является моим личным мнением, отнеситесь к ней соответственно.Общая информация о финалахСобеседования с командами проходят по следующей схеме:Ты рассказываешь о себе, своем опытеИнтервьюер задает тебе интересующие его вопросы: по твоему опыту, техническим знаниям, могут дать задачи на подумать либо покодить, твоей готовности к работе, мотивации и тдПредставитель команды рассказывает о своем проекте и иногда условиям прохождения стажировки у нихТы задаешь интересующие тебя вопросы интервьюеру Чем больше у тебя отказов и хуже отзывы в начале, тем меньше команд хотят приглашать тебя на собеседование, даже если твои фидбеки в итоге становятся хорошими. Получив первые пять отказов я узнал что можно было отказываться от собеседований с командами, которые мне были не интересны. Резюмирую: не интересна команда - отказываемся от собеседования с ней. Экономим своё время, время команд, не собираем лишние отказы и лучше готовимся к собеседованиямСобеседованияВсем новоприбывшим обычно предлагают сразу три команды. Я еще не знал что можно отказываться от собеседований, поэтому соглашался на всё. Здесь я допустил ошибку - поставил три собеседования один за другим, а когда получил сразу три фидбека то был неприятно удивлен результатами. Резюмирую: ставьте собеседования с интервалом минимум 2 дня, чтобы вы могли получить фидбек и проработать ошибки из него до следующего собеседования.Первая командаЯ полный уверенности начинаю рассказ \"о себе\", но оказалось что информации не достаточно - я не помнил нюансов проекта прошедшей стажировки. Это вылилось в то, что начали задавать вопросы по проекту о котором я мало что помнил, а некоторые нюансы стажировки и вовсе было стыдно озвучивать. В итоге я пытался выкручиваться давая размытые ответы, а мой потенциальный ментор пытался выяснить что же было на самом деле. Получился своеобразный допрос, в котором я явно выводил из себя интервьюера. Далее спрашивали иерархию исключений, но так как в точности её не помнил, то рассказал в общих чертах. Интересовались как я вижу своё развитие, на что ответил: \"мне интересен путь в сторону архитектуры, нежели управления людьми\". От себя добавил, что не хотел бы работать с \"высшей математикой\" (подразумевал ML, DS и подобные), потому что у меня нет \"глубокой математической базы\". Я не против ковыряться в алгоритмах, но если мне придется выполнять половину работы (либо всю) как ML - из этого ничего хорошего не выйдет.Фидбек\"Кандидат достаточно спорный. С одной стороны, очень уверен в себе и знает, чего хочет добиться. С другой - при своём небольшом опыте самоуверенность стреляет ему в ногу. Софты: были какие-то странные ответы на вопросы, уходящие совсем в сторону. Послушал рассказ про задачи для стажёров, вопросы по ним задавал корректные, но ощущения достигнутого понимания не было - кандидат просто решил забить, кажется. Техника: не успели долго поболтать, но что-то рассказывал про предыдущую стажировку (django и fastAPI) и телеграмного бота, а на вопросы по ходу дела отвечал сносно. При этом я бы посоветовал следующему интервьюеру тут углубиться - есть подозрение, что я просто не копал глубоко. Мотивация: хочет \"архитектурить\", но вкладывает в это понятие явно не то, что я. Почему-то очень настороженно относится к задачам, где нужно чуть поломать голову (алгоритмы, математика) - сразу уходит в защиту \"я не специалист, я не могу\". Хочет попасть в большую компанию в России, т.к. в Беларуси, судя по рассказу, сложно найти айтишную работу и \"кругом одни джависты, а я - питонист\", а за рубежом язык знать надо.\"ВыводыБыть прощеЛучше подготовить рассказ \"о себе\" и готовность ответить на все вопросы по прошлому опытуНе говорить то, чего у тебя не спрашивают, в том числе что тебе не нравится / с чем не хотел бы работатьПовторить иерархию исключенийВторая командаНачало стандартное - представление \"о себе\". После этого я сказал что мне не интересно направление команд (я еще не знал что можно отказывать до собеседования), при этом я не представлял чем вообще занимается команда даже из описания. Далее своеобразный блиц опрос по многопоточности / многопроцессорности, как это работает; в чем отличие asyncio и многопоточности на глубоком уровне; как работает / устроен GIL в python и для чего он нужен. Спросили теорию графов, где используются - я ответил что-то про социальные сети и какие-то примеры из того что мог вспомнить, но хотели услышать что используются в импортах модулей и тп.Фидбек\"В общении проблем не было, кандидат рассказал про свой опыт (писал tg бота (FastAPI) и бекенд на Django), поспрашивали про некотрые тонкости Python(тут в общем ок), параллельное и асинхронное программирование, БД, в ответах плавал, теоритеческой базы тут не хватает, есть практика, но почему именно так работает кандидат не знает. Год назад пробовал читать книги, но понял, что без практики толку мало, потому ушел в практику, а чтение книг забросил. Из наших комманд предпочтений не было и вообще сначала сказал, что ему не интересна инфраструктура и хочет продуктовой разработки, писать код, а вот деплой/алерты/сети ему не интересны. От стажировки ожидает, что попадет в штат и готов вложиться в нее по полной.\"ВыводыЛучше изучить асинхронность и параллельность \"под капотом\"Если о чем-то не спрашивают, не стоит это говоритьТретья командаКак обычно начинаю с рассказа \"о себе\". После этого дали задачу на реализацию переноса большого количества данных из одной базы в другую. Я рассказал что можно использовать генераторы (пришло в голову т.к. это экономит память), пытался вспомнить про OFFSET в SQL, но забыл как называется функция и объяснил это своими словами. Данный вариант не особо устроил интервьюера и он задал уточняющий вопрос - \"как я буду делать выборку данных из БД для прошлой задачи?\". Тут я предложу вам подумать, как будете готовы читайте ответ.ОтветПо UUID невозможно делать сортировку, поэтому необходимо отсортировать данные по времени создания и уже из получившегося результата пачками через LIMIT и OFFSET их забирать.Как я понял потом, мне необходимо было знать что первичные ключи хранятся в виде UUID, я же предлагал решение в виде численных первичных ключей. Так же я должен был знать что данные в базе перезаписываются.Вопросов по чистому SQL и работал ли я с ним не было.Фидбек\"Спорный кандидат. Самоучка. В прошлом опыт был в виде телеграмм бота и стажировки в стартапе. С базами работал только через orm. По ощущениям, есть знания языка и алгоритмов, остальное выглядит поверхностно. От стажировки ждет, что попадет в штат. Хочет в Москву\"ВыводыПочитать про UUIDПовторить SQLПолучив сразу три фидбека я провел работу над ошибками в рассказе \"о себе\", а так же подтянул остальную теорию.Четвертую команду мне искали около двух недель после получения фидбеков.Четвертая командаПосле краткого рассказа о своём опыте я спросил - \"рассказывать, чем я занимался на проекте стажировки более подробно?\", на что мне ответили - \"не надо\". Дальше был вопрос \"что было самым сложным на стажировке\", на что ответил - \"в начале стажировки пришлось изменять структуру проекта, попутно разбираясь в нем\". Были вопросы по SQL, точнее попросили в онлайн‑редакторе написать пару запросов на создание, объединение таблиц и выборку из них. Тут я неправильно выбрал столбец для подсчета и путал названия JOIN`ов. Интервьюер спрашивал об удаленке, так как у них есть данный вариант, но я сказал что рассматриваю только офис. В конце я задал вопросы про дежурства в данной команде и то, как в проекте взаимодействует бэкенд с фронтендом.Фидбек\"Из интересного:в чате на fastapi хотел отрефакторить код. недостатки: sql-запросы прямо в ручкахбыло 2 репы с чатом и календарем Простой опросник по SQL inner join называет cross join В целом норм, не знает некоторые конструкции типа FILTER в COUNT Пообщались по проект. К задавал вопросы про удаленку, дежурства. Кажется, К пытался срезать углы (например, спрашивал, что бэк только предоставляет API, или может еще генерирует html-страницы) К себе брать не готов. Интересного из К ничего вытащить не смог. На текущих проектах часто занимался непонятным рефакторингом. Нет понимания, как будет работать в команде и как общаться с заказчиками\"ВыводыПосмотреть конструкции FILTER в COUNT и перестать путать названия JOIN`овДоработать рассказ \"о себе\", чтобы в кратком изложении можно было донести максимум небходимой информациюНе задавать вопросов по условиям работы в командеПятую команду мне искали неделю после получения фидбека от четвертой команды. Пятая команда\"О себе\" с последующим переходом к вопросам о том, что такое хэшсет, как работает / устроен, писал ли я тесты и работал ли с asyncio. Далее вопрос о том, чего я хочу от стажировки, на что ответил - просто хочу работать (сказывалось состояние усталости от постоянных отказов). На уточняющие вопросы по данной теме отвечал в таком же стиле. Вопросов по БД и SQL не было.Фидбек\"На заказ разработал бота на python-telegram-bot для публикации сообщений в каналы, после стажировка в стартапе, проект - фриланс биржа занимался доработкой rest ручек монолита и чата в команде из 3 человек, синхронное апи чата - fast api, монолит django-rest, сообщения - через веб-сокеты - не изменял. есть базовое знание python асинхронное программирование в python - только теория не сталкивался с автоматическими тестами базы - знает select/join/group by, но в ответах не уверен. стажировку хочет проходит очно, с переездом в мск. на стажировке - работать работу, под присмотром ментора, с обратной связью для быстрого обучения. для меня - пограничный кандидат, харды для стажера ок, в целом в общении особых проблем нет, но стиль общения - размытые и краткие ответы на вопросы по предыдущим проектам, интересам, скилам, надо постоянно вести и вытаскивать вопросами информацию отталкивает. Пока к себе брать не готов\"ВыводыИзучить asyncio и pytest минимум на базовом уровнеДавать развернутые ответы на вопросыОтвечать подготовленным ответами на вопросы \"что хочешь от стажировки, почему мы и тд\"Академический отпускПосле получения пятого отказа мне сказали что желающих команд для меня сейчас нет и мне стоит лучше подготовиться, отправив на полу годовое самостоятельное \"повышение квалификации\". Я еще раз просмотрел все проблемы прошлых собеседований, сделал работу над ошибками и доработал рассказ \"о себе\". Так же на базовом уровне изучил и поработал с asyncio и pytest.Спустя 2.5 месяца ко мне возвращается рекрутер и предлагает продолжить поиск команд, на что я соглашаюсь. В течении следущих двух месяцев мне предлагают четыре команды, с одной из которых я соглашаюсь на собеседоване.Немного про отказы командам. Я не ищу идеальную команду, но если мне не хватит недели подготовки чтобы соответствовать основным требованиям команды - я отказываю. Мне например сложно за неделю изучить с нуля C++ и ML на базовом уровне.Шестая командаВ начале собеседования интервьюер говорит - \"это будет легкое для меня собеседование\". Дальше уже отточенный рассказ \"о себе\". Спросили про asyncio: как устроен, зачем нужен; когда стоит использовать asyncio, а когда многопоточность; про брокеры сообщений; типы БД и где какие использовать; про шардирование и репликацию (тут в голове всё перемешалось и временами путал термины). Спрашивали про многопроцессорность - \"как можно работать с CPU задачами\", на что ответил - \"можно распараллеливать по процессам, использовать технологии вроде map reduce и брокеры сообщений\". Возможно я не правильно понял вопрос. Из нюансов хотел отметить - я больше одного раза говорил что на прошлой стажировке была сложная задача, где приходилось самому разбираться и помочь с ней мне никто не мог (мол мне может понадобиться помощь). Так же после того как ответил что с брокерами не работал, интервьюер сказал - \"обычно разработчики изучают смежные технологии\" и продолжил свой рассказ, а я не успел отработать \"возражение\". В конце я спросил какой код предпочтителен в команде, \"чистый\", либо \"быстрый\".Фидбек не пришел в первые два дня, поэтому я понял что ситуация нестандартная. Я попросил рекрутера поставить поиск команд на паузу пока не будет ответа от данной команды. Спустя 9 дней я получил фидбек и отказ.ФидбекОбщее: • есть опыт коммерческой разработки (5 месяцев, в основном fastpai и django) • на одном из проектов писал телеграмм бота (бот-админ для публикации постов в каналы) • представляет как работает asyncio и есть опыт, про многопоточность и GIL в питоне тоже понимает, а вот с многопроцессорностью что-то не очень • слышал про брокеры сообщений, но без опыта (и прям совсем только слышал) • имеет хорошее представление, о том какие бд бывают и в каких задачах могут быть полезны (kv/document oriented/графовые/колоночные) • немного абстрактно пообщались про шардирование и репликацию, конечно не очень глубокие знания во всем этом, но где-то что-то слышал и общее представление местами имеетсяВпечатление: Кандидат достаточно хороший, есть опыт в бэке. Но у меня сложилось впечатление, что слишком много уделяет времени \"красоте кода\" (я был бы и рад такому Кандидату, но у нас тяжело будет с таким подходом). Не сошлись по софтам скорее, есть ощущение, что тяжело погружается в новые области. К себе взять не готов, но другим командам советую посмотретьВыводыПочитать про шардирование и репликациюОзнакомиться с брокерами сообщенийНе задавать вопросов, которые касаются условий работы в команде (в данном случае про чистоту кода)Не давить на жалось, повторяя что тебе \"надоело беспросветно тащить всё на себе без какой-либо помощи\". Достаточно спросить кто будет твоим ментором и как часто вы будете с ним созваниватьсяДаже если ты попал в \"шорт лист\" команды, не стоит прекращать поиск / собеседования с другими командами. Принимать решение стоит тогда, когда у тебя есть фактический офер от команд(ы)Через две недели мне находят еще одну команду, от которой я отказываюсь. Скоро будет месяц как я нахожусь в поиске очередной командыИтогЯ пока не собираюсь сдаваться, хотя силы уже не те что в начале.Общие выводыСоглашайтесь на собеседование только с интересующей вас командой. Перед тем как дать свой ответ - поищите в интернете информацию о ней, если это необходимо. Есть сомнения - один раз можете согласиться на такую команду ради опыта, но больше не советую. Не тратьте своё и время команды зря, ну и не забывайте про отказыСтавьте собеседования с командами с интервалом минимум два дня, чтобы вы могли получить фидбек от прошлой команды и проработать ошибки до следующего собеседованияНе стоит останавливать поиск других команд если вы попали в \"шорт лист\" какой-либо команды (отсутствие фидбека в течении 3 дней после собеседования). Принимайте решение только после получения офера от команд(ы)На собеседовании будьте \"проще\", общайтесь дружелюбно на \"ты\" (обычно интервьюеры делают это первыми в начале собеседования)Не стоит шутить на собеседовании - у всех разное чувство юмораПодготовьте краткий рассказ о себе, в котором будет максимум информации о вас и ваших проектах, которая может быть интересна интервьюеру. Будьте готовы ответить на все вопросы по вашим проектам, как будто вы это техническая документацияВам необходимо знать на хорошем уровне всё, что находится в road map вашей специальности до уровня middle+, а всё что выше - по одной статье. Никогда не знаешь что могут спросить (я хочу чтобы это была просто шутка)Никогда не говорите что чего-то не знаете. Отвечаем на это развернуто по схеме: где и в каком объеме встречался с данной темой, почему не может ответить на этот вопрос сейчас. Пример: \"читал когда-то об этом на заборе, но не знал что это может мне пригодиться\"Отвечаем на вопросы развернутоНе говорим того, чего не спрашивают - это потенциально может вас дискредитировать. Примеры: \"я люблю кофе\" - \"будет целыми днями пить кофе и ничего не делать\"; \"я общительный и коммуникабельный\" - \"мало того что днями будет пить кофе и ничего не делать, так еще и других будет постоянно отвлекать своими разговорами\"Интересуемся о проекте в общих чертах (мы же соглашаемся только на интересные нам команды). Вы можете поискать информацию о данном проекте (либо схожих) в интернете и задать вопросы из технических требований. Примеры: какую нагрузку выдерживает, какой аптайм, какая скорость и тдНе задавайте вопросы про условия работы в команде на собеседовании. Стандартные условия стажировки - фултайм офис с задачами, во время которых теряешь счет времени и регулярные дежурства. Если у вас специфичные требования - просим рекрутера связаться с командой и узнать ответ на эти вопросы у команды. Команда может предоставить такие условия - соглашаемся на собеседование. Не тратим своё время и время команды зряЗаранее готовим ответы на вопросы: почему именно эта команда, что хотите от стажировки, чего хотите добиться, какие были сложности / неудачи в прошлых проектах и тд. Про сложности и проблемы отвечаем в духе \"было сложно, но разобрался, сделал такие-то выводы и стало проще / таких ошибок больше не допускаю\"Расскажите о том что изучаете либо изучали в последнее время. Если это будет \"созвучно\" с требованиями команды - супер. Если ничего не изучаете - пора начинать",
  "Что такое podman kube и как он может быть полезен для работы в rootless режиме": "При развертывании пары десятков контейнеров в Podman в rootless-режиме мы внезапно обнаружили, что они не могут одновременно использовать одни и те же порты, как это обычно работает при использовании Docker.Решений несколько, но в рамках наложенных ограничений стандартные варианты не подходили. В итоге мы обратились к функционалу Podman Kube, который помог нам решить эту проблему. Подробнее о том, как это работает, и чем может быть полезно, ниже..Как простая задачка с контейнерами внезапно стала непростой  У нас много заказчиков, которым мы оказываем услуги DevOps. В какой-то момент для одной поддерживаемой нами облачной платформы потребовалось внедрить стороннее приложение – продвинутый чат-бот. Приложение разрабатывалось внешней организацией и поставлялось «как есть» без доступа к исходному коду. Архитектурно оно представляло собой порядка двух десятков Docker-контейнеров, разворачивающихся в Docker с помощью compose-файла. Именно в таком виде код для развертывания и сам дистрибутив передаются для установки. Из базовых наработок по автоматизации, предоставляемых вендором –  только bash-скрипты для загрузки образов и docker-compose файл. В качестве ограничений – утвержденный заказчиком стек инструментов, где для контейнерной оркестрации следовало использовать Podman или Kubernetes.Kubernetes мы откинули сразу, поскольку потребовалось бы очень много времени на адаптацию приложения, а его не было ни у нас, ни у заказчика, ни у вендора. Поэтому выбор сократился до безальтернативного Podman.Казалось бы, в Podman мы точно так же, как и в Docker, можем поставить podman-compose, а при желании даже docker-compose, который будет работать с Podman и запускать все в исходном виде – в том, как это предоставляется вендором. Но нам мешало дополнительное строгое требование заказчика – использовать Podman только в rootless-режиме. То есть, для работы инженеру предоставляется заранее подготовленная виртуальная машина с Podman, и работать с ним можно только под непривилегированным пользователем.Далее мы столкнулись с рядом  проблем.Первая – в Podman иная реализация разрешения имен контейнеров. Суть в том, что при создании виртуальной сети для контейнеров DNS-резолвером выступает шлюз этой сети. Если создать другую сеть, то будет другой шлюз и, соответственно, другой адрес резолвера.На что это влияет? Например, если у вас в конфигурации nginx присутствуют бэкенды, резолв имен которых опирается на имена контейнеров, то в директиве resolver следует менять адрес на шлюз вашей Podman-сети.Например, ниже конфигурация nginx:  server {\n  server_name _;\n  listen 80 default_server;\n  # optional SSL\n  include ssl*.conf;\n …\n  # global platform timeouts\n…\n  resolver \u003cшлюз podman сети\u003e;\n  # variables for the the proxy_pass forcing NGINX to re‑resolve the domain name when its TTL expires\n  set $front http://front.dns.podman:80;\n ...\n ...\n}\nВ директиве resolver требуется указать адрес gateway, который мы получаем из команды podman network inspect \u003cnetwork\u003e.Для docker это обычно 127.0.0.11).Также при указании фронтов рекомендуется добавлять домен dns.podman, чтобы избежать проблем с разрешением имен при обращении друг к другу через nginx.  Важно помнить, что в случае, когда используется CNI для настройки сетевого окружения контейнеров, необходимо убедиться, что установлены все требуемые плагины. Например, плагин dnsname отвечает за работу DNS в виртуальной podman-сети. При этом зачастую из коробки этот плагин не подключен к default сети, и требуется создать новую podman сеть, чтобы в ней заработал dnsname.Далее  начались проблемы, связанные непосредственно с rootless-режимом. Доступ к привилегированным портам в rootless-режимеПубликация портов для контейнеров без root-прав возможна только для «высоких портов». Все порты ниже 1024 являются привилегированными и не могут быть использованы для публикации.Обычно проблема решается разрешением на изменение непривилегированных портов с помощью команды sysctl net.ipv4.ip_unprivileged_port_start=80как это описано в решении от RedHat.После этого, казалось бы, уже все должно было заработать, однако появилась другая проблема, которая заставила нас погрузиться глубже в теорию. При запуске контейнеров в непривилегированном режиме с помощью обычного compose они конфликтуют за одинаковые порты, которые используются сервисами, например порт 80.  Конфликт портов  Суть в том, что в привычном нам Docker контейнеры работают в режиме bridge networking. В нем каждый микросервис может сделать bind 0.0.0.0:80 внутри контейнера, и контейнер становится доступен в Docker-сети как \u003cимя контейнера\u003e:80. При этом “0.0.0.0” внутри контейнера – это ip-адрес \u003cимя контейнера\u003e (или имя сервиса) внутри виртуальной контейнерной подсети, то есть это не IP самого хоста. Следовательно, в Docker в привычном режиме bridge networking конфликта за порт 80 между контейнерами нет.Но Podman использует другой подход. При использовании rootless-доступа настройка сети происходит автоматически с помощью режима сети slirp4netns, который создает изолированный сетевой стек, позволяющий подключаться к сети изнутри контейнера и привязывать определенные порты контейнера к тем портам, которые доступны для пользователя на самом хосте. Иными словами slirp4netns создает туннель от хоста в контейнер для пересылки трафика. С помощью slirp4netns контейнеры полностью изолированы друг от друга. Виртуальной сети нет, поэтому для связи друг с другом контейнеры могут использовать проброс портов на хостовую систему - port mapping, и в этом месте как раз возникает проблема, что тот или иной порт уже занят первым стартовавшим контейнером. Или же их можно поместить в один Pod, где они будут использовать одно и то же сетевое пространство имен, где также будет конфликт за порты.В описании от RedHat это объясняется следующим образом:   When using Podman as a rootless user, the network setup is automatic. Technically, the container itself does not have an IP address, because without root privileges, network device association cannot be achieved. If you're running Podman without root, it can only use the network mode slirp4netns, which will create an isolated network stack so that you can connect to the internet from the inside of your container and bind certain ports of your container to the user-bindable ports on you host, but nothing more. To be able to select the network mode bridged, which does exactly what I need, you'll have to run Podman as root.Или тоже самое объяснение из документации Podman на GitHub:One of the drawbacks of slirp4netns is that the containers are completely isolated from each other. Unlike the bridge approach, there is no virtual network. For containers to communicate with each other, they can use the port mappings with the host system, or they can be put into a Pod where they share the same network namespace.Примерно так это можно проиллюстрировать схематически:Небольшое отступление про slirp4nets.Slirp4netns — это сетевой инструмент, используемый контейнерными средами исполнения для обеспечения сетевого подключения контейнеров без необходимости повышения привилегий.Slirp — это, изначально, программа, эмулирующая подключение PPP, SLIP или CSLIP к Интернет с использованием учетной записи текстовой оболочки. Это уже давно legacy инструмент. Еще в 90х годах прошлого столетия студенты из США активно использовали slirp, чтобы серфить в сети через выдаваемые университетами dial-up shell-терминалы. И в сети до сих пор можно найти эти инструкции. Однако возможности slirp до сих пор активно используются как в QEMU, так и для сетевой работы контейнеров, особенно для непривилегированных сетевых пространств имен.Но получается, что этот режим крайне неудобен в случае, когда разные контейнеры используют изнутри одни и те же порты.  Поскольку контейнеры работают в одном и том же пространстве имен сети, они делят один и тот же эмулированный сетевой стек, что приводит к конфликту портов внутри их изолированной сети.А режим сети bridged доступен только при запуске Podman с root доступом.Варианты решения ситуации с портамиМы сформулировали четыре варианта решения проблемы:Заменить rootless Podman на rootless Docker, который не имеет таких ограничений. Использовать Podman в привилегированном режиме.Изменить порты микросервисов, чтобы они отличались от порта 80.Обойти ограничение rootless Podman путем распределения контейнеров на:разные виртуальные машиныразные виртуальные сетиразные подыПервые два варианта не проходили по административным ограничениям, установленным заказчиком. Третий вариант слишком ресурсозатратный и во многом бессмысленный. В итоге нужно было думать, как изолировать контейнеры, конкурирующие за 80-й порт. Разделение на разные виртуальные машины — слишком ресурсозатратно.Разделение на сети вызывает сразу много вопросов, учитывая, что в slirp4netns режиме мы не оперируем сетями в принципе. Даже если их создать отдельно, то как будет работать связь между контейнерами в разных сетях, как будет работать разрешение имен, ведь получится, что на каждую сеть будет свой DNS resolver адрес?И тут на помощь приходит такой функционал Podman, как Podman Kube.Podman Kube теорияПро функцию Podman Kube статей мало, и описанных случаев применения на практике в реальных кейсах мне тоже не встретилось. И даже если спросить любую GPT-модель про основные команды Podman, в ответе не найдем ничего про Podman kube. Возможно, для большинства ситуаций эта возможность Podman выглядит избыточной, но нам она очень помогла.Попробуем разобраться, как это работает.В теории, когда мы создаем несколько подов, каждый под имеет свой собственный сетевой namespace. Это обеспечивает базовую изоляцию сетевого стека для каждого пода.Важной особенностью оказалось то, что для коммуникации между подами при запуске через kube play даже в непривилегированном (rootless) режиме Podman подключает поды к bridged-сети. Эта сеть может быть создана заранее, либо Podman создает ее автоматически. Таким образом, если специально не использовать хостовую сеть (параметр network_mode:host), создается новый сетевой стек bridged, что делает возможным взаимодействие между подами.При этом, хотя сеть и называется bridge, в rootless-режиме она реализуется как виртуальная сеть в пользовательском пространстве с помощью slirp4netns и CNI плагинов. Это позволяет создавать изолированные сетевые окружения для контейнеров без привилегий root, обеспечивая при этом функциональность, похожую на сетевой мост.​​​​​​​​ ​Таким образом, каждый под изолирован в своем сетевом namespace, но при этом все они подключены к одной виртуальной сети, обслуживаемой одним процессом slirp4netns, что позволяет использовать в разных подах одни и те же порты.​​​​​​​​На узле мы можем это увидеть, например, с помощью определенных команд. Начнем с вывода списка сетевых namespaces:# sudo lsns -t net \n        NS TYPE NPROCS   PID USER      NETNSID NSFS COMMAND\n4026531840 net     290     1 root   unassigned      /sbin/init\n4026532564 net       8 15259 podman unassigned      /catatonit -P\n…\n4026532635 net       1 15242 podman unassigned      /usr/sbin/dnsmasq -u root --conf-file=/run/user/915/containers/cni/dnsname/...\nЗдесь мы можем определить, что NS 15242 принадлежит процессу slirp4netns, в котором настраивается сетевое окружение с помощью CNI плагинов.Смотрим, что делает сам slirp4netns:# ps aux |grep slirp4netns \npodman   15173  0.1  0.1  37428 35512 ?        S    Jun11  76:02 /usr/bin/slirp4netns --disable-host-loopback --mtu=65520 --enable-sandbox --enable-seccomp --enable-ipv6 -c -r 3 --netns-type=path /run/user/915/netns/rootless-netns-5df09f647b449af857cc tap0С помощью этой команды можно ходить по разным нейспейсам и смотреть сетевые настройки: # sudo nsenter -t \u003cns_pid\u003e -n ip a   В namespaces, принадлежащих подам, мы увидим только lo и eth0 порты. В namespace, принадлежащему процессу slirp4netns, мы увидим tap0, cni-podman0 и veth пары с соответствующими link-netnsid. Также в namespace slirp4netns можно проверить правила iptables, созданные для реализации NAT.1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n   …\n2: tap0: \u003cBROADCAST,UP,LOWER_UP\u003e mtu 65520 qdisc pfifo_fast state UNKNOWN group default qlen 1000\n    ..\n    inet 10.0.2.100/24 brd 10.0.2.255 scope global tap0\n    3: cni-podman0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default qlen 1000\n    …\n    inet 10.89.0.1/24 brd 10.89.0.255 scope global cni-podman0\n       …\n4: veth3c23371b@if2: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue master cni-podman0 state UP group default\n    link/ether \u003cmac_address\u003e link-netnsid 0\n   …\n5: veth71b41f66@if2: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue master cni-podman0 state UP group default\n    link/ether \u003cmac_address\u003e link-netnsid 1\n    …\nПроцесс следующий: при запуске пода slirp4netns создает виртуальный сетевой стек в пользовательском пространстве. CNI управляет распределением IP-адресов для контейнеров в виртуальной сети и отвечает за маршрутизацию. Внутри каждого сетевого namespace создается виртуальный eth0 интерфейс. Этот eth0 виртуально подключается к общей сети через veth-пару. А сама сеть представляет собой интерфейс cni-podman0 — виртуальный мост, созданный CNI. С точки зрения пода, он имеет прямое подключение к сети. Для связи с хостом slirp4netns использует tap0 интерфейс. Он также существует в контексте slirp4netns, а не в namespace подов.Подам назначаются IP-адреса из диапазона, определенного для созданной bridge сети. Эти адреса видны только внутри виртуальной сети и недоступны напрямую с хоста. При этом поды могут общаться друг с другом через виртуальную сеть. Slirp4netns обеспечивает NAT для исходящего трафика от подов, но входящие соединения обычно требуют явного проброса портов. А чтобы под оставался «живым» при перезапуске или остановке содержащегося в нем контейнера, Podman использует так называемый infra-контейнер, основанный на образе pause, который не делает ровным счетом ничего. Его задача — резервировать и поддерживать в рабочем состоянии сам под и, соответственно, связанный с ним namespace на протяжении всего жизненного цикла всех входящих в него контейнеров.Примерно так это можно проиллюстрировать схематически:Таким образом эта конфигурация обеспечивает баланс между изоляцией (отдельные namespaces) и связностью (общая виртуальная сеть). Каждый под изолирован, но при этом все они подключены к одной виртуальной сети, обслуживаемой одним процессом slirp4netns.​​​​​​​​​​​​​​​​В этой парадигме kube play предоставляет особые возможности, потому что он, вероятно, по задумке разработчиков RedHat, должен быть как можно ближе по поведению к Kubernetes.Интересно, что упоминание об этом алгоритме работы Podman попадается в основном только в обсуждениях багов на github тут и тут.  Podman Kube практикаТеперь рассмотрим, как можно управлять этой конструкцией.Во многом функционал Podman Kube можно описать как некий Kubernetes на минималках. Для запуска контейнеров используются манифесты в формате YAML, которые максимально приближены к YAML-манифестам K8s. В них есть возможность указать ConfigMap, Volumes, VolumeMounts и описать запускаемый контейнер.По аналогии с compose-файлом можно описывать несколько контейнеров и запускать их либо в одном поде, либо в разных.Примеры манифестов:1. Описание ПодаВ нем мы по аналогии с K8s описываем apiVersion, kind, metadata и spec. В spec указываем описание контейнеров, вольюмов и параметров окружения.apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: nats\n  name: nats\nspec:\n  containers:\n  - name: nats\n    image: nats:2.9-alpine\n    args:\n    - --jetstream\n    - --port\n    - \"4222\"\n    - --http_port\n    - \"8222\"\n    - --store_dir\n    - /data\n    volumeMounts:\n    - mountPath: /data\n      name: nats-data\n  volumes:\n  - hostPath:\n      path: /home/podman/nats/data\n      type: Directory\n    name: nats-data2. Описание Пода вместе с ConfigMapТакже по аналогии с K8s в манифесте для ConfigMap описываем apiVersion, kind, metadata и указываем наши переменные среды. Далее в описании пода ссылаемся на ConfigMap.apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-config\ndata:\n  QUERY_PREFIX: /api/v1\n  CRUD_PREFIX: /api/v1\n  SSL_KEY_PATH: /ssl/tls.key\n  SSL_CERT_PATH: /ssl/tls.crt\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: nginx-router\n  name: nginx-router\nspec:\n  containers:\n  - name: nginx-router\n    image: nginx-router:5.6\n    args:\n    - nginx\n    - -g\n    - daemon off;\n    envFrom:\n    - configMapRef:\n        name: nginx-config\n        optional: false\n    ports:\n    - containerPort: 80\n      hostPort: 80\n    - containerPort: 443\n      hostPort: 443\n    volumeMounts:\n    - mountPath: /ssl/tls.crt\n      name: https-crt\n      subPath: /home/podman/cert/tls.crt\n    - mountPath: /ssl/tls.key\n      name: https-key\n      subPath: /home/podman/cert/tls.key\n    - mountPath: /var/cache/nginx\n      name: nginx-cache\n  volumes:\n  - hostPath:\n      path: /home/podman/cert/tls.crt\n      type: File\n    name: https-crt\n  - hostPath:\n      path: /home/podman/cert/tls.key\n      type: File\n    name: https-key\n  - hostPath:\n      path: /home/podman/nginx/nginx-cache\n      type: Directory\n    name: nginx-cacheАвтоматическое создание YAML-манифестовЧтобы использовать функционал Podman Kube, требуется писать YAML-манифесты с нуля, либо делать ручной рефакторинг compose-файлов в YAML-манифесты.Хорошо, что Podman упрощает жизнь возможностью генерации YAML-манифестов с помощью команды podman kube generate. Условие одно: контейнеры должны быть запущены. Тут можно воспользоваться либо командами podman run, либо тем же compose-файлом. При этом не обязательно, чтобы контейнер работал корректно и без ошибок. Требуется только, чтобы Podman видел их запущенными.Получив таким образом основное описание, мы можем его модифицировать. Например, удалить лишние или добавить требуемые ENV-переменные, скорректировать имена и параметры для volumes и volume-mounts. В общем, привести к виду, удовлетворяющему все ваши требования для корректного запуска.Пример последовательности команд:Запуск контейнера: podman run -d -it imageIDСмотрим id контейнера: podman psСоздаем манифест: podman kube generate ctrIDПрочие детали можно найти в документации RedHat.Итак, мы получили желаемое — скомпоновали контейнеры в поды, точнее, распределили их по манифестам для запуска в изолированных подах. После подготовки манифестов используются команды podman kube play и podman kube down для запуска и остановки подов.Например:podman kube play pod.yaml --network mynetpodman kube down pod.yamlСразу стоит отметить, что в Podman нет аналогов ReplicationController или ReplicaSet. То есть мы, к сожалению, не можем управлять количеством запущенных инстансов нашего пода.Работа с Podman secretsКаждый раз при создании манифестов мы сталкиваемся с необходимостью передавать чувствительные данные, например пароли подключения к базе данных или элементы учетных записей. Работа с чувствительными данным в Podman Kube основывается на стандартной функции podman secret. В манифесте YAML для Podman Kube мы можем определить, какие переменные мы будем брать из созданных секретов.К сожалению, на момент написания статьи не было возможности интегрировать создание «секрета» в общий манифест описания пода. С другой стороны, это позволяет использовать в разных подах одни и те же «секреты», что удобно, если сервисы обращаются к одним и тем же базам или IAM-провайдерам.Например, мы можем создать «секрет», описав его в отдельном YAML-манифесте:apiVersion: v1\ndata:\n  password: base64encodedvalue\nkind: Secret\nmetadata:\n  creationTimestamp: null\n  name: my-passwordИ выполнить команду podman kube play secret.yml.Затем можем обратиться к нему в манифесте с помощью инструкции: env:\n    - name: MY_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: my-password\n          key: passwordПодробнее это описано в документации RedHat.Трудности доступа к логам подовОтдельное внимание придется уделить правильным настройкам логирования. «Из коробки» мы получали ошибки при попытках выводить логи с помощью podman logs и флагом --follow.В режиме rootless с драйвером journald у Podman возникают проблемы доступа к логам, и для решения этой проблемы требуется внести правки в локальный файл настройки .config/containers/containers.conf:[containers]log_driver = \"k8s-file\"Больше деталей по этой теме есть в базе знаний RedHat.Хранение данных и прочие настройкиИ пару слов про хранение данных. В Podman на момент написания статьи поддерживается два вида volumes: persistentVolumeClaim и hostPath.Задать тип persistentVolumeClaim можно непосредственно в основном YAML описания пода, например: apiVersion: v1\n kind: PersistentVolumeClaim\n metadata:\n   name: example-pv-claim\n   labels:\n     app: example\n spec:\n   accessModes:\n   - ReadWriteOnce\n   resources:\n     requests:\n       storage: 20GiИ далее использовать его в самом поде по аналогии с манифестами в K8s: volumes:\n   - name: example-persistent-storage\n     persistentVolumeClaim:\n       claimName: example-pv-claimНадо заметить, что проект Podman развивается достаточно динамично, и первые версии полны всевозможных багов.Например, чтобы Podman Kube мог использовать нативные podman secretes для интеграции их с ENV-переменными, требуется версия не ниже 4.4, в которой исправлен баг, не позволяющий правильно преобразовывать содержимое «секретов» в режиме play kube.В версии Podman 4.4 таже есть возможность интегрировать управление подами в systemd с помощью Quadled.Коротко о том, как это работает. Мы можем создать манифест описания пода, как показано в примерах выше. И создать конфигурационный файл с расширением .kube, например example.kube, в котором укажем наш манифест:[Install]\nWantedBy=default.target\n[Kube]\nYaml=example.ymlДополнительно мы можем вносить директивы по аналогии с конфигурацией systemd сервисов, например, если требуется, чтобы под стартовал после другого пода:[Unit]\nRequires=first-pod.service\nAfter=first-pod.serviceДалее оба файла необходимо поместить в директорию ~/.config/containers/systemd/ и перезапустить systemd daemon: systemctl --user daemon-reloadЕсли мы описываем ConfigMap в отдельном файле, его можно указывать в блоке [Kube]: ConfigMap=example-configmap.ymlВ таком случае этот файл также требуется поместить в ~/.config/containers/systemd/.В блоке [Kube] также есть возможность указывать отдельно сеть и пробрасывать порты:Network=example.networkPublishPort=8000:8080В итоге мы можем запустить под с помощью systemctl следующим образом: systemctl --user start example.serviceПодробнее это описано в документации RedHat.Краткие итогиОказалось, что для работы в непривилегированном режиме, что всегда считалось одним из главных преимуществ, Podman предлагает пользователям уйти от парадигмы compose-файлов к использованию более современных и повсеместно используемых YAML-манифестов и их запуску через функционал “play kube”.Это позволяет обходить ограничения непривилегированного сетевого стека slipr4netns. А с точки зрения разработки и дальнейшего развертывания приложений в Kubernetes такой подход выглядит даже более рациональным, так как разработчики могут с самого начала запускать и тестировать сервисы локально уже с помощью “K8s-ready” YAML-манифестов, делая процессы непрерывной интеграции и доставки более бесшовными.Надеюсь, наш опыт послужит своеобразной подсказкой тем, кто окажется в подобной ситуации, и поможет увидеть большинство подводных камней, которые попадаются в процессе реализации данного подхода."
}