{
  "AI и Spring Petclinic: Внедрение ИИ-ассистента с Spring AI (Часть II)": "В продолжении статьи автор углубляется в возможности генерации с поддержкой поиска (RAG), чтобы LLM могла учесть некоторую контекстуальную информацию данных, первоначально невходившую в ее обучающую выборку.В новом переводе от команды Spring АйО рассматривается польза Spring AI и векторной БД, благодаря которым система не просто сопоставляет ключевые слова, но и понимает смысловые запросы пользователей, делая взаимодействие с приложением еще более естественным.Обзор первой частиВ первой части этой серии статей мы изучили основы интеграции Spring AI с крупными языковыми моделями. Мы рассмотрели создание кастомного ChatClient, использование Function Calling для динамических взаимодействий и доработку промптов, чтобы адаптировать их под потребности проекта Spring Petclinic. К концу первой части у нас был функциональный ИИ-ассистент, способный понимать и обрабатывать запросы, связанные с ветеринарной клиникой.Теперь, во второй части, мы пойдем дальше и рассмотрим метод генерации с поддержкой RAG (Retrieval-Augmented Generation) — технику, которая позволяет работать с большими наборами данных, не подходящими для типичного подхода Function Calling. Давайте посмотрим, как RAG может бесшовно интегрировать ИИ со знаниями, специфичными для домена.Retrieval-Augmented Generation (RAG)Хотя вывод списка ветеринаров мог бы быть реализован стандартным способом, я решил использовать этот пример, чтобы продемонстрировать возможности генерации с поддержкой поиска (RAG).RAG объединяет крупные языковые модели с реальным извлечением данных, что позволяет создавать более точные и контекстно значимые тексты. Хотя эта концепция связана с нашей предыдущей работой, RAG обычно акцентирует внимание на извлечении данных из векторной БД.Векторная БД содержит данные в форме эмбеддингов (embeddings) — числовых представлений, которые передают смысл информации, такой как данные о наших ветеринарах. Эти эмбеддинги хранятся как многомерные векторы, что позволяет эффективно выполнять поиск по смыслу, а не по тексту.Например, рассмотрим следующих ветеринаров и их специализации:Д-р Алиса Браун — КардиологияД-р Боб Смит — СтоматологияД-р Кэрол Уайт — ДерматологияПри обычном поиске запрос \"Чистка зубов\" не даст точного совпадения. Однако, с семантическим поиском, основанным на эмбеддингах, система поймет, что \"Чистка зубов\" относится к \"Стоматологии\". В результате д-р Боб Смит будет выведен как лучший результат, даже если в запросе прямо не упоминается его специализация. Это показывает, как эмбеддинги улавливают смысл, не ограничиваясь точным совпадением по ключевым словам. Хотя реализация этого процесса выходит за рамки статьи, вы можете узнать больше, посмотрев это видео на YouTube.Забавный факт — этот пример был сгенерирован самим ChatGPT.По сути, поиск по сходству работает путем нахождения ближайших числовых значений запроса по отношению к данным в источнике. Возвращается самое близкое совпадение. Процесс преобразования текста в числовые эмбеддинги также обрабатывается языковой моделью (LLM).Генерация тестовых данныхИспользование векторной БД наиболее эффективно при обработке значительных объемов данных. Поскольку шесть ветеринаров можно легко обработать за один вызов к языковой модели, я решил увеличить их число до 256. Хотя даже 256 может показаться относительно небольшим объемом, этого достаточно для иллюстрации нашего процесса.В данном примере ветеринары могут иметь ноль, одну или две специализации, аналогично оригинальным примерам из Spring Petclinic. Чтобы избежать утомительной задачи создания всех этих тестовых данных вручную, я обратился за помощью к ChatGPT. Он сгенерировал объединенный запрос, который создает 250 ветеринаров и назначает специализации 80% из них:-- Создание списка имен и фамилий\nWITH first_names AS (\n    SELECT 'James' AS name UNION ALL\n    SELECT 'Mary' UNION ALL\n    SELECT 'John' UNION ALL\n    ...\n),\nlast_names AS (\n    SELECT 'Smith' AS name UNION ALL\n    SELECT 'Johnson' UNION ALL\n    SELECT 'Williams' UNION ALL\n    ...\n),\nrandom_names AS (\n    SELECT\n        first_names.name AS first_name,\n        last_names.name AS last_name\n    FROM\n        first_names\n    CROSS JOIN\n        last_names\n    ORDER BY\n        RAND()\n    LIMIT 250\n)\nINSERT INTO vets (first_name, last_name)\nSELECT first_name, last_name FROM random_names;\n\n-- Добавление специализаций для 80% ветеринаров\nWITH vet_ids AS (\n    SELECT id\n    FROM vets\n    ORDER BY RAND()\n    LIMIT 200  -- 80% of 250\n),\nspecialties AS (\n    SELECT id\n    FROM specialties\n),\nrandom_specialties AS (\n    SELECT \n        vet_ids.id AS vet_id,\n        specialties.id AS specialty_id\n    FROM \n        vet_ids\n    CROSS JOIN \n        specialties\n    ORDER BY \n        RAND()\n    LIMIT 300 -- В среднем 2 специализации на одного ветеринара\n\n)\nINSERT INTO vet_specialties (vet_id, specialty_id)\nSELECT \n    vet_id,\n    specialty_id\nFROM (\n    SELECT \n        vet_id,\n        specialty_id,\n        ROW_NUMBER() OVER (PARTITION BY vet_id ORDER BY RAND()) AS rn\n    FROM \n        random_specialties\n) tmp\nWHERE \n    rn \u003c= 2;  -- Назначить не более 2 специализаций на одного ветеринара\n\n-- Оставшиеся 20% ветеринаров не будут иметь специализаций, поэтому дополнительные команды вставки не требуются\nЧтобы гарантировать, что мои данные остаются статичными и последовательными при каждом запуске, я экспортировал соответствующие таблицы из базы данных H2 в виде жестко заданных команд вставки (INSERT). Эти команды были добавлены в файл data.sql:INSERT INTO vets VALUES (default, 'James', 'Carter');\nINSERT INTO vets VALUES (default, 'Helen', 'Leary');\nINSERT INTO vets VALUES (default, 'Linda', 'Douglas');\nINSERT INTO vets VALUES (default, 'Rafael', 'Ortega');\nINSERT INTO vets VALUES (default, 'Henry', 'Stevens');\nINSERT INTO vets VALUES (default, 'Sharon', 'Jenkins');\nINSERT INTO vets VALUES (default, 'Matthew', 'Alexander');\nINSERT INTO vets VALUES (default, 'Alice', 'Anderson');\nINSERT INTO vets VALUES (default, 'James', 'Rogers');\nINSERT INTO vets VALUES (default, 'Lauren', 'Butler');\nINSERT INTO vets VALUES (default, 'Cheryl', 'Rodriguez');\n...\n...\n-- Всего 256 ветеринаров \n\n-- Сначала убедимся, что у нас есть 5 специализаций\nINSERT INTO specialties (name) VALUES ('radiology');\nINSERT INTO specialties (name) VALUES ('surgery');\nINSERT INTO specialties (name) VALUES ('dentistry');\nINSERT INTO specialties (name) VALUES ('cardiology');\nINSERT INTO specialties (name) VALUES ('anesthesia');\n\nINSERT INTO vet_specialties VALUES ('220', '2');\nINSERT INTO vet_specialties VALUES ('131', '1');\nINSERT INTO vet_specialties VALUES ('58', '3');\nINSERT INTO vet_specialties VALUES ('43', '4');\nINSERT INTO vet_specialties VALUES ('110', '3');\nINSERT INTO vet_specialties VALUES ('63', '5');\nINSERT INTO vet_specialties VALUES ('206', '4');\nINSERT INTO vet_specialties VALUES ('29', '3');\nINSERT INTO vet_specialties VALUES ('189', '3');\n...\n…\nВнедрение тестовых данныхДля реализации векторной БД у нас есть несколько вариантов. Наиболее популярным выбором, вероятно, является Postgres с расширением pgVector. Greenplum — масштабируемая параллельная база данных на основе Postgres — также поддерживает pgVector. В справочной документации Spring AI перечислены поддерживаемые на данный момент векторные БД.Для нашего простого случая я выбрал предоставленный Spring AI класс SimpleVectorStore. Этот класс реализует векторную БД с использованием простой Java ConcurrentHashMap, чего более чем достаточно для нашего небольшого набора данных из 256 ветеринаров. Конфигурация для этого хранилища, а также реализация памяти чата, определены в классе AIBeanConfiguration, аннотированном @Configuration:@Configuration\n@Profile(\"openai\")\npublic class AIBeanConfiguration {\n\n\t@Bean\n\tpublic ChatMemory chatMemory() {\n\t\treturn new InMemoryChatMemory();\n\t}\n\n\t@Bean\n\tVectorStore vectorStore(EmbeddingModel embeddingModel) {\n\t\treturn new SimpleVectorStore(embeddingModel);\n\t}\n\n}Векторная БД должна включать данные о ветеринарах сразу после запуска приложения. Для этого я добавил бин VectorStoreController, который содержит аннотированный метод @EventListener, отслеживающий событие ApplicationStartedEvent. Этот метод автоматически вызывается Spring`ом сразу после запуска приложения, что гарантирует добавление данных о ветеринарах в векторную БД в нужный момент:@EventListener\npublic void loadVetDataToVectorStoreOnStartup(ApplicationStartedEvent event) throws IOException {\n    // Извлекает все сущности Vet и создает документ для каждого ветеринара\n    Pageable pageable = PageRequest.of(0, Integer.MAX_VALUE);\n    Page\u003cVet\u003e vetsPage = vetRepository.findAll(pageable);\n    Resource vetsAsJson = convertListToJsonResource(vetsPage.getContent());\n    DocumentReader reader = new JsonReader(vetsAsJson);\n    List\u003cDocument\u003e documents = reader.get();\n\n    // добавляет документы в векторную БД\n    this.vectorStore.add(documents);\n\n    if (vectorStore instanceof SimpleVectorStore) {\n        var file = File.createTempFile(\"vectorstore\", \".json\");\n        ((SimpleVectorStore) this.vectorStore).save(file);\n        logger.info(\"vector store contents written to {}\", file.getAbsolutePath());\n    }\n    logger.info(\"vector store loaded with {} documents\", documents.size());\n}\n\npublic Resource convertListToJsonResource(List\u003cVet\u003e vets) {\n    ObjectMapper objectMapper = new ObjectMapper();\n    try {\n        // Convert List\u003cVet\u003e to JSON string\n        String json = objectMapper.writeValueAsString(vets);\n        // Convert JSON string to byte array\n        byte[] jsonBytes = json.getBytes();\n        // Create a ByteArrayResource from the byte array\n        return new ByteArrayResource(jsonBytes);\n    }\n    catch (JsonProcessingException e) {\n        e.printStackTrace();\n        return null;\n    }\n}Здесь достаточно много нюансов, поэтому разберем код пошагово:Как и в методе listOwners, мы начинаем с извлечения всех ветеринаров из базы данных.Spring AI добавляет в векторную БД сущности типа Document. Document представляет собой числовые данные эмбеддингов вместе с оригинальным, читаемым текстом. Такое двойное представление позволяет коду сопоставлять связи между встроенными векторами и изначальным текстом.Чтобы создать эти сущности Document, нам нужно преобразовать наши сущности Vet в текстовый формат. Spring AI предоставляет два встроенных readers для этой цели: JsonReader и TextReader. Так как наши сущности Vet имеют структурированный формат, имеет смысл представить их в виде JSON. Для этого мы используем вспомогательный метод convertListToJsonResource, который с помощью парсера Jackson преобразует список ветеринаров в JSON-ресурс, загружаемый в память.Затем мы вызываем метод add(documents) для векторной БД. Этот метод отвечает за добавление данных, поочередно обрабатывая каждый документ (наших ветеринаров в формате JSON) и встраивая его, связывая с оригинальными метаданными.Хотя это и не обязательно, мы также создаем файл vectorstore.json, который представляет текущее состояние нашей базы данных SimpleVectorStore. Этот файл позволяет нам увидеть, как Spring AI интерпретирует сохраненные данные «за кулисами». Давайте посмотрим на сгенерированный файл, чтобы понять, что именно видит Spring AI.{\n  \"dd919c71-06bb-4777-b974-120dfee8b9f9\" : {\n    \"embedding\" : [ 0.013877872, 0.03598228, 0.008212427, 0.00917901, -0.036433823, 0.03253927, -0.018089917, -0.0030867155, -0.0017038669, -0.048145704, 0.008974405, 0.017624263, 0.017539598, -4.7888185E-4, 0.013842596, -0.0028221398, 0.033414137, -0.02847539, -0.0066955267, -0.021885695, -0.0072387885, 0.01673529, -0.007386951, 0.014661016, -0.015380662, 0.016184973, 0.00787377, -0.019881975, -0.0028785826, -0.023875304, 0.024778388, -0.02357898, -0.023748307, -0.043094076, -0.029322032, ... ],\n    \"content\" : \"{id=31, firstName=Samantha, lastName=Walker, new=false, specialties=[{id=2, name=surgery, new=false}]}\",\n    \"id\" : \"dd919c71-06bb-4777-b974-120dfee8b9f9\",\n    \"metadata\" : { },\n    \"media\" : [ ]\n  },\n  \"4f9aabed-c15c-43f6-9dbc-46ed9a18e176\" : {\n    \"embedding\" : [ 0.01051745, 0.032714732, 0.007800559, -0.0020621764, -0.03240663, 0.025530376, 0.0037602335, -0.0023702774, -0.004978633, -0.037364256, 0.0012831709, 0.032742742, 0.005430281, 0.00847278, -0.004285406, 0.01146276, 0.03036196, -0.029941821, 0.013220336, -0.03207052, -7.518716E-4, 0.016665466, -0.0052062077, 0.010678503, 0.0026591222, 0.0091940155, ... ],\n    \"content\" : \"{id=195, firstName=Shirley, lastName=Martinez, new=false, specialties=[{id=1, name=radiology, new=false}, {id=2, name=surgery, new=false}]}\",\n    \"id\" : \"4f9aabed-c15c-43f6-9dbc-46ed9a18e176\",\n    \"metadata\" : { },\n    \"media\" : [ ]\n  },\n  \"55b13970-cd55-476b-b7c9-62337855ae0a\" : {\n    \"embedding\" : [ -0.0031563698, 0.03546827, 0.018778138, -0.01324492, -0.020253662, 0.027756566, 0.007182742, -0.008637386, -0.0075725033, -0.025543278, 5.850768E-4, 0.02568248, 0.0140383635, -0.017330453, 0.003935892, ... ],\n    \"content\" : \"{id=19, firstName=Jacqueline, lastName=Ross, new=false, specialties=[{id=4, name=cardiology, new=false}]}\",\n    \"id\" : \"55b13970-cd55-476b-b7c9-62337855ae0a\",\n    \"metadata\" : { },\n    \"media\" : [ ]\n  },\n...\n...\n...Очень здорово! У нас есть ветеринар в формате JSON, рядом с набором чисел, которые, возможно, не имеют для нас особого смысла, но весьма значимы для языковой модели. Эти числа представляют собой данные эмбеддингов, которые модель использует для понимания связей и семантики сущности Vet на уровне, значительно превышающем простое сопоставление текста.Оптимизация затрат и ускорение запускаЕсли запускать этот метод векторизации данных при каждом перезапуске приложения, это приведет к двум значительным недостаткам:Долгое время запуска: каждый JSON-документ с данными о ветеринаре придется повторно встраивать, отправляя запросы к языковой модели, что негативно повлияет на загрузку приложения.Повышенные затраты: векторизация 256 документов потребует отправки 256 запросов к языковой модели при каждом запуске, что приведет к дополнительной тарификации.Эмбеддинги лучше всего подходят для процессов ETL (Extract, Transform, Load) или потоковой обработки, которые выполняются независимо от основного веб-приложения. Эти процессы могут выполнять векторизацию в фоновом режиме, не влияя на user experience и избегая лишних затрат.Чтобы упростить задачу в Spring Petclinic, я решил загружать заранее подготовленную векторную БД при запуске. Этот подход обеспечивает мгновенную загрузку и исключает дополнительные расходы на LLM. Вот, как я добавил это в метод, чтобы достичь такой оптимизации:@EventListener\npublic void loadVetDataToVectorStoreOnStartup(ApplicationStartedEvent event) throws IOException {\n    Resource resource = new ClassPathResource(\"vectorstore.json\");\n    // Проверка существования файла\n    if (resource.exists()) {\n        // Чтобы сэкономить кредиты на ИИ, используем pre-embedded базу данных, которая была сохранена\n        // на диск на основе текущих данных в файле h2 data.sql\n        File file = resource.getFile();\n        ((SimpleVectorStore) this.vectorStore).load(file);\n        logger.info(\"vector store loaded from existing vectorstore.json file in the classpath\");\n        return;\n    }\n\n    // Остальная часть метода остается прежней\n    ...\n    ...\n}Файл vectorstore.json расположен в каталоге src/main/resources, что гарантирует, что приложение всегда будет загружать заранее подготовленную векторную БД при запуске, а не создавать её заново. Если нам потребуется обновить векторную БД, мы можем просто удалить существующий файл vectorstore.json и перезапустить приложение. После того как новая БД будет создано, мы можем поместить обновленный файл vectorstore.json обратно в src/main/resources. Такой подход обеспечивает гибкость, избегая ненужной повторной векторизации данных при регулярных перезапусках.Реализация поиска по сходствуТеперь, когда наша векторная БД готова, реализация функции listVets становится простой. Функция определяется следующим образом:@Bean\n@Description(\"Список ветеринаров, работающих в клинике для животных\")\npublic Function\u003cVetRequest, VetResponse\u003e listVets(AIDataProvider petclinicAiProvider) {\n    return request -\u003e {\n        try {\n            return petclinicAiProvider.getVets(request);\n        }\n        catch (JsonProcessingException e) {\n            e.printStackTrace();\n            return null;\n        }\n    };\n}\nrecord VetResponse(List\u003cString\u003e vet) {\n};\n\nrecord VetRequest(Vet vet) {\n}Вот реализация в AIDataProvider:public VetResponse getVets(VetRequest request) throws JsonProcessingException {\n    ObjectMapper objectMapper = new ObjectMapper();\n    String vetAsJson = objectMapper.writeValueAsString(request.vet());\n    SearchRequest sr = SearchRequest.from(SearchRequest.defaults()).withQuery(vetAsJson).withTopK(20);\n    if (request.vet() == null) {\n        // Provide a limit of 50 results when zero parameters are sent\n        sr = sr.withTopK(50);\n    }\n    List\u003cDocument\u003e topMatches = this.vectorStore.similaritySearch(sr);\n    List\u003cString\u003e results = topMatches.stream().map(document -\u003e document.getContent()).toList();\n    return new VetResponse(results);\n}Давайте рассмотрим, что мы сделали:Мы начинаем с сущности Vet из запроса. Поскольку записи в нашей векторной БД представлены в формате JSON, первым шагом является преобразование сущности Vet также в JSON.Затем мы создаем SearchRequest, который передается в метод similaritySearch векторной БД. SearchRequest позволяет настроить поиск в зависимости от наших конкретных потребностей. В данном случае мы в основном используем параметры по умолчанию, за исключением параметра topK, который определяет количество возвращаемых результатов. По умолчанию это значение установлено на 4, но в нашем случае мы увеличиваем его до 20, чтобы обрабатывать более широкие запросы, такие как «Сколько ветеринаров специализируются на кардиологии?»Если в запросе не указаны фильтры (то есть сущность Vet пуста), мы увеличиваем значение topK до 50. Это позволяет нам возвращать до 50 ветеринаров для таких запросов, как «показать список ветеринаров клиники». Конечно, это будет не весь список, ведь мы хотели бы избежать перегрузки LLM избыточными данными. Тем не менее, это должно быть оптимально, так как мы постарались тщательно настроить системный текст для обработки подобных случаев:При работе с ветеринарами, если пользователь не уверен в полученных результатах, объясните, что может существовать дополнительная информация, которая не была возвращена. Только если пользователь спрашивает об общем количестве всех ветеринаров, ответьте, что их много, и запросите дополнительные критерии. Для владельцев, питомцев или визитов - предоставьте точные данные.Последним шагом является вызов метода similaritySearch. Затем мы преобразуем getContent() каждого возвращенного результата, так как именно он содержит фактические данные JSON о ветеринарах, а не встроенные данные эмбеддингов.Далее все идет как обычно. Языковая модель завершает вызов функции, получает результаты и определяет наилучший способ отображения данных в чате.Давайте посмотрим, как это работает на практике:Похоже, наш системный текст работает как ожидается, предотвращая любую перегрузку. Теперь давайте попробуем указать некоторые конкретные критерии:Данные, возвращенные от языковой модели, именно такие, как мы ожидали. Давайте попробуем задать более общий вопрос:Языковая модель успешно определила как минимум 20 ветеринаров, специализирующихся на кардиологии, соблюдая установленный нами верхний предел topK(20). Однако, если есть неопределенность в результатах, модель отмечает, что могут быть доступны дополнительные ветеринары, как указано в нашем системном тексте.Реализация пользовательского интерфейсаСоздание интерфейса для чат-бота включает работу с Thymeleaf, JavaScript, CSS и препроцессором SCSS.После анализа кода я решил разместить чат-бота в месте, доступном с любой вкладки, что делает layout.html идеальным выбором.Во время обсуждения PR с доктором Дейвом Сайером я понял, что не следует изменять файл petclinic.css напрямую, так как Spring Petclinic использует SCSS-препроцессор для генерации CSS-файла.Признаюсь, я в основном backend-разработчик, специализирующийся на Spring, облачной архитектуре, Kubernetes и Cloud Foundry. У меня есть некоторый опыт работы с Angular, но я не являюсь экспертом в разработке фронтенда. Вероятно, я мог бы что-то создать, но это вряд ли выглядело бы профессионально.К счастью, у меня был отличный партнер для парного программирования — ChatGPT. Если вам интересно, как я разработал код для интерфейса, вы можете ознакомиться с этой сессией ChatGPT. Удивительно, сколько можно узнать, сотрудничая с большими языковыми моделями в упражнениях по кодированию. Просто помните, что всегда следует тщательно проверять предложенные решения, а не копировать их вслепую.ЗаключениеПосле нескольких месяцев работы со Spring AI я по-настоящему оценил вложенные в проект мысли и усилия. Spring AI действительно уникален, так как позволяет разработчикам изучать мир ИИ, не требуя от них освоения нового языка, такого как Python. Более того, этот опыт подчеркивает еще одно важное преимущество: ваш код с ИИ может сосуществовать в том же коде, что и ваша бизнес-логика. Вы можете легко расширить старую кодовую базу возможностями ИИ, добавив всего несколько дополнительных классов. Способность избегать необходимости перестраивать все ваши данные с нуля в новом приложении, специально ориентированном на ИИ, значительно повышает производительность. Даже такие простые функции, как автоматическое завершение кода для существующих сущностей JPA в IDE, вносят огромный вклад.Spring AI имеет потенциал существенно улучшить приложения на базе Spring, упрощая интеграцию возможностей ИИ. Он дает разработчикам возможность использовать модели машинного обучения и сервисы на основе ИИ без необходимости глубоких знаний в области науки о данных. Абстрагируя сложные операции ИИ и встраивая их непосредственно в привычные фреймворки Spring, разработчики могут сосредоточиться на быстром создании интеллектуальных, управляемых данными функций. Этот бесшовный союз ИИ и Spring создает среду, где инновации не ограничиваются техническими барьерами, открывая новые возможности для разработки более умных и адаптивных приложений.Присоединяйтесь к русскоязычному сообществу разработчиков на Spring Boot в телеграм — Spring АйО, чтобы быть в курсе последних новостей из мира разработки на Spring Boot и всего, что с ним связано.Ждем всех, присоединяйтесь",
  "Documentation as Code: как мы создали новую версию документации для Rest API": "Привет! Меня зовут Сергей Востриков, я руковожу направлением Маркет и интеграций в Битрикс. Иными словами, я помогаю развивать функционал Битрикс24, доступный для разработчиков тиражных решений и индивидуальных кастомизаций. Это значит REST API и всё «вокруг» него — документацию, витрину Битрикс24 Маркет, кабинет разработчика решений и т.д.REST API Битрикс24 включает в себя просто страшно сказать сколько методов, событий, встроек виджетов и прочих нюансов. Без документации с этим, конечно, совершенно невозможно иметь дело. И хотя нельзя сказать, что документации у нас не было, надо признать, что с течением времени у разработчиков накопилось к ней немало претензий.Недавно мы показали сообществу разработчиков нашу новую документацию и, наконец, можем рассказать о том, как появился этот проект, как мы над ним работали и что планируем делать дальше. Изменения назревали давно — копились проблемы, существующая платформа требовала слишком большого количества ресурсов, вносить материалы через админку в инфоблоки становилось все труднее, не говоря уже о сложностях с автоматизацией процесса при такой «технологии».Добавлю, что  в требования к третьей версии Rest API, которая (спойлер!) сейчас находится в разработке,  мы заложили автоматическую генерацию документации из исходного кода продукта, вместо того чтобы продолжать вручную писать таблички с описанием методов.  Однако ждать выхода нового Rest API мы не стали — нам предстояла долгая работа, мы решили подготовиться заранее. Кроме того, текущую версию REST API мы будем поддерживать еще долго, так что хорошая документация для неё всё равно нужна.Новая документацияПланируя работу над новой версией, мы провели опрос среди разработчиков, чтобы понять, что нужно менять в первую очередь. Так мы сформулировали для себя несколько целей (помимо задачи автоматической генерации документации из исходного кода):Довaести контент до стандарта, принятого в технической документации по API. Каждый метод должен включать подробный и полный перечень параметров, их типов, взаимосвязь с другими методами, примеры вызова метода на основных языках веб-разработки, подробный пример возвращаемых данных, полный перечень возможных ошибок с описаниями этих ошибок.Сформировать единое информационное пространство, целиком посвященное REST API Битрикс24, и перенести в него информацию, которая раньше хранилась в нескольких курсах и справочнике методов. Перестроить структуру документации так, чтобы она, в первую очередь, помогала быстро погрузиться в REST API, быстро выполнить первый запрос, быстро найти готовые типовые сценарии использования REST API и не пропустить наши рекомендации для более сложных кейсов.Создать условия, которые помогут внешним разработчикам оперативно делиться своим практическим опытом и предлагать улучшения документации прямо в ней, на месте.Создать условия для быстрой локализации документации на другие языки, а также упростить включение примеров кода на разных языках программирования или с использованием разных SDK.Может показаться, что все эти цели достигаются, прежде всего, изменением контента и мало зависят от технологий. Но, фактически нам потребовалось полностью перестроить процесс создания документации, а это можно было сделать, только поменяв платформу. Именно этим опытом и хочется сегодня поделиться.Новый формат и новая платформаСтандартом отрасли для технической документации давно является markdown. Он не такой избыточный, как html, но поддерживает форматирование, удобен для технических писателей, нагляден при работе с контролем версий и прекрасно транслируется в самые разные конечные формы документации. С markdown умеют работать практически все платформы создания технической документации — все они поддерживают те или иные расширения разметки, интегрированы с Github и предоставляют необходимую функциональность. Мы посмотрели несколько решений и остановились на  российской Diplodoc от Яндекса. Для нас большой плюс в том, что мы постоянно на связи с разработчиками, они оперативно помогают нам.Новые процессыВыбор платформы — это не столько выбор «генератора документации», сколько кардинальный переход к парадигме «Documentation as Code». С помощью нашего искусственного интеллекта CoPilot в Битрикс24 мы перевели всю имеющуюся документацию в формат markdown/yaml и приземлили все материалы  в открытой репе Github.Мы больше не  работаем с документацией как с набором текстов. Мы работаем с ней как с программным кодом. И внешние контрибьюторы в лице разработчиков приложений теперь тоже часть процесса. Вместо «Оставьте комментарий», как это было в старой документации, любой теперь может внести изменение в материалы через pull-request или создать issue на Github. И то, и другое оперативно обрабатывается командой авторов.Цикл подготовки материалов теперь выглядит следующим образом.Внутренние разработчики продукта, получив задачу от команды документации, предоставляют материалы сразу в markdown. Они коммитят описание методов, параметров и базовый пример прямо в репозиторий документации. Базовый пример кода для каждого метода на JS должен максимально использовать все параметры метода, чтобы показать в документации все возможности. Pull-request от разработчиков проверяется авторами документации. Каждый автор оформляет описание метода в соответствии с нашими гайдами — у нас есть чек-лист из 20 пунктов, который обязательно должен быть пройден при получении заготовки от разработчиков. На этом этапе мы снова подключаем CoPilot. Имея базовый пример на JS, остальные мы генерируем автоматически, в этом нам помогает специальный бот «Генератор примеров». Так что обязательные примеры вызова для каждого метода мы теперь показываем сразу в 4-х вариантах. Окончательная приемка изменений ложится на выпускающего редактора, который, а точнее, которая, не просто работает с текстами, но по настоящему «умеет в REST API Битрикс24». Именно она решает, можно ли допустить публикацию изменений, или требуются доработки. Вся команда документации сидит в VSCode. Контроль версий позволяет точно видеть, какие изменения, кем и когда были сделаны. Коллеги уже не представляют себе, как можно было работать с документацией иначе.Все изменения в основной ветке документации автоматически запускают пересборку публичной версии с помощью Diplodoc, который легко решает технические задачи: автогенерацию общей структуры, автогенерацию оглавления внутри каждой статьи статьи, автогенерацию ссылок на предыдущую и следующую статью, встроенный механизм описания диаграмм и схем. Так что, диаграммы и схемы теперь не картинки, а структуры на markdown, а значит тоже легко отслеживаются в контроле версий и удобно локализуются: Отмечу еще один важный плюс платформы. Мы не экономим на ссылках в документации, всегда добавляем ссылки на имеющиеся туториалы, в которых используется описываемый метод. Платформа сама следит за целостностью структуры и не дает делать ссылки «в никуда». Это сильно облегчает задачу авторов подсказывать читателю, где он может найти связанные полезные статьи.Как это выглядит сейчасНовую документацию мы уже анонсировали в чатах с разработчиками, получили много позитивных откликов — видим, что многие с энтузиазмом восприняли идею портить документацию от Битрикс  вносить правки в документацию. Для разработчиков это привычный, понятный и удобный подход.  Для нас возможность пустить разработчиков напрямую в документацию — это возможность интегрировать их практический опыт использования REST API в документацию для остальных и более быстрое внесение изменений, поскольку техническим писателям не приходится вручную копипастить материалы. Им достаточно просто причесать сделанные разработчиками изменения по части текста.В планах у нас проработка гайда для внешних контрибьюторов, это поможет сделать работу с документацией еще более удобной и разработчикам, и нашей команде. Пока поделимся примерами оформления материалов и скриншотами новой версии. Что с английской версиейКак я писал выше, одной из целей изменения процессов и технологии была потребность обновления документации для западных разработчиков.Англоязычная документация имела те же проблемы, что и русскоязычная, но ещё и сильно отставала по времени обновления, так как переводили её наши специалисты из команды локализации, которые также занимаются и локализацией самого продукта. В результате в силу ограниченности ресурсов перевод документации сильно запаздывал. Изменением технологического процесса и переходом на markdown я хотел добиться того, чтобы мы могли включить в этот процесс автоперевод на другие языки. Сейчас есть ChatGPT, есть наш CoPilot, его можно использовать во многих задачах. Мы уже поэкспериментировали с этим — делая скринкасты для англоговорящих пользователей, я писал сценарий на русском, потом переводил с помощью ChatGPT и отдавал на аудит в отдел локализации. Правки у коллег были минимальные, в основном стилистические — то, что можно назвать вкусовщиной.Претензий к качеству перевода технической документации с помощью ИИ у них не было. Сейчас технический писатель раз в неделю собирает готовые изменения и с помощью CoPilot переводит их на английский,  проверяет и отправляет в английскую версию документации, она также пересобирается автоматически. Несмотря на укрепляющееся доверие к автопереводу с помощью ИИ на основе трехэтажного промпта, остаются ручные задачи, в частности:Проверка сборки переведенной документации. Не исключаем, что во время перевода могут быть испорчены какие-то фрагменты разметки. Хотя пока такого не было. Замена скриншотов интерфейса Битрикс24 на локализованные. Здесь искусственный интеллект пока бессилен, задачу решает команда Документации.С новой платформой и переводом от CoPilot нас ничто не останавливает, мы можем быстро переводить документацию на любые языки. ",
  "Tribuo и регрессия: как строить предсказательные модели на Java": "Привет, Хабр!В этой статье наш взор упадет на на Tribuo — библиотеку машинного обучения на Java от Oracle.Tribuo поддерживает различные алгоритмы для классификации, регрессии, кластеризации и многого другого. Но сегодня мы сосредоточимся на регрессии — фундаментальной задаче, которая позволяет предсказывать непрерывные значения. Одним из главных плюсов Tribuo является её удобный API, который позволяет быстро строить модели и оценивать их эффективность.Установка и настройка проектаНачнём с самого начала — настройки проекта. Для работы с Tribuo понадобится Java 11 или новее. Также будем использовать Maven для управления зависимостями. Если вы используете другую систему сборки, то принципы останутся теми же, только синтаксис зависимостей будет другим.Добавим Tribuo в наш pom.xml:\u003cdependencies\u003e\n    \u003c!-- Tribuo Core --\u003e\n    \u003cdependency\u003e\n        \u003cgroupId\u003eorg.tribuo\u003c/groupId\u003e\n        \u003cartifactId\u003etribuo-all\u003c/artifactId\u003e\n        \u003cversion\u003e4.3.0\u003c/version\u003e\n    \u003c/dependency\u003e\n    \u003c!-- Для работы с CSV --\u003e\n    \u003cdependency\u003e\n        \u003cgroupId\u003eorg.tribuo\u003c/groupId\u003e\n        \u003cartifactId\u003etribuo-data\u003c/artifactId\u003e\n        \u003cversion\u003e4.3.0\u003c/version\u003e\n    \u003c/dependency\u003e\n    \u003c!-- Для линейной регрессии --\u003e\n    \u003cdependency\u003e\n        \u003cgroupId\u003eorg.tribuo\u003c/groupId\u003e\n        \u003cartifactId\u003etribuo-regression-linear\u003c/artifactId\u003e\n        \u003cversion\u003e4.3.0\u003c/version\u003e\n    \u003c/dependency\u003e\n    \u003c!-- Для CART регрессии --\u003e\n    \u003cdependency\u003e\n        \u003cgroupId\u003eorg.tribuo\u003c/groupId\u003e\n        \u003cartifactId\u003etribuo-regression-cart\u003c/artifactId\u003e\n        \u003cversion\u003e4.3.0\u003c/version\u003e\n    \u003c/dependency\u003e\n\u003c/dependencies\u003eОбновляем зависимости и все готово!Подготовка данныхРабота с данными — это первый и один из самых важных шагов в машинном обучении. Для примера возьмём простой CSV-файл с данными о ценах домов:square_feet,rooms,has_garage,price\n1200,3,1,250000\n1400,3,0,300000\n1600,4,1,350000\n...Здесь:square_feet — площадь дома в квадратных футах,rooms — количество комнат,has_garage — наличие гаража (1 — есть, 0 — нет),price — цена дома (целевая переменная).Используем CSVLoader для загрузки данных:import org.tribuo.data.csv.CSVLoader;\nimport org.tribuo.data.csv.CSVDataSource;\nimport org.tribuo.regression.Regressor;\nimport org.tribuo.regression.RegressorFactory;\n\nimport java.nio.file.Paths;\n\npublic class DataLoader {\n    public static CSVDataSource\u003cRegressor\u003e loadData(String filePath, String targetColumn) throws IOException {\n        CSVLoader\u003cRegressor\u003e loader = new CSVLoader\u003c\u003e(new RegressorFactory());\n        return loader.loadData(Paths.get(filePath), targetColumn);\n    }\n}После этого нужно разделить данные, чтобы оценить модель на невидимых данных. Используем TrainTestSplitter:import org.tribuo.data.split.TrainTestSplitter;\nimport org.tribuo.regression.Regressor;\nimport org.tribuo.data.dataset.Dataset;\n\npublic class DataSplitter {\n    public static TrainTestSplitter\u003cRegressor\u003e splitData(Dataset\u003cRegressor\u003e data, double trainFraction, long seed) {\n        return new TrainTestSplitter\u003c\u003e(data, trainFraction, seed);\n    }\n}trainFraction — доля данных для обучения (например, 0.7 для 70%), а seed — случайное зерно для воспроизводимости.Построение регрессионной моделиТеперь перейдем к созданию моделей. Начнём с простой линейной регрессии.import org.tribuo.Model;\nimport org.tribuo.regression.Regressor;\nimport org.tribuo.regression.linear.LinearRegressionTrainer;\n\npublic class LinearRegressionModel {\n    public static Model\u003cRegressor\u003e trainModel(Dataset\u003cRegressor\u003e trainData) {\n        LinearRegressionTrainer trainer = new LinearRegressionTrainer(0.01, LinearRegressionTrainer.LossType.SQUARED);\n        return trainer.train(trainData);\n    }\n}LinearRegressionTrainer — тренер для линейной регрессии. LossType.SQUARED — тип функции потерь (в данном случае квадратичная).Для более сложных задач можно использовать CARTRegressionTrainer, который реализует алгоритм случайных лесов.import org.tribuo.regression.cart.CARTRegressionTrainer;\n\npublic class RandomForestRegressionModel {\n    public static Model trainModel(Dataset trainData, int numTrees) {\n        CARTRegressionTrainer trainer = new CARTRegressionTrainer(numTrees);\n        return trainer.train(trainData);\n    }\n}numTrees — количество деревьев в лесу.Оценка моделиПосле обучения модели важно понять, насколько она хороша. Для этого используем RegressionEvaluator.import org.tribuo.regression.RegressionEvaluator;\nimport org.tribuo.Model;\nimport org.tribuo.regression.Regressor;\n\npublic class ModelEvaluator {\n    public static void evaluateModel(Model model, Dataset testData) {\n        RegressionEvaluator evaluator = new RegressionEvaluator();\n        var evaluation = evaluator.evaluate(model, testData);\n        System.out.println(evaluation);\n    }\n}evaluate — метод, который принимает модель и тестовые данные, возвращает метрики.Пример прогнозирования цен на домаСоберём всё вместе и создадим полный пример.import org.tribuo.Model;\nimport org.tribuo.data.csv.CSVDataSource;\nimport org.tribuo.data.dataset.Dataset;\nimport org.tribuo.data.split.TrainTestSplitter;\nimport org.tribuo.regression.Regressor;\nimport org.tribuo.regression.RegressionEvaluator;\nimport org.tribuo.regression.linear.LinearRegressionTrainer;\nimport org.tribuo.regression.cart.CARTRegressionTrainer;\n\nimport java.io.IOException;\n\npublic class HousePricePrediction {\n    public static void main(String[] args) {\n        try {\n            // Шаг 1: Загрузка данных\n            CSVDataSource dataSource = DataLoader.loadData(\"house_prices.csv\", \"price\");\n            Dataset data = dataSource.getDataset();\n\n            // Шаг 2: Разделение данных\n            TrainTestSplitter splitter = DataSplitter.splitData(data, 0.7, 42L);\n            Dataset trainData = splitter.getTrainingDataset();\n            Dataset testData = splitter.getTestDataset();\n\n            // Шаг 3: Обучение линейной регрессии\n            Model linearModel = LinearRegressionModel.trainModel(trainData);\n            System.out.println(\"Линейная регрессия обучена!\");\n\n            // Шаг 4: Обучение Random Forest регрессии\n            Model rfModel = RandomForestRegressionModel.trainModel(trainData, 100);\n            System.out.println(\"Random Forest регрессия обучена!\");\n\n            // Шаг 5: Оценка моделей\n            System.out.println(\"Оценка линейной регрессии:\");\n            ModelEvaluator.evaluateModel(linearModel, testData);\n\n            System.out.println(\"Оценка Random Forest регрессии:\");\n            ModelEvaluator.evaluateModel(rfModel, testData);\n\n        } catch (IOException e) {\n            System.err.println(\"Ошибка при загрузке данных: \" + e.getMessage());\n        }\n    }\n}Параметры тренераTribuo позволяет настраивать множество параметров тренера. Например, для линейной регрессии можно настроить:Learning Rate (коэффициент обучения): Влияет на скорость сходимости.Loss Type (тип функции потерь): Помимо квадратичной, доступны другие типы, такие как абсолютная ошибка.Пример с другими параметрами:LinearRegressionTrainer trainer = new LinearRegressionTrainer(\n    0.05, // Коэффициент обучения\n    LinearRegressionTrainer.LossType.HUBER, // Функция потерь Хьюбера\n    1.0 // Параметр delta для функции Хьюбера\n);Как избежать переобученияКросс-валидация — мощный инструмент для оценки модели. Внедрим 5-кратную кросс-валидацию.import org.tribuo.data.cross.CrossValidator;\nimport org.tribuo.data.cross.CrossValidationResult;\nimport org.tribuo.regression.Regressor;\n\npublic class CrossValidationExample {\n    public static void performCrossValidation(Dataset data, int folds) {\n        LinearRegressionTrainer trainer = new LinearRegressionTrainer(0.01, LinearRegressionTrainer.LossType.SQUARED);\n        CrossValidator crossValidator = new CrossValidator\u0026lt;\u0026gt;(trainer, folds);\n        CrossValidationResult result = crossValidator.evaluate(data);\n        System.out.println(result);\n    }\n}folds — количество разбиений (например, 5 для 5-кратной кросс-валидации). evaluate — метод, который выполняет кросс-валидацию и возвращает результат.Как подобрать гиперпараметрыПодбор гиперпараметров может значительно бустануть качество модели. Рассмотрим простой пример перебора параметров с использованием Grid Search.import org.tribuo.regression.linear.LinearRegressionTrainer;\nimport org.tribuo.Model;\nimport org.tribuo.regression.Regressor;\n\npublic class HyperparameterTuning {\n    public static void gridSearch(Dataset trainData, Dataset testData) {\n        double[] learningRates = {0.001, 0.01, 0.1};\n        LinearRegressionTrainer.LossType[] lossTypes = {\n            LinearRegressionTrainer.LossType.SQUARED,\n            LinearRegressionTrainer.LossType.HUBER\n        };\n\n        for (double lr : learningRates) {\n            for (LinearRegressionTrainer.LossType lt : lossTypes) {\n                LinearRegressionTrainer trainer = new LinearRegressionTrainer(lr, lt);\n                Model model = trainer.train(trainData);\n                RegressionEvaluator evaluator = new RegressionEvaluator();\n                var evaluation = evaluator.evaluate(model, testData);\n                System.out.println(\"Learning Rate: \" + lr + \", Loss Type: \" + lt);\n                System.out.println(evaluation);\n            }\n        }\n    }\n}Перебираем различные значения learning rate и loss type, после чего обучаем модель для каждой комбинации и оцениваем её на тестовых данных.Визуализация результатовВ самом Tribuo нет инструментов для визуализации, но можно экспортировать результаты и использовать, например, JFreeChart или JavaFX для построения графиков.Простой пример экспорта:import java.io.FileWriter;\nimport java.io.IOException;\nimport java.util.List;\n\npublic class PredictionExporter {\n    public static void exportPredictions(Model model, Dataset testData, String outputPath) throws IOException {\n        try (FileWriter writer = new FileWriter(outputPath)) {\n            writer.append(\"Actual,Predicted\\n\");\n            for (var example : testData) {\n                double actual = example.getOutput().getOutput();\n                double predicted = model.predict(example).getOutput();\n                writer.append(actual + \",\" + predicted + \"\\n\");\n            }\n        }\n    }\n}Подробнее с Tribuo можно ознакомиться здесь.Все актуальные методы и инструменты DS и ML можно освоить на онлайн-курсах OTUS: в каталоге можно посмотреть список всех программ, а в календаре — записаться на открытые уроки.Один из уроков пройдет 11 ноября и будет посвящен теме «Временные ряды Фурье и вейвлет-анализ». Этот урок будет особенно интересен ML-инженерам, которые начинают знакомство с временными рядами и хотят выйти за границы модели SARIMA. Подробнее",
  "«Коммуникация — ключ к успеху»": "Лонгрид получилсяНа недавнем тренинге по управлению проектами все участники сошлись в одном — коммуникация является ключевым фактором успеха любого проекта.Очень часто проекты буксуют не из-за нехватки навыков или ресурсов ( хотя и это  не редкая причина провалов), а потому, что люди не могут нормально общаться друг с другом.Команда не всегда понимает,что именно от нее хотят, а заказчик не до конца видит цели проекта, его значимость, что должно получиться в конце проекта. И здесь важно, чтобы все говорили \"на одном языке\" — использовали одни и те же понятия, термины и четко понимали, о чем идет речь.Почему коммуникация — это основа успешного проекта? 💛Команда:Когда команда не понимает задачи или цель проекта, возникает хаос. Важно не просто ставить задачи, но и объяснять, почему эта задача важна, как она влияет на общий результат.Команда должна понимать не только «что», но и «зачем». Это увеличивает вовлеченность и помогает избежать недоразумений и конфликтов.💛Проектный спонсор и заинтересованные стороны: Очень часто спонсоры проектов или другие заинтересованные стороны видят только вершину айсберга. Они могут не до конца понимать всю ценность проекта или его необходимость, если коммуникация с ними происходит на уровне «это нужно сделать, потому что так надо». Успешные проекты всегда сопровождаются ясным объяснением их ценности, где спонсор видит четкие цели и результаты. Если спонсор проекта понимает, к чему вы стремитесь, и как это повлияет на его бизнес, он будет больше поддерживать команду и проект в целом. Если нет поддержки от руководства - даже не думайте за этот проект браться!💛Единый понятийный аппарат: На тренинге все единогласно  отметили, что использование одинаковых терминов и понятий невероятно важно. Это то, что позволяет всем участникам проекта говорить на одном языке. Разные интерпретации одних и тех же понятий могут стать источником путаницы. Поэтому важно с самого начала проекта договориться о терминологии, которую все понимают одинаково.Как улучшить коммуникацию в проекте?➡️Прозрачность целей: Четкое объяснение целей проекта — это залог того, что все работают на один результат. Команда, которая понимает стратегическое значение проекта, всегда будет более мотивирована и продуктивна.➡️Доступность и открытость: Убедитесь, что члены команды и стейкхолдеры могут легко общаться друг с другом. Когда коммуникация ограничивается или закрыта, это приводит к недопониманиям и снижению эффективности. Участники проекта должны чувствовать себя свободно, задавая вопросы и предлагая свои идеи. Здесь есть свои нюансы и ограничения  ( нужно учитывать  зрелость команды , готовность стейкхолдеров общаться с линейным персоналом итп). Но если вы развернетесь хотя бы в сторону открытости - это уже даст свои плоды.➡️Регулярные встречи: Необходимо проводить регулярные встречи (стендапы, ретроспективы и другие форматы), где каждый может высказать свои идеи и озабоченности. Это помогает выровнять понимание процессов, задачи и прогресса.➡️Устранение барьеров в общении: Иногда между разными уровнями команды (например, между руководством и исполнителями) возникает барьер, из-за которого информация передается с искажениями. Лидер проекта должен создать такую среду, где каждый будет слышим и понят.➡️И, конечно, составьте план коммуникаций. И следуйте ему. Обновляйте. И продолжайте следовать. Вовремя и нужным способом  информируйте заинтересованных в проекте лиц.Коммуникация — это не просто обмен информацией. Это основа для доверия, взаимопонимания и, в конечном счете, успеха проекта. Если все участники команды, спонсоры и заинтересованные стороны не только слышат друг друга, но и понимают, что говорят, если они едины в терминах и целях, шансы на успешную реализацию проекта значительно возрастают.Так что помните: будь то стратегия, тактическое планирование или просто обсуждение задачи, коммуникация всегда стоит на первом месте. Убедитесь, что вы говорите на одном языке с командой и спонсором, и тогда у вашего проекта чуть больше шансов на успех ))😉",
  "Войти в айти за год с нуля: быстро, качественно, недорого": "Здравствуй, Хабр!Сегодня хочу поделиться своими размышлениями на тему курсов для войтишников, но немного с другой стороны. Обычно мы привыкли ругать курсы, их агрессивный маркетинг и уловки, но мало где затрагивается вопрос мотивации самих людей, идущих туда.Надеюсь, статья будет полезна тем, кто раздумывает пойти на платные разрекламированные IT-курсы, обещающие полгода-год потрогать клавиатуру и устроить вас на работу.Коротко о курсах и их производителях...Их мотивация предельно ясна: прибыль. А максимальную прибыль можно получить либо на жадности людей, либо на страхе. Но заработок через страх - это не всегда законно, поэтому маркетологам онлайн-(да и оффлайн)-школ ничего не остается, как делать крупные кричащие заголовки: БЫСТРО! КАЧЕСТВЕННО! НЕДОРОГО!ПОЛУЧИ ПРОФЕССИЮ ЗА 6 МЕСЯЦЕВ С ГАРАНТИЕЙ ТРУДОУСТРОЙСТВА!А теперь задайте себе вопрос: ничего ли вас не смущает в первой фразе? Если нет - у меня для вас плохие новости: рыночная экономика так не работает.Но во второй фразе многих людей, не знакомых с IT-сферой, как правило ничего не смущает, хотя по сути она повторяет первую...БыстроВсего полгода - и вы готовый специалист! За вас будут драться на рынке труда! Но стоп...Пошли бы вы на прием к хирургу, который закончил 6-месячные курсы? Вряд ли, хотя да, это же другое... Окей, как насчет полететь на самолете под управлением пилота, окончившего 6-месячные курсы? Может, юрист? Педагог?Стань врачом всего за 6 месяцев с гарантией трудоустройстваБольшинство рабочих профессий, таких как парикмахер, повар, автомеханик, действительно можно освоить за 6 месяцев даже прогуливая пары. Но если мы говорим про программиста или инженера - то нет, эти специальности требуют определенного набора фундаментальных знаний и навыков, которые не приобрести за 6 месяцев легкого времяпрепровождения в обнимку с клавиатурой.За себя скажу: да, мне удалось войти в айти после 9 месяцев обучения веб-разработке, при этом у меня были козыри в рукаве:У меня высшее техническое образование по специальности информационная безопасность телекоммуникационных систем;Я занимался на различных (правда бесплатных) курсах каждый божий день до поздней ночи. Да, есть люди, которым IT-сфера интересна с детства, у них крепкая база, но...зачем тогда им курсы? Все, что им нужно - это развить софт-скиллы для лучшей самопрезентации. Для всех остальных от первоначального лозунга остается только надежда на: БЫСТРО! КАЧЕСТВЕННО! НЕДОРОГО! Есть еще люди с очень сильной внутренней мотивацией, и такие действительно могут 6 месяцев денно и нощно грызть гранит компьютерных наук, но в этом случае мы должны вычеркнуть следующий пункт.НедорогоЕсли вы энтузиаст, если вы пришли в состояние \"жп горит\", если у вас жгучее желание освоить новую профессию - вы можете быстро, но точно не недорого. За все нужно платить. В случае платных курсов - деньгами. В случае внутренней мотивации - личным временем, иногда сном. Просто просидеть у монитора в течение полугода-года пару раз в неделю по часу, слушая объяснения преподавателя и делая учебные задания строго от и до, вам не даст ровным счетом ничего. Вы просто будете знать, что программисты не просто стучат по клавишам , как автор этой статьи, а еще и занимаются умственным трудом.Но давайте же разберемся с последним пунктом нашего замечательного коммерческого предложения.КачественноС гарантией трудоустройства, прошу заметить! Про гарантию, кстати, пишут здесь.На самом деле, если вы все же купились на маркетинговые предложения от платных курсов, вы должны смириться с тем, что максимум, что вы можете получить - это обзорная экскурсия в сферу IT и, возможно, понимание, чем вам интересно заниматься дальше. Это всего лишь хороший старт, и если вас \"зацепило\" - вы будете заниматься дальше, но для этого не обязательно тратить время и деньги на платные курсы, вы просто садитесь и учитесь в интернете по бесплатным ресурсам. И кстати, в этом случае вы платите временем, которое будет затрачено на поиск этих самых бесплатных ресурсов.Скажем по-другому: платные курсы могут стать только внешней мотивацией (\"уплОчено\"), но она не работает без внутренней мотивации (\"жп горит\"), а если есть внутренняя мотивация - внешняя становится необязательной.Есть такой хороший пример про туалет: если вам приспичило, то вас ничто не остановит. У вас не будет отговорок \"у меня не было времени\", \"я устал\", \"было трудно добраться\", \"некрасивая дверь\" и тд. Но представьте, что у вас есть оплаченный билет в грязный туалет, а вам не хочется - пойдете?УплОченоЕще один момент на подумать: все курсы предлагают обучение и сопровождение экспертов. Но зачем экспертам обучать именно вас, особенно если вы нулевик, который даже не умеет пользоваться компьютером? Что им даст такое сотрудничество? Не лучше ли эксперту обучать уже опытного специалиста, с которым возможен какой-никакой обмен знаниями, мнениями, который не ждет чуда, что его за ручку приведут в красивый офис с лавандовым рафом? От которого в конце концов есть отдача? Ответ в том, что весомая часть преподавателей курсов - это специалисты начального или среднего уровня, которые...будут учиться вместе с вами - да, метод Фейнмана (напишу о нем в следующих статьях). И хорошо, если преподаватель будет обладать достаточным уровнем софт-скиллз, чтобы объяснять вам материал, который сам понял только что. И да, кстати: материал устаревает. Особенно в IT. Но эта тема выходит за рамки моей публикации.ВыводПрежде чем купиться на предложение получить IT-профессию за 6 месяцев с гарантией трудоустройства, спросите себя: готовы ли вы эти 6 месяцев хреначить как конь усердно работать, постоянно напрягая мозги? жертвовать свободным временем и сном? действительно ли вы этого хотите, или вы просто хотите большую зарплату? Может, этого хотят ваши родители или супруг(а)? Есть ли бесплатные курсы в той же области и что мешает вам начать проходить их вместо платных? Если да - готовьтесь...Более честного отзыва о входе в IT я еще не виделНадеюсь, для кого-то мой опус будет полезным и сохранит пару десятков тысяч рублей денег, а еще лучше - какое-то количество времени, потраченного впустую.Ну а всем, кто дочитал, спасибо за все ваши плюсы (рад, что вы оценили) и минусы (рад, что вы не прошли мимо)!",
  "Действительно ли фотоны вечные?": "Во всей Вселенной лишь несколько частиц вечно стабильны. Фотон, квант света, имеет бесконечное время жизни. Или нет?Одна из самых стойких идей во всей Вселенной заключается в том, что всё, что существует сейчас, когда-нибудь прекратит своё существование. Звёзды, галактики и даже чёрные дыры, занимающие пространство нашей Вселенной, когда-нибудь сгорят, потускнеют и распадутся, перейдя в состояние, которое мы называем «тепловой смертью»: когда из равномерного, равновесного состояния с максимальной энтропией невозможно будет извлечь энергию никаким способом. Но, возможно, из этого общего правила есть исключения, и некоторые вещи действительно будут жить вечно.Одним из таких кандидатов в действительно стабильные сущности является фотон — квант света. Всё электромагнитное излучение, существующее во Вселенной, состоит из фотонов, а фотоны, насколько мы можем судить, имеют бесконечное время жизни. Значит ли это, что свет действительно будет жить вечно? Ответить на этот вопрос не так-то просто. Мы можем представить себе обстоятельства, при которых они действительно будут жить вечно, но мы также можем представить себе случаи, когда они распадаются, превращаются в другие частицы или даже превращаются в нечто новое или неожиданное. Это большой и интересный вопрос, который ставит нас на грань всего, что мы знаем о Вселенной. Вот лучший ответ, который есть у науки на сегодняшний день.Как впервые заметил Весто Слайфер в 1910-х годах, некоторые наблюдаемые нами объекты имеют спектральные признаки поглощения или излучения определённых атомов, ионов или молекул, но с систематическим смещением в красную или синюю часть светового спектра. В сочетании с измерениями расстояния до этих объектов эти данные привели к первоначальной идее расширяющейся Вселенной: чем дальше галактика, тем больше её свет будет казаться красным нашим глазам и приборам.Впервые вопрос о том, что фотон имеет конечное время жизни, возник по очень веской причине: мы только что обнаружили ключевое доказательство расширяющейся Вселенной. Спиральные и эллиптические туманности в небе оказались галактиками, или, как их тогда называли, «островными вселенными», далеко выходящими за пределы Млечного Пути. Эти скопления миллионов, миллиардов или даже триллионов звёзд располагались на расстоянии не менее миллионов световых лет, что позволяло отнести их далеко за пределы Млечного Пути. Более того, быстро выяснилось, что эти далёкие объекты не просто далеки, а, похоже, удаляются от нас, поскольку чем больше они удалялись, тем больше свет от них оказывался систематически смещён в сторону более красных и более красных длин волн.Конечно, к тому времени, когда эти данные стали широко доступны в 1920-1930-х годах, мы уже узнали о квантовой природе света, которая показала нам, что длина волны света определяет его энергию. Мы также хорошо знали специальную и общую теории относительности, из которых следовало, что, как только свет покидает свой источник, он может изменить свою частоту, только:из-за взаимодействия с какой-либо формой материи и/или энергии,из-за движения наблюдателя либо к нему, либо от него,из-за изменения свойств кривизны самого пространства, например, в результате гравитационного красного/голубого смещения или расширения/сжатия Вселенной.Первое потенциальное объяснение, в частности, привело к формулировке увлекательной альтернативной космологии: космологии усталого света. За достаточно долгое время свет, испущенный далёким объектом, дойдёт до наших глаз даже в расширяющейся Вселенной. Однако если скорость рецессии далёкой галактики достигает и остаётся выше скорости света, мы никогда не сможем достичь её, даже если сможем поймать свет из её далёкого прошлого.Впервые сформулированная в 1929 году Фрицем Цвикки — да, тем самым Фрицем Цвикки, который придумал термин «сверхновая», впервые сформулировал гипотезу тёмной материи и однажды пытался «успокоить» турбулентный атмосферный воздух, стреляя из винтовки в трубу телескопа, — гипотеза усталого света выдвинула идею о том, что распространяющийся свет теряет энергию в результате столкновений с другими частицами, присутствующими в пространстве между галактиками. Чем больше пространство, через которое свет распространяется, тем больше энергии теряется в результате этих взаимодействий, и именно это, а не особые скорости или космическое расширение, объясняет, почему свет кажется более сильно красным для более удалённых объектов.Однако для того, чтобы этот сценарий выполнялся, должны быть верны два предсказания.Когда свет проходит через среду, даже разреженную, он замедляется от скорости света в вакууме до скорости света в этой среде. Это замедление влияет на свет разных частот в разной степени. Подобно тому как свет, проходящий через призму, расщепляется на разные цвета, свет, проходящий через межгалактическую среду, которая взаимодействовала с ним, должен замедлять свет разных длин волн на разное количество. Когда этот свет снова попадёт в настоящий вакуум, он снова будет двигаться со скоростью света в вакууме. В вакууме космоса весь свет, независимо от длины волны или энергии, движется с одинаковой скоростью: скоростью света в вакууме. Когда мы наблюдаем свет от далёкой звезды, мы наблюдаем свет, который уже прошёл путь от источника до наблюдателя.И всё же, наблюдая за светом, исходящим от источников на разных расстояниях, мы не обнаружили зависимости длины волны от величины красного смещения, которое демонстрирует свет. Напротив, на всех расстояниях все длины волн излучаемого света смещаются точно на такой же коэффициент, как и все остальные; никакой зависимости от длины волны для красного смещения не существует. Из-за этого нулевого наблюдения первое предсказание космологии усталого света опровергнуто.Но есть и второе предсказание, с которым тоже нужно считаться.Если более удалённый свет теряет больше энергии, проходя большее расстояние по «среде с потерями», чем менее удалённый свет, то более удалённые объекты должны казаться размытыми на всё большую и большую величину, чем менее удалённые. И снова, когда мы проверяем это предсказание, мы обнаруживаем, что оно совершенно не подтверждается наблюдениями. Более удалённые галактики при наблюдении рядом с менее удалёнными галактиками выглядят такими же чёткими и с высоким разрешением, как и менее удалённые. Это справедливо, например, для всех пяти галактик в квинтете Стефана, а также для фоновых галактик, видимых за всеми пятью членами квинтета. Это предсказание также оказалось опровергнутым. Основные галактики квинтета Стефана, открытые «Уэббом» 12 июля 2022 года. Расстояние до галактики слева составляет всего ~15 % от расстояния до остальных галактик, а фоновые галактики находятся во много десятков раз дальше. Но изображение их всех одинаково чёткое, что демонстрирует, что Вселенная полна звёзд и галактик практически везде, куда бы мы ни посмотрели.\nХотя эти наблюдения достаточно хороши, чтобы опровергнуть гипотезу усталого света — и, по сути, они были достаточно хороши, чтобы опровергнуть её сразу же, как только она была предложена, — это лишь один из возможных вариантов того, как свет может быть нестабильным. Свет может либо угаснуть, либо превратиться в какую-то другую частицу, и есть несколько интересных способов рассмотреть эти возможности.Первый вытекает просто из того факта, что у нас есть космологическое красное смещение. Каждый порождённый фотон, независимо от того, как он был порождён — термически, в результате квантового перехода или любого другого взаимодействия, — будет распространяться по Вселенной, пока не столкнётся и не вступит во взаимодействие с другим квантом энергии. Но если речь идёт о фотоне, испущенном в результате квантового перехода, то, если он не сможет довольно быстро вступить в обратную квантовую реакцию, он начнёт путешествовать по межгалактическому пространству, причём его длина волны будет растягиваться из-за расширения Вселенной по мере путешествия. Если ему не повезёт, и его не поглотит какой-нибудь атом в квантовом связанном состоянии с подходящей допустимой частотой перехода, он будет всё дальше и дальше уходить в красную часть спектра, пока не перейдёт рубеж максимальной длины волны, после которой его уже нельзя будет поглотить. Этот синтез трёх различных наборов спектральных линий от лампы на ртутных парах показывает, какое влияние может оказывать магнитное поле. В (А) магнитное поле отсутствует. В (B) и (C) магнитное поле есть, но оно ориентировано по-разному, что объясняет разное расщепление спектральных линий. Многие атомы демонстрируют такую тонкую или даже гипертонкую структуру без приложения внешнего поля, и эти переходы очень важны, когда речь идёт о создании функциональных атомных часов. Многие переходы, такие как показанные здесь, являются дискретными, а не непрерывными процессами.Однако есть и второй набор возможностей, который существует для всех фотонов: они могут провзаимодействовать со свободной квантовой частицей, вызвав один из множества эффектов.Это может быть, например, рассеяние, когда заряженная частица — обычно электрон — поглощает, а затем вновь излучает фотон. При этом происходит обмен энергией и импульсом, и либо заряженная частица, либо фотон могут достичь более высокого уровня энергии за счёт того, что у другой частицы останется меньше этой энергии.При достаточно высоких энергиях столкновение фотона с другой частицей — даже с другим фотоном, если энергия достаточно высока, — может спонтанно привести к образованию пары частица-античастица, если имеется достаточно энергии для того, чтобы они обе прошли через эйнштейновское E = mc². На самом деле, космические лучи с самой высокой энергией могут делать это даже с фотонами с удивительно низкой энергией, которые являются частью реликтового излучения (РИ) — остаточного свечения Большого взрыва. Для космических лучей с энергией выше ~1017 эВ один типичный фотон РИ имеет шанс произвести электрон-позитронные пары. При ещё более высоких энергиях, скорее ~1020 эВ, фотон РИ имеет значительно большие шансы превратиться в нейтральный пион, что довольно быстро лишает космические лучи энергии. Это основная причина, по которой наблюдается резкий спад в популяции самых высокоэнергетичных космических лучей: они находятся выше этого критического энергетического порога. Энергетический спектр космических лучей самых высоких энергий в разбивке по коллаборациям, которые их обнаружили. Все результаты невероятно хорошо согласуются от эксперимента к эксперименту и показывают значительный спад на пороге ГЗК ~5 x 10^19 эВ. Тем не менее, многие подобные космические лучи превышают этот энергетический порог, что указывает на недостаток в самом упрощённом представлении об этих космических лучах.Другими словами, даже очень низкоэнергетические фотоны могут быть преобразованы в другие частицы при столкновении с другой частицей с достаточно высокой энергией.Есть ещё третий способ изменения фотона, помимо космического расширения или превращения в частицы с ненулевой массой покоя: рассеяние от частицы, в результате которого образуются дополнительные фотоны. Практически в каждом электромагнитном взаимодействии, или взаимодействии между заряженной частицей и хотя бы одним фотоном, существуют так называемые «радиационные поправки», которые возникают в квантовых теориях поля. Для каждого стандартного взаимодействия, в котором в начале и в конце существует одинаковое количество фотонов, существует чуть менее 1% шансов — точнее, 1/137, — что в конце вы излучите дополнительный фотон сверх того количества, с которым вы начали.И каждый раз, когда у вас есть энергичная частица, обладающая положительной массой покоя и положительной температурой, эти частицы также будут излучать фотоны, теряя энергию в виде фотонов.Фотоны очень, очень легко создать, и хотя их можно поглотить, вызвав соответствующие квантовые переходы, большинство возбуждений через определённое время прекращают своё существование. Как и в старой поговорке «Что поднимается, то и опускается», квантовые системы, которые возбуждаются до более высоких энергий за счёт поглощения фотонов, в конце концов тоже распадаются, производя по крайней мере то же количество фотонов, как правило, с той же чистой энергией, что и поглощённые вначале. Когда образуется атом водорода, спины электрона и протона с равной вероятностью могут быть выровнены и анти-выровнены. Если они анти-выровнены, то никаких дальнейших переходов не произойдёт, а если выровнены, то они могут квантово туннелировать в состояние с более низкой энергией, испуская фотон с очень специфической длиной волны (21 см) в очень специфическом и довольно длительном временном интервале. Этот переход был измерен с точностью более 1 части на триллион и не менялся на протяжении многих десятилетий, в течение которых он был известен. Это первый свет, появившийся во Вселенной после образования нейтральных атомов и ещё до образования первых звёзд. Появлялся он и после: при формировании новых звёзд ультрафиолетовое излучение ионизирует атомы водорода, создавая эту сигнатуру снова, когда эти атомы спонтанно переформируются.Учитывая, что существует так много способов создания фотонов, вы, вероятно, слюной исходите от желания найти способ их уничтожить. В конце концов, простое ожидание, пока эффекты космического красного смещения приведут их к асимптотически низкому значению энергии и плотности, займёт произвольно много времени. Каждый раз, когда Вселенная увеличивается в 2 раза, общая плотность энергии в виде фотонов падает в 16 раз: в 24 раза. Множитель 8 появляется потому, что количество фотонов — несмотря на все способы их создания — остаётся относительно фиксированным, а удвоение расстояния между объектами увеличивает объём наблюдаемой Вселенной в 8 раз: в два раза в длину, в два раза в ширину и в два раза в глубину.Четвёртый и последний фактор, равный двум, возникает в результате космологического расширения, которое растягивает длину волны в два раза по сравнению с первоначальной длиной волны, тем самым вдвое уменьшая энергию на один фотон. На достаточно больших временных масштабах это приведёт к тому, что плотность энергии Вселенной в виде фотонов будет асимптотически стремиться к нулю, но никогда не достигнет его.В то время как материя и излучение становятся менее плотными по мере расширения Вселенной из-за увеличения её объёма, тёмная энергия — это форма энергии, присущая самому пространству. По мере того как в расширяющейся Вселенной образуется новое пространство, плотность тёмной энергии остаётся постоянной.Можно попытаться поумничать и представить себе какую-нибудь экзотическую частицу сверхмалой массы, которая соединяется с фотонами и в которую фотон может превратиться при соответствующих условиях. Какой-нибудь бозон или псевдоскалярная частица — например, аксион или аксино, конденсат нейтрино или какая-нибудь экзотическая куперовская пара — могли бы привести именно к таким явлениям, но, опять же, это работает только в том случае, если фотон имеет достаточно высокую энергию, чтобы превратиться в частицу с ненулевой массой покоя через E = mc². Как только энергия фотона смещается ниже критического порога, это уже не работает.Аналогичным образом можно представить себе конечный способ поглощения фотонов: столкновение их с чёрной дырой. Как только что-либо переходит извне горизонта событий внутрь, оно не только никогда не сможет убежать, но и всегда будет увеличивать энергию массы покоя самой чёрной дыры. Да, со временем во Вселенной появится множество чёрных дыр, и они будут увеличиваться в массе и размерах с течением времени.Но даже это произойдёт лишь до определённого момента. Как только плотность Вселенной упадёт ниже определённого порога, чёрные дыры начнут распадаться под действием излучения Хокинга быстрее, чем расти, а это означает производство ещё большего количества фотонов, чем попало в чёрную дыру изначально. В течение следующих ~10100 лет или около того все чёрные дыры во Вселенной в конце концов полностью распадутся, причём подавляющее большинство продуктов распада будут фотонами. Моделируемый распад чёрной дыры приводит не только к испусканию излучения, но и к распаду центральной орбитальной массы, которая поддерживает стабильность большинства объектов. Чёрные дыры не являются статичными объектами, а скорее изменяются с течением времени. Однако чёрные дыры, сформированные из различных материалов, должны иметь различную информацию, закодированную на их горизонтах событий, и неясно, закодирована ли эта информация в исходящем излучении Хокинга.Так погибнут ли они когда-нибудь? Согласно современным законам физики — нет. На самом деле, ситуация даже более плачевна, чем вы, вероятно, думаете. Вы можете вспомнить каждый фотон, который был или будет создан:создан во время Большого взрыва,создан в результате квантовых переходов,создан в результате радиационных поправок,создан в результате излучения энергии,или созданный в результате распада чёрной дыры,и даже если вы подождёте, пока все эти фотоны достигнут произвольно низких энергий из-за расширения Вселенной, Вселенная всё равно не будет лишена фотонов.Почему?Потому что во Вселенной всё ещё есть тёмная энергия. Как объект с горизонтом событий, например чёрная дыра, будет непрерывно излучать фотоны из-за разницы в ускорении вблизи и вдали от горизонта событий, так и объект с космологическим (или, более точно, риндлеровским) горизонтом. Принцип эквивалентности Эйнштейна говорит нам, что наблюдатели не могут отличить гравитационное ускорение от ускорения, вызванного любой другой причиной, и любые два несвязанных места будут казаться ускоренными друг относительно друга из-за присутствия тёмной энергии. Физика, которая при этом возникает, идентична: испускается непрерывное количество теплового излучения. Исходя из значения космологической постоянной, которое мы предполагаем сегодня, это означает, что спектр излучения чёрного тела с температурой ~10-30 К всегда будет пронизывать всё пространство, независимо от того, как далеко в будущее мы зайдём. Вся падающая снаружи в чёрную дыру материя излучает свет и всегда видна, в то время как из-за горизонта событий ничто не может выйти наружу. Но если бы вы сами упали в чёрную дыру, ваша энергия могла бы, по идее, вновь вырваться наружу в виде части горячего Большого взрыва в новорождённой Вселенной.Даже в самом конце своего существования, как бы далеко в будущее мы ни заходили, Вселенная всегда будет продолжать производить излучение, гарантируя, что она никогда не достигнет абсолютного нуля, что она всегда будет содержать фотоны, и что даже при самых низких энергиях, которых она когда-либо достигнет, фотону больше не во что будет распадаться или переходить. Хотя плотность энергии во Вселенной будет продолжать падать по мере её расширения, а энергия, присущая каждому отдельному фотону, будет продолжать падать по мере того, как время будет устремляться всё дальше и дальше в будущее, никогда не будет ничего «более фундаментального», во что бы они переходили.Конечно, есть экзотические сценарии, которые мы можем придумать, чтобы изменить историю. Возможно, у фотонов действительно есть ненулевая масса покоя, из-за чего они замедляются до скорости меньше скорости света, когда проходит достаточно времени. Возможно, фотоны действительно нестабильны по своей природе, и есть что-то другое, действительно безмассовое, например, комбинация гравитонов, в которую они могут распадаться. И, возможно, существует некий фазовый переход, который произойдёт далеко в будущем, где фотон обнаружит свою истинную нестабильность и распадётся в ещё неизвестное квантовое состояние.Но если всё, что у нас есть, — это фотон, как мы понимаем его в Стандартной модели, то фотон по-настоящему стабилен. Вселенная, наполненная тёмной энергией, гарантирует, что даже если существующие сегодня фотоны будут смещаться до произвольно низких энергий, всегда будут создаваться новые, что приведёт к тому, что во Вселенной число фотонов и плотность энергии фотонов всегда будут конечными и положительными. Мы можем быть уверены в правилах только в той степени, в которой мы их измерили, но если только не существует какого-то большого фрагмента головоломки, который мы просто ещё не раскрыли, мы можем рассчитывать на то, что фотоны могут затухать, но никогда не умрут по-настоящему.",
  "Как мы уменьшили размер нашего Javascript монорепозитория в Git на 94%": "Это не кликбейт. Мы и правда сделали это! В Microsoft мы работаем с очень большим монорепозиторием, который между собой называем 1JS. Недавно мы достигли 1000 активных пользователей в месяц, около 2500 пакетов и ~20 млн строк кода! Последнее клонирование репозитория вернуло невероятные 178 ГБ. По множеству причин, это попросту слишком большой размер, некоторые ребята из Европы попросту не могут успешно клонировать репозиторий из‑за его размера.Вопрос в том, как это вообще произошло?!Урок #1  Когда я впервые присоединился к репозиторию несколько лет назад, я заметил, что он растет. Когда я впервые его склонировал, его размер составлял около одного‑двух гигабайт, но через несколько месяцев он уже достигал около 4 гигабайт. Трудно было понять, почему именно это происходит. Тогда я использовал инструмент git-sizer, который поведал мне несколько деталей о некоторых больших блобах. Они возникают, когда кто‑то случайно добавляет бинарные файлы и мало, что можно сделать в таком случае, кроме ограничения размеров файлов — фичи Azure DevOps. В целом, после того, как файл попал в репозиторий, он в каком‑то смысле «застревает» в истории.Также было показано предупреждение о неудаленных файлах изменений Beachball. Мы используем их как Changesets, достигая того же результата, как и при использовании semantic‑release, где мы говорим пакетам, как автоматически увеличивать их диапазоны версий в соответствии с semver.В какие‑то моменты у нас было больше 40 тысяч таких файлов в одной папке, что приводило к созданию огромных tree‑объектов при каждом добавлении новых файлов в этой папке.Итак, урок номер один, который мы извлекли, был... Не храни тысячи файлов в одной папке Чтобы облегчить ситуацию, мы сделали две вещи. Первая — pull request в beachball для вноса нескольких изменений в один файл вместо создания отдельного файла на каждый пакет.Вторая — мы написали пайплайн, который периодически выполняется и при запуске автоматически чистит папку изменений для избежания ее разрастания.Ура! Мы пофиксили раздувание git!Урок #2  Наш флоу версионирования поддерживает зеркало main под названием versioned, хранящее актуальные версии пакетов, чтобы избежать конфликтов git в main и иметь возможность точно видеть, какие коммиты относятся к какой semver версии, выпускаемой посредством пакетов NPM. (Это потребует отдельного поста, ну да ладно…)Я заметил, что версионированная ветка становилась все сложнее и сложнее для клонирования из‑за ее размера. Но мы уже разобрались с проблемой файлов изменений и все, что происходило в ветке versioned с точки зрения коммитов это добавление к файлам CHANGELOG.md и CHANGELOG.json.Время шло, а репозиторий, хоть и понемногу, но разрастался. Но было сложно определить, был ли связан рост просто с масштабом или дело в чем‑то совершенно другом. Мы добавляли сотни тысяч строк кода и сотни разработчиков с 2021 года, так что можно было привести аргумент, что это просто естественный рост. Однако, когда мы поняли, что превзошли темпы роста одного из крупнейших монорепозиториев Microsoft — Office, мы осознали, что что‑то здесь явно не так!Пришло время звать на подмогу…Автор таких фич git как git shallow checkout, git sparse index и всяких других только вернулся в нашу организацию, поработав в Github и подарив эти фичи всему миру.Он глянул и сразу понял, что что‑то не так с подобным темпом роста. Когда мы запуллили версионированные ветки, те самые, в которых меняется только CHANGELOG.md и CHANGELOG.json, мы получили 125ГБ дополнительных данных git?! НО КАК??Итак, после очень глубокого погружения в git, оказалось, что некий старый код упаковки, добавленный Linux Torvalds (может, слышали про такого 🤷‍♂️) на самом деле проверял только последние 16 символов названия файла при подготовке к сжатию перед отправкой diff'ов. Для контекста, обычно git отправляет только diff'ы измененных файлов, но из‑за проблемы с упаковкой, git сравнивал файлы CHANGELOG.md из двух разных пакетов! Stolee хорошо объяснил это тут.К примеру, если вы изменили repo/packages/foo/CHANGELOG.json, когда git готовился пушить изменения, он генерировал diff относительно repo/packages/bar/CHANGELOG.json! Это значит, что во многих случаях раз за разом отправлялся весь файл целиком, что могло иногда составлять десятки мегабайт, и вы догадываетесь, как в репозитории нашего размера это может стать проблемой.Затем мы попробовали перепаковать наш репозиторий с увеличенным окном командой git repack -adf --window=250, чтобы Git смог лучше сжать пак‑файлы и уменьшить размер репозитория. Это действительно значительно снизило размер репозитория, но мы можем лучше!Этот PR https://github.com/git‑for‑windows/git/pull/5171?ref=jonathancreamer.com добавил новый способ упаковки репозитория на основе обхода путей git вместо стандартного обхода коммитов.Результаты впечатляют…Вчера я запустил новый git clone на своей машине, чтобы проверить новую версию git в форке git Microsoft (версия git version 2.47.0.vfs.0.2)… И после выполнения новой команды git repack -adf --path-walk … Невероятно. С 178ГБ до 5. 😱 Ещё одна новая опция конфигурации, которая будет добавлена, обеспечит генерацию нужных типов дельт во время выполнения команды git push...git config --global pack.usePathWalk trueЭто позволит убедиться, что команда git push использует правильное сжатие.Любой разработчик на версии git 2.47.0.vfs.0.2 теперь может переупаковать склонированный локально репозиторий, а также использовать новый алгоритм обхода путей при git push, чтобы остановить рост репозитория.На Github переупаковка и сборка мусора git происходит периодически, но, опять же, используемый Github тип упаковки не будет корректно рассчитывать дельты файлов CHANGELOG.md и CHANGELOG.json или потенциально любых часто изменяющихся файлов с одинаковыми 16-ю последними символами в названии. К примеру, большие файлы со строками i18n и им подобные.В Azure DevOps, который мы используем, пока вообще нет такой переупаковки. Поэтому мы также работаем над этим, чтобы уменьшить размер репозитория на серверной стороне.Эти изменения также будут добавлены в апстрим git! Ура OSS.Итоги  Если вы работаете в околобольшом монорепозитории и в нем есть файлы CHANGELOG.md или, на самом деле, любой файл с относительно длинным названием (\u003e16 символов), который часто обновляется, вам может быть полезно последить за этой штукой с обходом путей.Также можете попробовать новую команду git survey, чтобы просмотреть разную эвристику, к примеру Top Files By Disk Size, Top Directories By Inflated Size, или Top Files By Inflated Size.Это поможет вам понять, повлияет ли обход путей на размер вашего репозитория или нет.В целом я очень впечатлен и воодушевлен нашей приверженностью созданию решений, которые помогают нам масштабировать репозитории в Microsoft, а также возможностью передавать эти решения всему миру.",
  "Краткий гайд для самых маленьких по Tensor Flow": "TensorFlow — один из самых мощных и популярных фреймворков для машинного обучения, разработанный компанией Google Brain в 2015 году. Изначально фреймворк создавали как платформу для внутреннего использования в Google, заменив предшествующую библиотеку DistBelief, которая была ограничена возможностями только для небольших исследований. TensorFlow, в отличие от DistBelief, задумывался как кросс-платформенное решение с возможностью гибкой и масштабируемой настройки. Первая версия TensorFlow использовала концепцию графа вычислений и статической компиляции, что хотя и требовало большего количества ресурсов для разработки, позволяло проводить распределённое обучение и значительно ускоряло процесс выполнения.С выпуском TensorFlow 2.x фреймворк претерпел значительные изменения, ориентированные на упрощение разработки за счёт поддержки динамических графов (Eager execution) и интеграции Keras как стандартного API высокого уровня. Эти улучшения сделали TensorFlow более доступным для разработчиков, сосредоточенных на быстрой итерации и разработке гибких моделей. Короче говоря, превратили фреймворк в удобный инструмент для продакшена. TensorFlow поддерживает полный цикл разработки и применения моделей машинного обучения — от подготовки данных и обучения до развертывания и оптимизации. TensorFlow поддерживает интеграцию с XLA (Accelerated Linear Algebra), что позволяет ускорить вычисления на CPU и GPU. Плюсом всегда можно подключить TPU (ускорители с тензорными процессорами). Если еще говорить о скорости, то TensorFlow позволяет использовать стратегию распределённого обучения. Фреймворк предоставляет несколько решений для развертывания моделей, включая TensorFlow Serving для облака, TensorFlow Lite для мобильных устройств и IoT, а также TensorFlow.js для работы в браузере.Эта статья — небольшой гайд по основным возможностям Tensor Flow с примерами и краткими инструкциями. В основе TensorFlow лежат ключевые концепции, которые делают его универсальным и гибким инструментом для машинного обучения и глубокого обучения: вычислительные графы, сессии и операции. На уровне низкоуровневого API TensorFlow работает с вычислительным графом — направленным ациклическим графом (DAG), представляющим потоки данных и управления.В TensorFlow 2.x добавлен режим Eager Execution, который позволяет работать с графами динамически, делая разработку моделей интуитивнее, а код — более читаемым. Эти изменения, наряду с интеграцией API Keras, упростили использование TensorFlow, сократив количество абстракций, необходимых для создания моделей.Архитектура TensorFlow: устройство и базовые концептыВычислительный граф TensorFlow представляет собой структуру, в которой узлы — это операции (определённые функции или вычисления), а рёбра — данные (тензоры), передаваемые между узлами. Такой подход позволяет TensorFlow разбивать задачи на подзадачи и выполнять их параллельно на разных устройствах, что существенно увеличивает скорость и производительность, особенно при работе с GPU и TPU. Граф направленный и ациклический, что означает отсутствие циклов, а данные перемещаются в одном направлении — от входов к выходам. Основные элементы вычислительного графа:Операции (Operations): базовые блоки, такие как матричные умножения или свёртки.Тензоры (Tensors): многомерные массивы данных, представляющие входные и выходные значения для операций.Переменные (Variables): тензоры, чьи значения могут изменяться в процессе обучения (например, веса нейронной сети).Dataflow (поток данных) управляет передачей значений между узлами графа. Например, значения передаются от одного слоя нейронной сети к следующему в виде тензоров. Control flow (поток управления) включает конструкции, такие как условные операторы и циклы, обеспечивая гибкость и адаптивность графа, что позволяет строить более сложные структуры, такие как рекуррентные нейронные сети. Раньше работа с вычислительными графами в TensorFlow требовала запуска сессий для выполнения операций. Однако в TensorFlow 2.x эту необходимость устранили, что позволило сосредоточиться на логике модели и упростило взаимодействие с данными.TensorFlow 2.x и Eager ExecutionС введением TensorFlow 2.x фреймворк поддерживает режим Eager Execution по умолчанию, что позволяет исполнять операции сразу, а не откладывать их до построения всего графа. Это значит, что код становится проще для чтения и отладки. Eager Execution делает TensorFlow более похожим на стандартный Python, так как все операции выполняются в режиме реального времени, что упрощает написание и тестирование кода.Приведем пример, где создадим и выполним граф с использованием как Eager Execution, так и классов переменных и операций для выполнения основных вычислений:import tensorflow as tf# Включение режима Eager Execution (в TensorFlow 2 он включен по умолчанию)print(\"Eager execution:\", tf.executing_eagerly())# Создадим простые тензоры и выполним базовую операциюa = tf.constant(3.0)b = tf.constant(4.0)c = tf.add(a, b)  # Складываем два тензораprint(f\"Результат сложения: {c.numpy()}\")  # Выводим результат операции# Создание переменной и выполнение нескольких операцийx = tf.Variable(5.0)with tf.GradientTape() as tape:  # Отслеживаем градиенты    y = x * x + b  # Операция, которая выполняется немедленно благодаря Eager Executiondy_dx = tape.gradient(y, x)  # Рассчёт градиента функции y по xprint(f\"Градиент функции y по x: {dy_dx.numpy()}\")# Пример создания пользовательской функцииdef my_model(x):    return 2 * x ** 2 + 3 * x + 1# Вычисляем градиенты для пользовательской функцииwith tf.GradientTape() as tape:    tape.watch(x)    y = my_model(x)grad = tape.gradient(y, x)print(f\"Градиент пользовательской функции по x: {grad.numpy()}\")При Eager Execution тензоры a и b объединяются в сумму, которая вычисляется немедленно. Это упрощает процесс отладки и позволяет увидеть результаты на каждом этапе.Использование GradientTape: контекстный менеджер GradientTape помогает автоматически отслеживать операции с тензором x, что полезно для вычисления градиентов. В конце приведён пример определения функции my_model и использования GradientTape для нахождения производной по x.Переход к API KerasС переходом на TensorFlow 2.x для создания моделей нейронных сетей разработчики получили возможность использовать стандартный API Keras, который интегрирован в TensorFlow. Этот API позволяет упростить определение слоев, функций активации и компиляцию моделей, избегая низкоуровневого кода. Вот пример использования Keras для создания последовательной модели:from tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Dense# Создаем простую модель с Keras APImodel = Sequential([    Dense(32, activation='relu', input_shape=(10,)),  # Первый слой с 32 нейронами и активацией ReLU    Dense(1)  # Выходной слой])# Компилируем модельmodel.compile(optimizer='adam', loss='mean_squared_error')# Входные данные и их предсказаниеimport numpy as npdata = np.random.rand(10).reshape(1, -1)  # Пример входного вектора размером (1, 10)prediction = model(data)  # Eager Execution позволяет немедленно получить предсказаниеprint(f\"Предсказание модели: {prediction.numpy()}\")TensorFlow использует тензоры в качестве основной структуры данных. Эти многомерные массивы представляют собой обобщение скалярных значений, векторов и матриц. Понимание типов данных и форматов тензоров – это просто база. Типы данных и форматы тензоров в TensorFlowTensorFlow поддерживает несколько типов данных для тензоров, что позволяет пользователям выбирать наиболее подходящие для своих задач. Наиболее распространённые типы данных включают:float32: широко используется для хранения значений с плавающей запятой. Этот тип обеспечивает хороший баланс между точностью и объемом памяти.float64: используется для более точных вычислений, хотя требует больше памяти.int32 и int64: целочисленные типы, подходящие для работы с индексами и счётчиками.bool: логический тип, используемый для работы с булевыми значениями.Тензоры могут быть одномерными (векторы), двумерными (матрицы) или многомерными, в зависимости от количества измерений, которые они содержат. Например, цветное изображение может быть представлено тензором размерности 3 (ширина, высота, цветовые каналы).Определение и создание тензоровСоздание тензоров в TensorFlow осуществляется с помощью нескольких удобных функций, таких как tf.constant(), tf.zeros(), tf.ones(), tf.random.uniform() и tf.random.normal(). Приведём примеры:import tensorflow as tf# Создание тензоровscalar = tf.constant(3.14)  # Скалярный тензорvector = tf.constant([1, 2, 3])  # Векторmatrix = tf.constant([[1, 2, 3], [4, 5, 6]])  # Матрицаtensor_3d = tf.random.uniform(shape=(2, 3, 4), minval=0, maxval=1)  # 3D тензорprint(\"Скаляр:\", scalar.numpy())print(\"Вектор:\", vector.numpy())print(\"Матрица:\\n\", matrix.numpy())print(\"3D тензор:\\n\", tensor_3d.numpy())Трансформация тензоровTensorFlow предоставляет множество операций для трансформации тензоров, включая изменение формы, объединение и разделение тензоров. Рассмотрим несколько операций:# Изменение формы тензораreshaped_tensor = tf.reshape(tensor_3d, (6, 4))  # Изменяем размерностьprint(\"Изменённый тензор:\\n\", reshaped_tensor.numpy())# Объединение тензоровtensor_a = tf.constant([[1, 2], [3, 4]])tensor_b = tf.constant([[5, 6]])concatenated_tensor = tf.concat([tensor_a, tensor_b], axis=0)  # Объединение по первому измерениюprint(\"Объединённый тензор:\\n\", concatenated_tensor.numpy())# Разделение тензораsplits = tf.split(concatenated_tensor, num_or_size_splits=2)  # Разделяем на 2 частиprint(\"Разделённые тензоры:\\n\", [s.numpy() for s in splits])Основные операции над тензорамиTensorFlow предоставляет множество операций для работы с тензорами, включая арифметические операции, свёртки, операции активации и многое другое. Например:# Арифметические операцииa = tf.constant([1, 2, 3])b = tf.constant([4, 5, 6])addition = tf.add(a, b)  # Сложениеmultiplication = tf.multiply(a, b)  # Умножениеprint(\"Сумма:\", addition.numpy())print(\"Произведение:\", multiplication.numpy())# Операции активацииx = tf.constant([-1.0, 0.0, 1.0])relu = tf.nn.relu(x)  # Функция активации ReLUsoftmax = tf.nn.softmax(x)  # Функция активации softmaxprint(\"ReLU:\", relu.numpy())print(\"Softmax:\", softmax.numpy())Управление вычислениями с использованием tf.GradientTape()tf.GradientTape() позволяет пользователям отслеживать операции над тензорами для вычисления градиентов. Это очень полезно для оптимизации, особенно в процессе обучения моделей. Вот пример:# Пример использования tf.GradientTape для отслеживания градиентов# Определяем простую функцию y = x^2def simple_function(x):    return x ** 2# Создаем переменнуюx = tf.Variable(3.0)# Используем GradientTape для отслеживания градиентовwith tf.GradientTape() as tape:    y = simple_function(x)  # Вычисляем y# Вычисляем градиент функции y по xdy_dx = tape.gradient(y, x)print(f\"Значение функции: {y.numpy()}, Градиент: {dy_dx.numpy()}\")# Теперь изменим значение x и повторимx.assign(5.0)with tf.GradientTape() as tape:    y = simple_function(x)dy_dx_new = tape.gradient(y, x)print(f\"Значение функции: {y.numpy()}, Новый градиент: {dy_dx_new.numpy()}\")Оптимизация и обучение моделейTensorFlow предлагает богатый набор встроенных оптимизаторов, которые управляют процессом настройки весов модели с целью минимизации функции потерь. Короче говоря, сейчас мы говорим про обучение моделей. Правильный выбор оптимизатора и функции потерь — основа успешного обучения моделей, и TensorFlow поддерживает такие популярные оптимизаторы, как стохастический градиентный спуск (SGD), Adam и RMSprop. Каждый из них имеет свои преимущества для различных типов данных и задач. Например, стохастический градиентный спуск, являясь базовым методом, обновляет параметры, перемещаясь по направлению градиента функции потерь. Несмотря на свою простоту, SGD может замедлять процесс обучения при резких изменениях градиента и трудностях с локальными минимумами.Пример использования стохастического градиентного спуска с небольшим шагом обучения:import tensorflow as tf# Создание простой линейной моделиmodel = tf.keras.models.Sequential([    tf.keras.layers.Dense(1, input_shape=(1,))])# Использование оптимизатора SGDoptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)# Компиляция модели с функцией потерь и метрикойmodel.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])Adam (адаптивный момент градиента) устраняет многие проблемы SGD, регулируя скорость обучения для каждого параметра. Он использует экспоненциально взвешенные средние значения первого и второго моментов градиента, что стабилизирует процесс оптимизации. Adam подходит для более сложных задач, поскольку его адаптивный шаг обучения ускоряет процесс сходимости. Пример настройки Adam выглядит следующим образом:# Заменяем оптимизатор на Adamoptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])RMSprop, в отличие от Adam, более эффективен при нестабильных градиентах. Он корректирует скорость обучения путем добавления коэффициента сглаживания, что помогает при обработке данных с сильной разреженностью или с резко изменяющимися значениями. Такой подход делает RMSprop полезным для задач, где градиенты могут быстро изменяться или быть разреженными:# Заменяем оптимизатор на RMSpropoptimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])Настройка функций потерь и метрик позволяет отслеживать качество обучения и направлять оптимизатор в сторону минимизации потерь. Для задач регрессии популярна функция `mean_squared_error`, которая измеряет среднюю квадратную ошибку предсказаний. Для классификационных задач, например, в задачах бинарной классификации, часто выбирают функцию `binary_crossentropy`. Она измеряет различия между истинными и предсказанными вероятностями:# Использование функции потерь binary_crossentropy для бинарной классификацииmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])Для кастомных задач часто требуются пользовательские функции потерь и оптимизаторы. TensorFlow позволяет определять собственные функции потерь. Например, можно создать функцию абсолютной ошибки для отслеживания более значительных отклонений между предсказанными и истинными значениями, избегая сглаживания больших ошибок, как это делает `mean_squared_error`:# Определение пользовательской функции потерьdef custom_loss(y_true, y_pred):    return tf.reduce_mean(tf.abs(y_true - y_pred))# Компиляция модели с пользовательской функцией потерьmodel.compile(optimizer=optimizer, loss=custom_loss, metrics=['mae'])Также возможна реализация пользовательских оптимизаторов, наследующих базовый класс `tf.keras.optimizers.Optimizer`. Пользовательский оптимизатор можно адаптировать для специфических задач, например, с собственным подходом к обновлению весов или специфической обработкой градиентов. Подобные решения повышают гибкость и дают возможность создавать уникальные методы обучения, соответствующие особенностям задачи. В примере ниже представлен простой оптимизатор с использованием специального коэффициента ускорения:# Создание пользовательского оптимизатораclass CustomOptimizer(tf.keras.optimizers.Optimizer):    def __init__(self, learning_rate=0.01, name=\"CustomOptimizer\", **kwargs):        super().__init__(name, **kwargs)        self.learning_rate = learning_rate    def _resource_apply_dense(self, grad, var, apply_state=None):        var.assign_sub(self.learning_rate * grad)# Использование пользовательского оптимизатораoptimizer = CustomOptimizer(learning_rate=0.01)model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])Кастомизация функций потерь и оптимизаторов в TensorFlow позволяет тонко настраивать процесс обучения под конкретные условия задачи и тип данных. В этом большая часть фреймворка. Модельные API TensorFlow: Keras и Subclassing APITensorFlow предлагает два основных подхода для создания моделей через высокоуровневое API Keras: это Sequential API и функциональное API. Эти подходы ориентированы на простую и быструю разработку моделей, что позволяет гибко настраивать архитектуру нейронных сетей и сокращать время на создание прототипов. Sequential API идеально подходит для задач, где модель представляет собой простой стек слоев, подключенных последовательно, тогда как функциональное API позволяет строить модели с более сложными связями, например, многовходные и многовыходные архитектуры, где каждый слой может получать входные данные от нескольких предыдущих слоев или передавать результаты сразу в несколько последующих слоев.Sequential API интуитивно прост и используется для последовательных моделей. Каждый слой добавляется друг за другом, и архитектура строится на основе линейного потока данных. Пример нейронной сети с двумя полносвязанными слоями можно создать так:import tensorflow as tf# Определение модели с использованием Sequential APImodel = tf.keras.Sequential([    tf.keras.layers.Dense(64, activation='relu', input_shape=(32,)),  # первый слой с 64 нейронами    tf.keras.layers.Dense(10, activation='softmax')                   # выходной слой с 10 нейронами для классификации])# Компиляция моделиmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])Функциональное API предоставляет больше гибкости, так как позволяет подключать слои нелинейным образом. Это полезно для сложных архитектур, таких как модели с параллельными ветвями. Оперирует понятием «графа», где каждый слой может быть представлен как узел, и поэтому может быть подключен к нескольким другим слоям или принимать на вход несколько источников данных. Рассмотрим пример создания модели с параллельными слоями:# Входные данные с размерностью 32inputs = tf.keras.Input(shape=(32,))# Первый параллельный слойx1 = tf.keras.layers.Dense(64, activation='relu')(inputs)x1 = tf.keras.layers.Dense(32, activation='relu')(x1)# Второй параллельный слойx2 = tf.keras.layers.Dense(64, activation='relu')(inputs)x2 = tf.keras.layers.Dense(32, activation='relu')(x2)# Объединяем параллельные ветвиx = tf.keras.layers.Concatenate()([x1, x2])# Выходной слойoutputs = tf.keras.layers.Dense(10, activation='softmax')(x)# Создание модели с параллельной архитектуройmodel = tf.keras.Model(inputs=inputs, outputs=outputs)# Компиляция моделиmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])Sequential и функциональное API идеально подходят для большинства случаев, но когда необходимо создать модель с кастомной логикой, специфическим поведением, или реализовать нетривиальные алгоритмы, становится полезным Subclassing API. Этот подход требует создания класса, наследующего `tf.keras.Model`, что позволяет переопределять методы `call` и гибко управлять тем, как данные проходят через слои. Это необходимо для сложных сценариев, таких как рекуррентные слои с состоянием, при настройке собственной логики на каждом этапе или при интеграции нескольких моделей.Пример реализации модели с Subclassing API показан ниже. Здесь реализуется кастомная логика обработки данных, позволяющая менять поведение модели при каждом вызове:# Определение кастомной модели с Subclassing APIclass CustomModel(tf.keras.Model):    def __init__(self):        super(CustomModel, self).__init__()        self.dense1 = tf.keras.layers.Dense(64, activation='relu')        self.dense2 = tf.keras.layers.Dense(32, activation='relu')        self.out = tf.keras.layers.Dense(10, activation='softmax')    def call(self, inputs, training=False):        # Кастомная логика: например, условное использование слоев        x = self.dense1(inputs)        if training:            x = tf.nn.dropout(x, rate=0.5)  # добавляем dropout только при обучении        x = self.dense2(x)        return self.out(x)# Создание экземпляра кастомной моделиmodel = CustomModel()# Компиляция моделиmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])Этот подход дает полную свободу в настройке обучения, управления процессом прохода данных и использования сложной логики для обработки входов и состояний. Например, вы можете легко реализовать условное включение слоев, разные пути вычислений в зависимости от параметров, переключение поведения между режимами обучения и предсказания и более гибко обрабатывать динамическую размерность данных.Обучение и распределённое вычисление: стратегии и параллелизацияПри распределённом обучении ключевой задачей является организация вычислений и данных таким образом, чтобы параллельное исполнение задач шло с минимальными накладными расходами, сохраняя эффективность вычислений. Основные стратегии TensorFlow включают MirroredStrategy, MultiWorkerMirroredStrategy и TPUStrategy, каждая из которых подходит для различных типов вычислительных сред и вариантов распределения нагрузки.MirroredStrategy используется для локального распределения вычислений, то есть параллельного выполнения на нескольких GPU в одной машине. Она дублирует (mirrors) модель на каждый доступный GPU и распределяет мини-пакеты данных между этими устройствами, что позволяет сократить время обучения за счёт использования всех доступных вычислительных ресурсов. Настроить MirroredStrategy достаточно просто. Достаточно указать её как стратегию и обернуть определение модели и процесс обучения:import tensorflow as tf# Определяем стратегию для распределения обучения между несколькими GPUstrategy = tf.distribute.MirroredStrategy()# Определяем модель и компиляцию в рамках стратегииwith strategy.scope():    model = tf.keras.Sequential([        tf.keras.layers.Dense(64, activation='relu', input_shape=(32,)),        tf.keras.layers.Dense(10, activation='softmax')    ])    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])# Обучение модели с распределением на несколько GPUmodel.fit(train_dataset, epochs=5)Здесь стратегия MirroredStrategy автоматически создаёт копии модели на каждом GPU и синхронизирует градиенты после каждого шага обучения. Параллелизация происходит по принципу data parallelism: данные распределяются между устройствами, и каждый GPU обрабатывает часть пакета данных. При этом потери и градиенты синхронизируются и агрегируются после каждой итерации.MultiWorkerMirroredStrategy позволяет распределить обучение на несколько машин, где каждая машина может иметь один или несколько GPU. Эта стратегия особенно полезна в случае кластеров, так как она организует синхронное распределение данных и вычислений между рабочими узлами (workers). Чтобы использовать MultiWorkerMirroredStrategy, необходимо настроить кластер и определить переменные окружения, которые указывают TensorFlow на конфигурацию узлов, участвующих в процессе:# Определяем стратегию MultiWorkerMirroredStrategy для распределенного обученияstrategy = tf.distribute.MultiWorkerMirroredStrategy()with strategy.scope():    model = tf.keras.Sequential([        tf.keras.layers.Dense(64, activation='relu', input_shape=(32,)),        tf.keras.layers.Dense(10, activation='softmax')    ])    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])model.fit(train_dataset, epochs=5)MultiWorkerMirroredStrategy автоматически обеспечивает синхронизацию параметров модели между узлами, что позволяет добиться сходимости модели. Однако нужно учитывать, что такая стратегия накладывает определённые ограничения, например, на пропускную способность сети, которая может стать узким местом, если количество узлов слишком велико. Синхронные стратегии, такие как MultiWorkerMirroredStrategy, требуют точного согласования работы между узлами, что при ошибках конфигурации может приводить к потере производительности или зависаниям.Для высокопроизводительных кластеров, оснащённых тензорными процессорами (TPU), TensorFlow предлагает TPUStrategy. Эта стратегия позволяет масштабировать обучение с использованием тензорных процессоров, которые оптимизированы для выполнения операций, характерных для нейронных сетей. TPU кластеры предоставляют уникальные возможности для скоростной обработки данных и подходят для задач глубокого обучения, требующих больших вычислительных ресурсов. Настроить TPUStrategy можно следующим образом:# Подключаем TPU и определяем стратегиюresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')tf.config.experimental_connect_to_cluster(resolver)tf.tpu.experimental.initialize_tpu_system(resolver)strategy = tf.distribute.TPUStrategy(resolver)with strategy.scope():    model = tf.keras.Sequential([        tf.keras.layers.Dense(64, activation='relu', input_shape=(32,)),        tf.keras.layers.Dense(10, activation='softmax')    ])    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])model.fit(train_dataset, epochs=5)TPUStrategy выполняет аналогичное распределение данных по TPU-ядрам, где каждая мини-партия обрабатывается параллельно на доступных ядрах TPU. Однако в отличие от GPU и CPU, TPU требуют более тщательной настройки данных и параметров. Также TPU могут иметь ограничения в поддержке некоторых специфических функций TensorFlow, что важно учитывать при адаптации моделей.Работа с данными и их подготовка: Dataset API и трансформации данныхВ TensorFlow API для работы с данными tf.data позволяет эффективно обрабатывать и подготавливать данные для обучения моделей. С помощью Dataset API можно создавать конвейеры обработки данных, которые не только загружают и обрабатывают данные, но и оптимизируют их под конкретные задачи. В TensorFlow доступно несколько ключевых классов и функций для работы с данными: Dataset для создания потоков данных, Iterator для итерации по этим данным, а также трансформации, такие как batch, shuffle, map и другие, для манипуляций над данными.Создание и использование датасетов начинается с объекта tf.data.Dataset, который принимает данные в различных форматах, включая массивы, списки, тензоры, файлы и другие источники. Например, для загрузки данных из массива можно использовать from_tensor_slices, чтобы создать объект Dataset, который разбивает массив на отдельные элементы. Важно отметить, что Dataset API поддерживает ленивые вычисления: операции над данными выполняются только при вызове их, что позволяет экономить память и улучшает производительность при работе с большими объёмами данных.import tensorflow as tf# Пример массива данныхdata = tf.range(10)# Создаем датасет из массиваdataset = tf.data.Dataset.from_tensor_slices(data)# Просмотр содержимогоfor element in dataset:    print(element.numpy())Когда данные загружены в виде датасета, можно использовать несколько стандартных методов трансформации для их подготовки к обучению. Ключевые методы включают batch, который группирует элементы по указанному размеру, и shuffle, который перемешивает данные для случайного порядка, что помогает избежать переобучения. Метод batch особенно важен, поскольку он задаёт размер мини-пакетов, используемых для оптимизации и обновления весов модели на каждом шаге обучения. Пример использования:# Перемешиваем данные и группируем в батчи по 3 элементаdataset = dataset.shuffle(buffer_size=10).batch(3)for batch in dataset:    print(batch.numpy())В этом примере shuffle принимает параметр buffer_size, который определяет количество элементов, удерживаемых в буфере для случайного выбора, что позволяет достичь достаточного уровня перемешивания. Использование buffer_size, равного размеру датасета, обеспечивает максимально случайный порядок, однако это может потребовать больше памяти, поэтому размер буфера стоит выбирать с учетом возможностей системы.С помощью метода map можно применять преобразования к каждому элементу датасета, что полезно для выполнения операций, таких как нормализация, масштабирование, кодирование меток и другие виды предобработки данных. Допустим, нужно удвоить каждый элемент в датасете:# Применяем преобразование к каждому элементу датасетаdataset = dataset.map(lambda x: x * 2)for batch in dataset:    print(batch.numpy())При работе с большими и сложными наборами данных, такими как изображения или текстовые данные, tf.data API предоставляет дополнительные возможности для оптимизации конвейера данных. Например, prefetch позволяет загружать следующий батч данных, пока текущий батч обрабатывается на GPU, что сокращает время простоя между шагами обучения. Это делается с помощью метода prefetch, в который можно передать параметр tf.data.AUTOTUNE для автоматической настройки количества предзагружаемых элементов, что оптимизирует работу с учётом доступных ресурсов.# Добавляем предзагрузку данныхdataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)Для более сложных сценариев подготовки данных можно использовать tf.data.experimental, который включает дополнительные функции и расширения для работы с данными. Например, если нужно выполнить агрессивную оптимизацию или подстроить конвейер данных под более сложные требования, такие как смешивание данных или фильтрация, tf.data.experimental предоставляет специальные утилиты для этих задач. Например, для фильтрации элементов в датасете:# Фильтрация элементов по условиюdataset = dataset.filter(lambda x: x % 2 == 0)for element in dataset:    print(element.numpy())Таким образом, Dataset API в TensorFlow предоставляет гибкость и контроль над конвейером данных, позволяя настроить его под нужды конкретной модели и инфраструктуры. Сочетание таких операций, как batch, shuffle, map и prefetch, в связке с продвинутыми методами оптимизации из tf.data.experimental, позволяет создать высокоэффективный и параллельно исполняемый конвейер данных для любого сценария, включая обучение на больших наборах данных и реальное время.Сохранение и развертывание моделейTensorFlow предоставляет несколько способов сохранения моделей и контроля версий, начиная с SavedModel и HDF5-формата и заканчивая оптимизацией для продакшена с помощью TensorFlow Lite и TensorFlow Serving. SavedModel — универсальный формат, поддерживающий хранение архитектуры, весов, метаданных и графа вычислений, который широко используется для экспорта моделей и их дальнейшего разворачивания. Сохранение в формате HDF5 также является опцией для некоторых моделей, особенно при разработке с использованием Keras. В первую очередь стоит рассмотреть, как сохранить модель для последующего развертывания.Чтобы сохранить модель, созданную на основе Keras, можно воспользоваться методом model.save. Например, при сохранении модели в формате SavedModel, TensorFlow сохраняет её структуру, веса и оптимизатор, что позволяет использовать модель для различных платформ и сценариев развертывания. Код для этого выглядит так:import tensorflow as tf# Создаем простую модель для примераmodel = tf.keras.models.Sequential([    tf.keras.layers.Dense(128, activation='relu'),    tf.keras.layers.Dense(10)])# Сохраняем модель в формате SavedModelmodel.save(\"path_to_saved_model\")TensorFlow автоматически создаёт директорию с метаданными, которые содержат структуру модели, веса и служебные данные, что позволяет легко загружать её для inference или дообучения. Также поддерживается HDF5-формат, который можно выбрать, указав параметр save_format=\"h5\". HDF5 формат удобен при переносе на другие системы и интеграции с библиотеками, которые могут не поддерживать SavedModel:# Сохраняем модель в формате HDF5model.save(\"model.h5\", save_format=\"h5\")После сохранения модели следующим этапом является её оптимизация для продакшена. TensorFlow Lite позволяет сжать и ускорить модель для мобильных и встраиваемых устройств, таких как смартфоны или IoT устройства. Оптимизация с помощью TensorFlow Lite включает этапы квантования, которые уменьшают размер и ускоряют вычисления модели. Преобразование модели в TensorFlow Lite формат происходит с помощью tf.lite.TFLiteConverter, который создаёт сжатую версию модели, удобную для развертывания на устройствах с ограниченными ресурсами.# Конвертация модели в TensorFlow Liteconverter = tf.lite.TFLiteConverter.from_saved_model(\"path_to_saved_model\")tflite_model = converter.convert()# Сохраняем TensorFlow Lite модельwith open(\"model.tflite\", \"wb\") as f:    f.write(tflite_model)Для сервисов, которые требуют масштабируемого развертывания, TensorFlow Serving предоставляет готовый сервер для модели, поддерживающий высокопроизводительный inference на CPU и GPU. TensorFlow Serving — это решение для развёртывания модели в веб-сервисе, который обрабатывает запросы с предсказаниями в реальном времени. TensorFlow Serving поддерживает горячую замену модели без перезапуска сервиса и версионирование, что позволяет управлять несколькими версиями модели параллельно. Развертывание модели через TensorFlow Serving обычно требует Docker-контейнера или отдельного сервера, и можно воспользоваться REST или gRPC API для выполнения предсказаний.Для развертывания на облачных платформах, таких как Google Cloud AI Platform или Amazon SageMaker, SavedModel-формат также является оптимальным, поскольку поддерживается для серверного инференса и позволяет быстро разворачивать масштабируемые ML-приложения. Например, на Google Cloud, загрузка модели в SavedModel формате позволяет использовать платформенные инструменты для автошкалирования, автоматического управления версиями и мониторинга.Оптимизация также может включать квантование и прайнинг (удаление нерелевантных частей модели) для работы на мобильных устройствах и IoT, что выполняется через TensorFlow Model Optimization Toolkit. Квантование может сократить размер модели в 4 раза и ускорить производительность, что существенно важно для приложений с ограниченной вычислительной мощностью.",
  "Мини-ПК ноября 2024 года: на какие модели стоит обратить внимание": "\nМини-ПК становятся все более популярными благодаря своей компактности и мощности. С их помощью пользователи могут решать широкий спектр задач в условиях ограниченного пространства. В этом обзоре рассмотрим актуальные модели мини-ПК, их основные характеристики, функциональные возможности и стоимость. Независимо от того, ищете ли вы устройство для офиса, учебы или развлечений, здесь найдется подходящий вариант для любых целей и бюджета.\n\nTopton D13\n\nИсточник.\n\nTopton D13 — это компактный мини-ПК, размеры которого составляют всего 108 x 108 x 60 мм. Он оснащен мощным процессором AMD Ryzen™ 7 7840HS, 32 ГБ оперативной памяти LPDDR5-6400 и тремя слотами M.2 2280 для накопителей с поддержкой PCIe 4.0 x4. Устройство отличается универсальностью, поддержкой внешнего графического дока через порт OCuLink и возможностью подключения до трех дисплеев, что делает его отличным решением для разнообразных задач.\n\nУ девайса есть активное охлаждение. Верхняя деревянная крышка добавляет стильный акцент в дизайне, наверное. \n\nХарактеристики\n\nПроцессор: AMD Ryzen™ 7 7840HS, 8 ядер Zen 4, TDP 35-54 Вт.\nОперативная память: 32 ГБ LPDDR5-6400 (наборная).\nХранилище: три слота M.2 2280 (PCIe 4.0 x4), поддержка до 6 ТБ PCIe NVMe (при доплате).\nПорты:\n \n1 x USB4 40 Гбит/с,\n4 x USB 3.2 Gen 2 Type-A,\n2 x HDMI 2.1,\n2 x LAN 2.5 Гбит/с (Intel® i226-V),\n1 x 3.5 мм аудиоразъем,\n\nБеспроводная связь: WiFi 6, Bluetooth 5.2.\nГрафика: встроенная Radeon 780M с 12 RDNA и тремя GPU-юнитами,\nПоддержка eGPU: OCuLink для внешнего графического дока с пропускной способностью 64 Гбит/с.\n\nМини-ПК Topton D13 доступен на AliExpress по стартовой цене около $ 400 за версию с 32 ГБ оперативной памяти, без накопителя или операционной системы.\n\nGMKtec EVO-X1 HX370\n\nИсточник.\n\nGMKtec EVO-X1 HX370 — это компактный мини-ПК от китайского производителя GMKtec, который оснащен новейшим процессором AMD Strix Point, AMD Ryzen™ AI 9 HX 370. Устройство поддерживает высокоскоростное подключение внешней графики через порт OCuLink, что делает его отличным выбором для тех, кто хочет комбинировать компактность устройства с возможностью использования мощного графического дока. GMKtec EVO-X1 HX370 также отличается высоким уровнем производительности и энергоэффективностью благодаря процессору с интегрированным чипом для AI-вычислений.\n\nХарактеристики\n\nПроцессор: AMD Ryzen™ AI 9 HX 370, 12 ядер, 24 потока, 4 ядра Zen 5 и 8 ядер Zen 5C, частота до 5.1 ГГц.\nОперативная память: LPDDR5x-7500 (наборная).\nГрафика: встроенная AMD Radeon 890M, 16 RDNA 3.5 GPU-юнитов, частота до 2.9 ГГц\nПоддержка AI: Ryzen AI с производительностью до 50 Топс.\nПорты:\n\n1 x OCuLink 64 Гбит/с,\n1 x USB Type-C,\n2 x USB Type-A (на передней панели),\n1 x 3.5 мм аудиоразъем.\n\nПока GMKtec не объявила цену и дату выхода GMKtec EVO-X1 HX370. Подробности ожидаются после релиза.\n\nApple Mac Mini (2024)\n\nИсточник.\n\nНовый Mac Mini 2024 года от Apple представляет собой компактный настольный компьютер с обновленным дизайном, который занимает вдвое меньше места на рабочем столе, чем предыдущие модели. Основные изменения коснулись процессоров и портов. Базовая модель оснащена чипом M4, а более дорогие конфигурации — M4 Pro, что позволяет добиться значительного прироста производительности, особенно в задачах 3D-рендеринга и обработки видео. Модель с чипом M4 Pro предлагает поддержку Thunderbolt 5 для высокоскоростной передачи данных и поддержки высокоразрешаемых дисплеев. Это делает Mac Mini универсальным решением для рабочих и творческих задач.\n\nThunderbolt 5 на модели с M4 Pro обеспечивает двустороннюю скорость передачи данных до 80 Гбит/с и асимметричные скорости до 120 Гбит/с для вывода видео. Это один из немногих настольных ПК с такими возможностями. \n\nХарактеристики M4\n\nПроцессор: десять ядер (четыре производительных и шесть энергоэффективных).\nГрафика: десять ядер.\nNeural Engine: 16-ядерный.\nПропускная способность памяти: 120 ГБ/с.\nОперативная память / Накопитель: от 16 ГБ / 256 ГБ до 32 ГБ / 2 ТБ\nПорты: 3 x Thunderbolt 4 до 40 Гбит/с.\nПоддержка дисплеев: до трех дисплеев (2 x 6K/60Hz + 1 x 5K/60Hz или 1 x 5K/60Hz + 1 x 8K/60Hz).\nБеспроводная связь: WiFi 6E, Bluetooth 5.3.\nДополнительно: встроенный динамик.\n\nХарактеристики M4 PRO\n\nПроцессор: 12 ядер (восемь производительных и четыре энергоэффективных).\nГрафика: 16 ядер с возможностью обновления до 20 ядер за доплату.\nNeural Engine: 16-ядерный.\nПропускная способность памяти: 273 ГБ/с.\nОперативная память / Накопитель: от 24 ГБ / 512 ГБ до 64 ГБ / 8 ТБ\nПорты: \n\n3 x Thunderbolt 5 (до 120 Гбит/с), \n2 x USB 3.2 Gen 2 Type-C (10 Гбит/с), \n1 x HDMI, 1 x 3.5 мм аудиоразъем,\n1 x Ethernet (Гбит или 10 ГБ), \n1 x Power.\n\nПоддержка дисплеев: до трех дисплеев (3 x 6K/60 ГГц или 1 x 6K/60 ГГц + 1 x 8K/60 ГГц)\nБеспроводная связь: WiFi 6E, Bluetooth 5.3.\nДополнительно: встроенный динамик.\n\n\nИсточник.\n\nMac Mini доступен для предварительного заказа с $ 599 за модель с чипом M4 и $ 1399 за версию с M4 Pro. Ожидаемая дата начала продаж — 8 ноября 2024 года.\n\n\nAsus NUC 14 Essential (NUC14MNK)\n\nИсточник.\n\nAsus NUC 14 Essential — компактный настольный компьютер размером 135 x 115 x 36 мм, который поддерживает до трех дисплеев. Идеально подойдет для мультимедийных и офисных задач. Устройство — одна из первых моделей, совместимых с новейшими процессорами Intel серии N, включая пока не анонсированные чипы, которые обеспечивают улучшенную производительность и энергоэффективность. Asus предлагает эту модель в виде полностью готового ПК или как barebones-версию без памяти, накопителя и операционной системы.\n\nУстройство ориентировано на подключение нескольких дисплеев, предлагая широкий набор портов, включая поддержку DisplayPort 1.4 и HDMI 2.1. Важная особенность модели — наличие порта 2.5 Гбит/с Ethernet для высокоскоростного сетевого соединения, что редко встречается в компактных системах этого класса.\n\nХарактеристики\n\nДоступно четыре варианта процессоров:\n\nIntel® N97 (4 ядра, Intel UHD Graphics, 12 Вт TDP);\nIntel® N150 (6 МБ кэш, до 3.6 ГГц, 6 Вт TDP);\nIntel® N250 (6 МБ кэш, до 3.8 ГГц, 6 Вт TDP);\nIntel® Core 3 N355 (6 МБ кэш, до 3.9 ГГц, 15 Вт TDP).\n\nОперативная память: до 16 ГБ, одноканальная DDR5-4800.\nХранилище: один слот M.2 2280/2242 PCIe Gen 3 x4 (поддержка NVMe или SATA).\nПорты:\n\n1 x USB 3.2 Gen 2 Type-C (10 Гбит/с с поддержкой DisplayPort 1.4);\n1 x USB 3.2 Gen 2 Type-C (10 Гбит/с, только для передачи данных);\n4 x USB 3.2 Gen 2 Type-A (10 Гбит/с);\n1 x USB 2.0 Type-A (480 Мбит/с);\n1 x HDMI 2.1;\n1 x DisplayPort 1.4;\n1 x 2.5 GbE Ethernet (RTL8125BG-CG);\n1 x 3.5 мм аудиоразъем;\n1 x вход питания (19 В/3.42 А, 65 Вт).\n\nБеспроводная связь: WiFi 6E и Bluetooth 5.x.\nПоддержка дисплеев: до 3 дисплеев.\n\nПока Asus не объявила точную цену и дату выхода NUC 14 Essential, эта информация ожидается в ближайшее время.\n\nMINISFORUM EliteMini AI370\n\nИсточник.\n\nMINISFORUM EliteMini AI370 — это один из первых мини-ПК, который оснащен процессором AMD Strix Point с ядрами Zen 5, графикой RDNA 3.5 и нейронным процессором Ryzen AI для высокопроизводительных вычислений в области искусственного интеллекта. Устройство поддерживает одновременное подключение трех дисплеев 4K с частотой 120 Гц, а компактный прочный корпус делает его удобным для использования как в домашних, так и в рабочих условиях.\n\nХарактеристики\n\nПроцессор: AMD Ryzen AI 9 HX 370, 12 ядер, 24 потока, частота до 5.1 ГГц, TDP 45 Вт.\nГрафика: 16-ядерная Radeon 890M, частота до 2.9 ГГц\nNeural Engine: Ryzen AI с производительностью до 50 TOPS, до 80 TOPS в общем при работе с CPU и GPU.\nОперативная память: 32 ГБ LPDDR5x-7500 (не заменяемая пользователем).\nХранилище: два слота M.2 2280 PCIe 4.0 (до 4 ТБ общей емкости)\nПорты:\n\n1 x USB4 40 Гбит/с,\n4 x USB 3.2 Gen 2 Type-A 10 Гбит/с,\n1 x HDMI 2.1,\n1 x DisplayPort 2.0,\n2 x 2.5 GbE Ethernet,\n1 x 3.5 мм аудиоразъем.\n\nБеспроводная связь: WiFi 6E, Bluetooth 5.3.\nСистема охлаждения: три медных тепловых трубки, большой тихий вентилятор и дополнительный вентилятор для охлаждения SSD.\n\nMINISFORUM EliteMini AI370 доступен для предзаказа по цене $ 1 099 за модель с 32 ГБ ОЗУ и 1 ТБ накопителя. Ожидается, что поставки начнутся 4 ноября 2024 года.\n\nВ статье представлены модели на любой вкус и цвет, как говорится. Если у вас есть собственные предпочтения, пишите о них в комментариях.",
  "Немного про UEFI и GRUB": "Долго думал, писать статью эту или нет, но может информация в ней будет кому-то полезна, особенно в части формирования grubx64.efi во время grub-install - об этом собственно основная часть статьи. Как обычно, не могу сказать, что я эксперт в данном вопросе и быть может что-то из написанного окажется не совсем верным, совсем не правильным или не полным. Не обладаю глубокими знаниями по теме статьи, хоть и какой-то опыт имеется. Очень может быть, что я не раскрою всех возможностей описываемых в данной статье вещей.Как обычно, началось всё с рабочей задачи: приехало новое железо и нужно установить на него специализированное ПО определённой версии.Проблема в том, что данное ПО довольно старое и самый простой способ его перенести - это склонировать с установленного сервера вместе с ОС (в моём случае Debian Stretch). Сервер старый и устанавливалось всё это давно и понятно, что там Legacy в части загрузки. А новый сервер настолько новый (2024 года), что Legacy он как-то очень плохо умеет и его биос штатно называется UEFI биос и если его выключить, становилось как-то совсем грустно. К слову старое ядро ОС еще и raid контролер не видело, а соответственно и диски, но сейчас не об этом.Думаю ни для кого не секрет, что большинство современного железа умеет загружаться в Legacy режиме (по сути это обычная загрузка биоса с помощью boot сектора и дальнейшая загрузка с MBR) и в UEFI режиме (загрузка на основе NVRAM из произвольного диска и раздела, который как правило имеет GPT разметку диска). Хотя судя по этому серверу, тенденция идёт на постепенное вытеснение поддержки Legacy, что в целом логично.Стоит упомянуть, что раздел с UEFI загрузчиком по опыту лежит в FAT32 разделе. Он есть и в WIndows и в Linux и в Mac: как правило это небольшой раздел с папкой EFI. В данной папке находится загрузчик (вместо небольшого бут-сектора в MBR), который уже делает всё остальное.Вот например как выглядит запись в NVRAM с загрузичком для Proxmox (Debian)Boot0007* proxmox       HD(2,GPT,62b4a0b9-53eb-4b34-9e7f-e637286893fb,0x800,0x100000)/File(\\EFI\\proxmox\\grubx64.efi)Кроме айди и uuid раздела, видно путь \\EFI\\proxmox\\grubx64.efiДанную запись можно увидеть с помощью команды efibootmgr -v. Ей так же можно, а иногда нужно управлять записями в NVRAM.Для начала попробую объяснить на пальцах на примере Linux, как происходит загрузка в том или ином режиме.LegacyРаботает в MBR. BIOS производит поиск загрузочного сектора в начале диска. Данный сектор является мини загрузчиком (до 512 байт). Базово он просто находит активный раздел среди 4 возможных разделов и производит оттуда загрузку. В случае Linux и GRUB, MBR загружает промежуточный код, который умеет чуть больше, чем открыть FAT на разделе. Например может загрузить EXT2. Далее уже происходит запуск самого grub, который уже управляет дальнейшей загрузкой ОС. Ну и стоит упомянуть, что MBR поддерживает максимум 2 тб.UEFIРаботает чуть иначе. Разница по сути только в том, что базовый загрузчик находится не в загрузочном секторе, а в своём разделе на своей ФС, вместо костылей в MBR. Это может быть даже не базовый загрузчик, который передаёт управление большому GRUB, а в целом может самостоятельно полноценно загрузить ОС без посредников и классического GRUB. Да и в целом, даже Doom чисто из UEFI можно запустить https://github.com/Cacodemon345/uefidoom, куда же без него. А информация об этом загрузчике хранится в специальной NVRAM, где есть ссылка на диск/раздел и путь к бинарному файлу, который необходимо выполнить BIOS-у для загрузки. Таблица разделов используется при этом всём GPT и нет ограничения в 2 тб.Как я уже говорил, с EFI раздела по сути можно целиком загрузить ОС. Вероятно туда можно даже запихнуть ядро, но это не точно. В общем штука эта прикольная и со временем про MBR все забудут. Есть конечно один минус UEFI - это необходимость записи в NVRAM. Из-за этого нельзя взять и легко переставить диски с UEFI с одного компа на другой, а так же встречающиеся глюки некоторых материнок, связанные с этой записью в NVRAM, но в целом это не такая большая проблема. Глюки - обходятся. Запись восстанавливается. К тому же всегда можно загрузить загрузочный бинарь с помощью UEFI Shell.Вообще UEFI это не только чисто про загрузку ОС, но много про чего еще. Но в основном в реальной жизни мне лично интересна только эта его часть, да и вообще не хочется сильно часто вспоминать это слово.Перенос ОС и установка UEFI GRUB.Процесс переноса ОС с MBR на UEFI не совсем тривиальный, однако про него много информации в интернете и в целом он обычно не вызывает проблем - напишу кратко. Алгоритм там простой - создаём нужные разделы в GPT (EFI раздел, /boot раздел (не обязательно), и раздел под /). Можно в целом всё засунуть в LVM, включая /boot, можно LVM накатить поверх mdraid). В общем - тут всё по вкусу. Далее переносим файлы и редактируем файлы, где меняются blkid, пересоздаём initramfs, делаем grub-install. И по идее всё должно работать. Так же есть опция для обратной совместимости с MBR (раздел BIOS Boot), но это особо не интересно и не обязательно.Но статьи бы не было, если бы всё пошло гладко. В Linux нет каких-то способов автоматизировать данное восстановление, поэтому делается всё вручную. Возможно дело еще в староватой версии ОС - Debian Stretch. В общем грузиться автоматически ОС никак после переноса не хотела. Загружался вручную из оболочки UEFI GRUBinsmod lvm\ninsmod all_video\nset root=(hd0,gpt2)\nlinux /vmlinuz-4.19.0-0.bpo.19-amd64 root=/dev/mapper/a947--cf--b2g01--vg-root ro\ninitrd /initrd.img-4.19.0-0.bpo.19-amd64\nbootТут возможно нужны пояснения. В данном случае производится загрузка без участия \"большого\" grub, сразу из UEFI GRUB. Как я говорил, такое возможно только с ним. В MBR так бы не получилось скорее всегоЗагружаем модуль LVM, чтоб видно было root разделВидео-модуль - без него на консоль ничего не выводитсяВыставляем root на gpt2, на самом деле это не root, там просто лежит ядро и initrdВыставляем строку загрузки указываем рут раздел. Важно указывать через /, т.к на gpt2 никакого /boot нетВыставляем initrdГрузимся! Так я хотя бы мог загрузится в ОС без необходимости грузится с rescue cd.grubx64.efiНа EFI-системах grub-install делает две вещи - он вызывает grub-mkimage, чтобы собрать grubx64.efi, а потом прописывает его в NVRAM.Во время сборки туда зашивается расположение grub.cfg, либо его содержимое, либо он берёт grub.cfg из той директории, откуда запущен.То есть как ни странно grubx64.efi это не просто статический бинарь, который откуда-то копируется или собирается, но он еще и содержит в себе мини-конфиг и прежде чем продолжить чтение статьи, если это интересно - надо это осознать!Проверить можно, вытащив секцию mods из grubx64.efi:objcopy -j mods -O binary /boot/efi/EFI/debian/grubx64.efi /tmp/mods\nhexdump -C /tmp/mods | tail -n 8Находим там либо \"03 00 00 00 SS SS SS SS\", либо \"02 00 00 00 SS SS SS SS\". Тип 3 - это prefix, тип 2 - это config, SS - размер в little-endian.То есть перед каждым блоком лежит его тип (0=elf, 1=memdisk, 2=config, 3=prefix, 4=pubkey) и размер.Например:00003f00  00 00 00 00 00 00 00 00  f0 08 00 00 00 00 00 00  |................|\n00003f10  66 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |f...............|\n00003f20  01 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\n00003f30  03 00 00 00 20 00 00 00  28 2c 67 70 74 32 29 2f  |.... ...(,gpt2)/|\n00003f40  62 6f 6f 74 2f 67 72 75  62 00 00 00 00 00 00 00  |boot/grub.......|\n00003f50  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|Это ошибочный параметр в моём случае. Должен быть типа того00003f00  00 00 00 00 00 00 00 00  f0 08 00 00 00 00 00 00  |................|\n00003f10  66 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |f...............|\n00003f20  01 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\n00003f30  03 00 00 00 18 00 00 00  28 2c 67 70 74 32 29 2f  |........(,gpt2)/|\n00003f40  67 72 75 62 00 00 00 00  00 00 00 00 00 00 00 00  |grub............|\n00003f50  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|Или можно просто сделатьstrings /boot/efi/EFI/debian/grubx64.efi |tail -5Поскольку на gpt2 разделе (а это по сути папка /boot) папка с файлами grub расположена по пути /grub.Именно в этом была проблема не загрузки. В целом проблема простая, я не первый раз на неё натыкаюсь из-за того, что в системе ядро и \"большой\" grub лежит в /boot, но он находится по привычке на своём разделе, поэтому относительно него, grub и ядро лежит в корне этого раздела.Еще немного теорииУ GRUB есть модуль normal, который в момент загрузки ищет grub.cfg в следующих местах:встроенный в mods через тип 2;$prefix/grub.cfg, где $prefix берётся из mods, тип 3;./grub.cfg, просто из текущей директории.Конфиг генерируется утилитой grub-mkconfig, а в Debian/Ubuntu есть обёртка поверх неё - update-grub, которая записывает результат работы grub-mkconfig в grub.cfg и вызывается триггером из пакетного менеджера при установке/удалении ядер, чтобы перегенерировать меню.grub-mkconfig конфигурируется из /etc/default/grub. Для генерации  конфига используется grub-probe, чтобы определить необходимые модули для  работы с /boot и /etc/grub.d, где лежат шаблоны. mkconfig обходит /boot  (и, возможно, ещё ходит в os-prober для того, чтобы найти другие ОС) и создаёт по записи в меню на каждое ядро, которое может запустить.grub-install ничего с конфигом не делает. grub-install и update-grub запускаются в postinst-скрипте пакетов grub-pc и grub-efi: https://salsa.debian.org/grub-team/grub/-/blob/debian/2.12-5/debian/postinst.inЕсли в этой директории нет grub.cfg, нет этой директории, она не на  втором разделе или её нет, таблица разделов не GPT, или диск другой, или в core.img нет драйвера для файловой системы, контроллера диска или  GPT, то ничего не получится. fallback у этой схемы нет - то есть если блок с префиксом зашит в grubx64.efi, и префикс этот неправильный, то GRUB не будет использовать другие способы поиска grub.cfg, а просто остановится.Тут главное что надо понять, если у grubx64.efi прописан какой-то тип и он не работает, то к другим способам поиска конфига grub efi не приступит.Что делать? (известно что 🐜)Нужно просто запустить правильную команду grub-install --target=x86_64-efi --efi-directory=/boot/efi --bootloader-id=debian --recheck  Просто grub-install почему-то не работал, надо было именно полную команду вводить.Неправильная команда:grub-install  --target=x86_64-efi --efi-directory=/boot/efi --bootloader-id=debian --recheck --root-directory=/bootПриводила как раз к ошибкам.Проверяем что теперь всё как надо# strings /boot/efi/EFI/debian/grubx64.efi |tail -5\n.rela.data\n.module_license\n.bss\n.modname\n(,gpt2)/grubПерезагружаемся и радуемся.Еще может быть не лишним привести в порядок параметры загрузки UEFI через команду efibootmgr - осторожно удалить лишнее. Из-за этого тоже что-то может не грузиться.Так же, в случае с mdraid с raid1, EFI разделы по хорошему должны быть на обоих дисках, но при этом не быть сами в mdraid. Хотя в теории ничего не мешает и EFI в raid засунуть при использовании метаданных 0.9 в mdadm, но смысла кажется в этом особо нет.Так вот, может случиться так, что в одном разделе grubx64.efi вы исправили, а в другом нет. А грузится как раз оттуда и можно несколько часов убить, не понимая что происходит.В качестве интересного бонуса для тех, кому интересно. Так выглядит grubx64.efi в Debian 12:003e1dc0  02 00 00 00 28 00 00 00  6e 6f 72 6d 61 6c 20 28  |....(...normal (|\n003e1dd0  6d 65 6d 64 69 73 6b 29  2f 67 72 75 62 2e 63 66  |memdisk)/grub.cf|\n003e1de0  67 0a 00 00 00 00 00 00  03 00 00 00 18 00 00 00  |g...............|\n003e1df0  2f 45 46 49 2f 64 65 62  69 61 6e 00 00 00 00 00  |/EFI/debian.....|\n003e1e00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|И весит этот файл 4 мб, по сравнению с Debian 9, где он весит 120 кб.Грузится он там соответственно на основе grub.cfg. Наверное можно было бы просто подсунуть от Deb12 файл grubx64.efi в Deb9, возможно это бы даже сработало, но это было бы не так интересно.ЗаключениеНельзя сказать, что удалось полностью разобраться с механизмами формирования grubx64.efi, а особенно вот этой его странной и на мой взгляд не удобной особенностью, что файл содержит всё в себе (и кажется даже модули), при этом не совсем понятно, как с ним работать. Возможно я не слишком усердно гуглил. Например до сих пор, я не понимаю почему размеры файла в deb9 и deb12 так отличаются, как и отличается наполнение секции с \"конфигом\".Собственно это и есть причина, почему я решил написать данную статью - мало информации в интернете.ps. Во время решения данных проблем обращался к @kmeawи он вывел меня на правильный путь, за что ему огромное спасибо. ",
  "Неувядающая классика или «чёрный ящик»: кто кого в битве за прогноз. Глава вторая. Продолжение": "В прошлой части мы с вами остановились на том, что обнаружили у временного ряда с температурой две сезонности и, несмотря на это, решили двигаться дальше в выполнении сезонной модели САРПСС по методологии АРПСС. В этой части второй главы мы с вами продолжим применение методологии для поиска оптимальных параметров модели, которая будет адекватно описывать целевой временной ряд с температурой.Напоминаю, что в нашем ряду проявляются сильные сезонные зависимости и имеется тенденция к увеличению температуры со временем. Давайте выполним дополнительную проверку и разложим оригинальный временной ряд на три компоненты - годовую сезонность, тенденцию и случайную компоненту. Для этого применим метод statsmodels seasonal_decompose(), установив период равный 8760-ти шагам.По выполненным в предыдущих частях графикам с температурой можно предположить, что амплитуда колебаний годовой сезонности остаётся относительно постоянной с увеличением тенденции. Данное предположение оправдывает построение аддитивной модели. Хотя Дж.Бокс и Г.Дженкинс из своего опыта советовали «в качестве начального приближения» строить мультипликативную модель, метод seasonal_decompose() не даст нам это сделать по причине присутствия множества нулевых значений во временном ряду.  Конечно, было бы правильным потратить n-ое время на преобразования данных и построить мультипликативную модель, тем более что концы у каждой годовой дуги разнятся. Однако предлагаю выполнить аддитивную модель: будем работать с тем, что есть.from statsmodels.tsa.seasonal import seasonal_decompose\n\ndef show_seasonal_decompose_plot(series, period):\n    \"\"\"Декомпозиция временного ряда на компоненты\"\"\"\n    decomposition = seasonal_decompose(series, period=period, model='additive')\n\n    fig, axs = pyplot.subplots(4, 1)\n\n    axs[0].plot(series, color='blue', alpha=0.8)\n    axs[0].set_title('Оригинальный временной ряд')\n\n    axs[1].plot(decomposition.seasonal, color='coral')\n    axs[1].set_title('Сезонная компонента')\n\n    axs[2].plot(decomposition.trend, color='green')\n    axs[2].set_title('Тенденция')\n\n    axs[3].plot(decomposition.resid, color='blue', alpha=0.3)\n    axs[3].set_title('Случайная компонента')\n\n    pyplot.suptitle(f\"Декомпозиция временного ряда (период={period})\")\n    pyplot.show()\n\n\nshow_seasonal_decompose_plot(series=temperature, period=365*24)Вдобавок построим отдельный график АКФ, чтобы подтвердить годовую сезонность в 8760 шагов:def show_autocorrelation_plot(series, name_column='Температура'):\n    \"\"\"График автокорреляции временного ряда\"\"\"\n    plot_acf(series, lags=arange(len(series)), marker='')\n    pyplot.xlabel('Шаг запаздывания', fontsize=12)\n    pyplot.ylabel('Автокорреляция', fontsize=12)\n    pyplot.title(f\"График автокорреляции временного ряда \\\"{name_column}\\\"\")\n    pyplot.grid(True)\n    pyplot.show()\n\n\nshow_autocorrelation_plot(series=temperature)Итак, сразу бросается в глаза синусообразная форма автокорреляционной функции, которая затухает с увеличением количества шагов запаздывания. Годовой период равняется 8760 шагам и, как хорошо видно по оригинальному временному ряду и по сезонной компоненте разложенного ряда, имеет повторяющуюся форму в виде дуги, которую можно попробовать смоделировать с помощью математической функции. Дневной период не имеет чётко выраженной структуры (далее на графике с температурой, разграниченной на части, мы с вами убедимся в этом): несмотря на то, что в дневных данных есть тенденция к чередованию значений «вверх-вниз», будущие значения не могут быть точно описаны математической функцией. В этой связи можно предположить, что целевой временной ряд, состоящий из случайных элементов, может быть частично детерминированным по годовой сезонности.Также в литературе по прогнозированию временных рядов часто отмечается, что синусообразная и «бесконечная» (именно такой термин используют Дж.Бокс и Г.Дженкинс) форма автокорреляционной функции может указывать на необходимость дополнительного взятия разности (параметр d). Однако мы должны помнить, что «конечную разность берут столько раз, сколько необходимо, чтобы обеспечить стационарность» [194 с.], поэтому полностью доверяться поведению АКФ с сильной дневной зависимостью в данных с более 70 тыс. строк не стоит. В дальнейшем мы выполним анализ остатков прогнозной модели, включая проверку в них автокорреляции, чтобы выяснить, имеет ли смысл увеличить порядок разности с 1 до 2 или нет.Для более точного определения параметров p и q временного ряда с разностью 1-го порядка повторно выполним команду из первой части, но уменьшим количество шагов временного запаздывания до 25:plot_target_analysis(temperature, lags=25)Хотя ЧАКФ и обрывается после первого шага, далее она представлена в форме плавно затухающей синусоиды и бесконечна. Форма затухающей синусоиды у обеих функций - АКФ и ЧАКФ - говорит о том, что мы имеем дело со смешанным процессом авторегрессии - скользящего среднего (АРСС).Отдельно отмечу, что Дж.Бокс и Г.Дженкинс в своей книге разбирают случаи поведения АКФ и ЧАКФ, поэтому для общего понимания рекомендую ознакомиться с таблицей 3.2 «Сводка характеристик процессов авторегрессии, скользящего среднего и смешанных (АРСС) процессов» (97 с. книги 1-го выпуска).По верхнему графику АКФ видно, что большая часть шагов запаздывания значительно удалена от доверительного интервала, который в данном случае вовсе не заметен, поэтому мы не можем сразу определить какое значение q взять за основу.  Вообще говоря, в книгах по прогнозированию временных рядов отмечается - в частности в полюбившейся нам книге Дж.Бокса и Г.Дженкинса - что «адекватное описание наблюдаемых временных рядов достигается при помощи моделей авторегресии, скользящего среднего или комбинированной модели, в которых p и q не больше, а часто и меньше 2» [26 с.]. При этом коллеги эконометристы оставляют важное дополнение: «если нет сезонной компоненты» [из книги «Эконометрика. Начальный курс: Учеб.» - 5-е изд., испр. / Магнус Я.Р., Катышев П.К., Пересецкий А.А. - М.: Дело, 2001. - 400]. Учитывать их опыт необходимо, но ограничивать себя этими рамками, имея дело с «большими» временными рядами с сильными сезонными особенностями, было бы ошибкой.Нужно понимать, что наши данные далеко не игрушечные: они имеют сильное влияние сезонной компоненты (дневная и годовая сезонности) и их объём значителен (к сведению: набор временных рядов конкурса М3 из приведённого ранее исследования 2018 года представлен временными рядами с тремя тысячами наблюдений [ссылка]). Поэтому для начала предлагаю принять q равным 24. Дополнительно возьмём значения 24 и 48.Обращаю внимание, что нулевые шаги запаздывания как в АКФ, так и в ЧАКФ неинформативны в плане определения зависимостей между наблюдениями в разные моменты времени, поскольку представляют собой корреляцию временного ряда сами с собой в тот же самый момент времени. И они всегда равны 1. Поэтому их не берут в расчёт.На графике с ЧАКФ видно, что первое запаздывание представлено в виде обособленной пики, что является хорошим знаком в плане наличия статистически значимой автокорреляции на этом шаге, поэтому начальное значение для p определим равным 1. Однако и здесь наблюдается удалённость большей части запаздывающих шагов от доверительного интервала; к тому же на графиках с ЧАКФ (lags=120 и 25) также выявилась сезонность в шагах запаздывания. При этом видно, что наиболее предпочтителен для дальнейшего использования и анализа 23-й шаг запаздывания, поскольку он наиболее удалён от нулевой линии, чем соседние шаги. Но и здесь мы не будем себя сдерживать: для учёта краткосрочных влияний процесса авторегрессии помимо значений 1 и 23 возьмём также 24. Вдобавок, ради интереса прихватим значение 46, чтобы посмотреть, способно ли дальнейшее увеличение количества шагов запаздывания повлиять на результат.Важно отметить, что принимая такие значения запаздываний, как 23 и 24, в качестве основы мы не только пытаемся с их помощью учесть краткосрочные (дневные) зависимости  процессов АРСС, но и обыграть ограничение модели SARIMA/SARIMAX в одну сезонность, которую отнесём к долгосрочной (годовой) зависимости с использованием параметра m=8760. При этом определение моделей с «избыточными» параметрами (значения 46, 48) оправдано не только из-за наличия в данных двух сезонностей. Выполнение диагностической проверки с использованием метода введения избыточного числа параметров позволит выявить («угадать») неадекватные свойства модели: «Идентифицировав модель и считая её правильной [в нашем случае мы предполагаем, что это ARIMA[23, 1, 24]. - Примечание], мы затем подгоняем более сложную модель. Это ставит под угрозу идентифицированную модель, потому что более сложная модель содержит дополнительные параметры, с помощью которых можно ликвидировать возможные отклонения. Нужно тщательно продумать вопрос о том, как следует усложнить модель.» [из книги «Анализ временных рядов: прогноз и управление /Дж.Бокс и Г.Дженкинс. Выпуск 1. - Издательство «Мир», Москва 1974. - 314 с.»]Итак, с учётом результатов статистических тестов для определения стационарности и анализа вышерассмотренных графиков АКФ и ЧАКФ примем следующие порядки (p, d, q) для модели АРПСС:arima_orders = [\n    (1, 1, 24),\n    (23, 1, 1),\n    (23, 1, 24),\n    (24, 1, 24),\n    (46, 1, 48)\n    ]Заметим, что в начале мы начинаем с порядков, смешанный процесс которых ограничен одним «сильным» процессом — 24 для процесса скользящего среднего и 23 для процесса авторегрессии. Это поможет лучше понять «природу прогнозирования» временного ряда с температурой.В дальнейших расчётах мы будем использовать модель SARIMAX библиотеки statsmodels, но поскольку на начальном этапе мы не учитываем сезонную компоненту и экзогенные регрессоры, то для несезонных порядков выполним модель ARIMA той же библиотеки.Сезонный порядок (P, D, Q, m) значительно увеличит время расчётов и потребление памяти, поэтому в целях экономии времени рассмотрение всех временных рядов я выношу за рамки данного исследования. В ходе дальнейших шагов мы рассмотрим только следующие из них, которые были выбраны выборочно («на глаз») с учётом разнообразия состояния (стационарное/нестационарное), без корреляционного анализа или анализа связи по тесту Грейнджера:colls = [\n    'p (mbar)',\n    'T (degC)',\n    'VPmax (mbar)',\n    'sh (g/kg)',\n    'rho (g/m**3)',\n    'wv (m/s)'\n    ]При этом в расчётах буду задействованы не все годы, а только первые четыре.    Нижеследующий график наглядно продемонстрирует разделение данных. Как можно догадаться, принимая эти ограничения мы хорошенько подготавливаемся к борьбе с вычислительной затратностью модели SARIMAX применительно к годовой сезонности в 8760 шагов.def show_plot_split_temperature():\n    \"\"\"График температуры, разграниченной по частям\"\"\"\n    fig, axs = pyplot.subplots(2, 1)\n\n    axs[0].plot(temperature['2009-01-01':'2011-12-31'], label='Обучающая часть')\n    axs[0].plot(temperature['2012-01-01':'2012-01-07'], label='Валидационная часть')\n    axs[0].plot(temperature['2012-01-08':'2012-01-14'], label='Тестовая часть')\n    axs[0].set_ylabel('Температура (град. C)')\n    axs[0].legend(loc='best')\n\n    axs[1].plot(temperature['2009-01-01':'2011-12-31'][-168:], label='Обучающая часть')\n    axs[1].plot(temperature['2012-01-01':'2012-01-07'], label='Валидационная часть')\n    axs[1].plot(temperature['2012-01-08':'2012-01-14'], label='Тестовая часть')\n    axs[1].set_xlabel('Дата наблюдения')\n    axs[1].set_ylabel('Температура (град. C)')\n    axs[1].legend(loc='best')\n\n    pyplot.suptitle('График температуры, разграниченной по частям')\n    pyplot.show()\n\n\nshow_plot_split_temperature()Период [2009-01-01 : 2011-12-31] включительно будет использоваться для обучения моделей, следующий за ним период [2012-01-01 : 2012-01-07] (то есть 7 дней) - для их валидации. Тестовая часть, охватывающая период [2012-01-08 : 2012-01-14] (следующие 7 дней после валидационной части), будет использоваться для сравнения с прогнозом окончательно определившейся модели по результатам отбора.Я дополнительно выполнил проверку на стационарность отобранных данных с учётом их сокращения согласно периоду [2009-01-01 : 2011-12-31]: отличий от ранее выполненных тестов - расширенного теста Дики-Фуллера и теста КПСС - не выявлено.for column in colls:\n    print('\\n'f\"Проверка стационарности временного ряда {column}\")\n    series_p_value_verification(df[column].loc[:'2011-12-31'])Целью данного исследования в части выполнения моделей является выполнение прогнозирования почасовых значений температуры на следующие 7 дней, что с учётом почасового формата данных составит горизонт прогнозирования, равный 168 наблюдениям.Оценивать модели мы будем с помощью двух основных подходов к оценке модели в прогнозировании временных рядов - предсказание в пределах выборки и прогнозирование вне выборки (в англоязычной терминологии in-sample prediction и out-of-sample forecasting соответственно). Чаще всего к данным, которые находятся в пределах выборки, относят как обучающие, так и валидационные данные, которые затем объединяются друг с другом для окончательного обучения модели, и уже после этого выполняется прогноз на основе последнего известного значения, который сравнивается с тестовыми данными (с вневыборочными данными).Данный подход к оцениванию модели хорошо объясняет разницу между понятиями «предсказать» и «спрогнозировать» применительно к временным рядам: предсказание — это оценка известных данных (обучающих и валидационных), а прогноз — это оценка новых данных (тестовых, как в нашем случае), которые не использовались при обучении модели.Это важно запомнить, поскольку по этой же логике выполнено несколько методов моделей ARIMA/SARIMAX библиотеки statsmodels. Методы predict() и get_prediction() применяются для выполнения внутривыборочных предсказаний, forecast() и get_forecast() — для выполнения вневыборочных прогнозов на основе последнего известного значения. При этом методы predict() и forecast() возвращают только предсказанные/спрогнозированные значения, а методы get_prediction() и get_forecast() позволяют получить «дополнительные результаты» [ссылка] - как предсказанные/спрогнозированные значения (mean), так и граничные значения интервала предсказания/прогнозирования для этих значений, но никак не доверительные интервалы, как, по всей видимости, ошибочно написано в документации statsmodels к этим методам.Доверительный интервал обычно используется для оценки надёжности параметров модели (далее мы убедимся в этом по итоговой таблице с результатами обученной модели), в то время как интервал прогнозирования охватывает диапазон возможных значений, которые могут быть получены при применении модели по отношению к новым данным. Поскольку метод get_forecast() предоставляет прогнозы для будущих наблюдений, то логично, что интервал, который он возвращает, должен отражать неопределённость этих прогнозов, а не оценок параметров модели.Возвращаясь к нашим данным, я предлагаю не выполнять предсказания обученной модели на обучающих данных, поскольку это будет неинформативно в плане обобщения: модели будут показывать очень высокий результат на обучающих данных и жуткий на валидационных данных, что связано с такой проблемой, как переобучение модели. В этой связи о качестве моделей мы будем судить как по информационному критерию Акаике AIC [ссылка на оригинальную статью] из результатов обученной модели, так и по ошибкам на валидационных данных с помощью метрики «Средняя абсолютная процентная ошибка» (в англоязычной терминологии - mean absolute percentage error, mape). Чем они меньше, тем лучше модель описывает данные. Тестовые данные будут использоваться только для сравнения с прогнозом, выполненного определившеюся моделью по результатам отбора.Итак, с помощью нижеприведённого кода мы для каждого порядка из списка arima_orders последовательно выполним следующие действия: обучим модель, выполним предсказания по меткам валидационных данных на 168 шагов и рассчитаем ошибку mape, а результаты (AIC, BIC, preds_mean и mape) сохраним в словарь results, который как результат функции build_arima_model() вложим в общий для всех порядков (=несезонных моделей) словарь sum_results. По ключам и элементам этого общего словаря мы будем оценивать эффективность выполнения предсказывания рассматриваемых моделей.train_data = temperature['2009-01-01':'2011-12-31'].copy()\nval_data = temperature['2012-01-01':'2012-01-07'].copy()\n\narima_orders = [\n    (1, 1, 24),\n    (23, 1, 1),\n    (23, 1, 24),\n    (24, 1, 24),\n    (46, 1, 48)\n    ]\n\nsum_results = {}\n\ndef build_arima_model(order):\n    \"\"\"Обучение модели ARIMA и сохранение её результатов\"\"\"\n    results = {}\n\n    arima_model = ARIMA(endog=train_data,\n                        order=order,\n                        trend=None,\n                        enforce_stationarity=False,\n                        enforce_invertibility=False)\n\n    arima_model = arima_model.fit()\n\n    preds = arima_model.get_prediction(start=val_data.index.min(),\n                                       end=val_data.index.max())\n\n    mape = mean_absolute_percentage_error(y_true=val_data,\n                                          y_pred=preds.predicted_mean)\n\n    results['AIC'] = arima_model.aic\n    results['BIC'] = arima_model.bic\n    results['preds_mean'] = preds.predicted_mean\n    results['MAPE'] = mape\n    return results\n\nfor order in arima_orders:\n    key = f\"ARIMA{order}\"\n    print(f\"Выполняется обучение модели {key}\")\n    sum_results[key] = build_arima_model(order)Завершив выполнение расчётов, объединим результаты и выведем их на общем графике с помощью следующей функции:def show_plot_prediction(model_dict):\n    \"\"\"График сравнения истинных значений с предсказанными\"\"\"\n    pyplot.figure()\n\n    pyplot.plot(val_data, label='Валидационные данные')\n    for key, model_data in model_dict.items():\n        predictions = model_data['preds_mean']\n        label_order = f\"{key} \"\n        label_mape = f\"[mape: {model_data['MAPE']*100:.2f}%] \"\n        label_aic = f\"[AIC: {int(model_data['AIC'])}]\"\n        pyplot.plot(predictions, label=(label_order + label_mape + label_aic))\n\n    pyplot.xlabel('Дата наблюдения')\n    pyplot.ylabel('Температура (град C)')\n    pyplot.title('Предсказания на валидационных данных')\n    pyplot.grid(True)\n    pyplot.legend(loc='best')\n    pyplot.show()\n\n\nshow_plot_prediction(model_dict=sum_results)По результатам видно, что модель ARIMA(1, 1, 24) показала наихудший результат из всех по критерию AIC, а предсказания этой модели представлены в виде практически прямой линии с минимальными колебаниями, которые наблюдаются только в начале предсказания.ARIMA(23, 1, 1) показала значительно лучший результат по AIC, но по mape она оказалась немного хуже. Предсказания этой модели уже не выглядят как постоянная прямая, а демонстрируют незначительные колебания. Совмещение процесса авторегрессии со значением 23 и процесса скользящего среднего со значением 24 в модели ARIMA(23, 1, 24) значительно повысило эффективность как по AIC, так и по mape. Предсказания модели стали более вариативными, учитывая подъёмы и спады в данных.ARIMA(24, 1, 24) показало немного лучший результат по AIC, чем ARIMA(23, 1, 24), но хуже по mape. Предсказания обеих моделей практически идентичны.Дальнейшее увеличение количества временных запаздываний, представленное в модели ARIMA(46, 1, 48) лишь незначительно улучшило AIC, но при этом ошибка mape ухудшилась по сравнению с моделями ARIMA(23, 1, 24) и ARIMA(24, 1, 24).Таким образом, заключаем, что модель ARIMA(23, 1, 24) оказалась наиболее подходящей для дальнейшего использования в силу баланса между минимизацией критериев оценки модели AIC и mape.Важно понимать, что использование модели ARIMA(23, 1, 24) фактически учитывает только последние 24 значения для предсказания текущего значения временного ряда. Это означает, что модель будет использовать последние сутки как основу для предсказания будущего часового значения температуры. Однако, несмотря на то что предсказания модели учитывают колебания, они недостаточно точно отражают валидационные данные и представлены в виде волнообразной линии. В этой связи можно предположить, что модель, не учитывающая годовую сезонную компоненту, не смогла воспроизвести валидационные данные в должной мере и, следовательно, необходимо пересмотреть параметры модели с учётом сильного влияния годовой сезонности на предсказания.Но сперва посмотрим на результаты обученной модели ARIMA(23, 1, 24), воспользовавшись удобными методами библиотеки statsmodels — summary() и plot.diagnostics() и проанализируем их. (Для этого я отдельно обучу модель ARIMA(23, 1, 24) и между делом зафиксирую, что для её выполнения потребуется 14-14.2 ГБ памяти согласно мониторингу TPU.)Итак, выведем итоговую информацию обученной модели с помощью метода summary(). Для улучшения читаемости воспользуемся функцией print(), чтобы отформатировать вывод и придать ему структурированный вид.print(arima_model.summary())Поскольку итоговая таблица на загляденье удобна и информативна, то ниже привожу описание её основных разделов, каждый из которых содержит важную информацию о модели и её параметрах. Для улучшения восприятия разделы таблицы выделены разным цветом.Основные разделы, на которые можно подразделить вывод, следующие:В итоговой таблице «поджабным» цветом подсвечены коэффициенты, p-значение которых меньше уровня значимости 0.05. Следовательно, для них мы отклоняем нулевую гипотезу, принимая, что эти коэффициенты статистически значимы. В связи с тем, что для процесса АР наиболее значимыми оказались шаги запаздывания 1 и 23, а для процесса СС - 22, 23 и 24, то это может означать, что во временном ряду с температурой имеется не только сильная сезонная структура, но и то, что текущее значение температуры зависит от предшествующих ему значений днём ранее. При этом надо признать, что наша модель ARIMA(23, 1, 24) смогла выделить эти значимые характеристики временного ряда и можно предположить, что она адекватно описывает данные (вспомните, мы приняли такие значения, чтобы учесть краткосрочные зависимости). Однако не будем спешить с выводами и посмотрим на результаты дополнительно выполненных статистических тестов, представленных в итоговой таблице.В дополнение к итоговой таблице выполним графическую диагностику остатков модели на нормальность распределения и автокорреляцию.arima_model.plot_diagnostics()\npyplot.show()По графику стандартизированных остатков (z-оценка) видно, что имеется множество как положительных, так и отрицательных значений, которые значительно отклоняются от нуля как среднего значения. Эти значения модель принимает за так называемые выбросы.На графике «квантиль-квантиль» представлено сравнение квантиля распределения остатков модели с квантилем теоретического нормального распределения в виде красной линии. Видно, что половине точек до лампочки эта «красная линия»: если в середине графика точки располагаются вдоль линии нормального распределения, то в крайних точках они сильно загибаются (имеются так называемые тяжёлые хвосты). Данное поведение говорит о том, что остатки модели избыточно рассеяны относительно среднего значения по сравнению с нормальным распределением и в них много выбросов [ссылка]. В целом, график «квантиль-квантиль» ясно показывает, что точки квантилей не лежат на теоретической нормальной кривой и, следовательно, данные распределены ненормально.Это подтверждает гистограмма остатков: вытянутая пика и немного узкое распределение остатков говорят об отклонении их распределения от теоретического нормального. При этом на графике с гистограммой не видно, но по результатам вышерассмотренных тестов мы должны помнить, что имеется как отрицательная асимметрия, так и неоднородная дисперсия остатков.Коррелограмма (график с АКФ) демонстрирует «быстрый спад» автокорреляционной функции и отсутствие значительной автокорреляции остатков; на графике нет значительных пик, которые бы указывали на необходимость пересмотра параметров модели (например, увеличения порядка разности параметра d или шагов процесса скользящего среднего).Впечатляет, правда?! Всего две строчки кода, не считая pyplot’а — и мы получили довольно полную диагностику модели! Единственное, чем хотелось бы расширить графическую диагностику — графиком с ЧАКФ и коробчатой диаграммой для остатков модели. Давайте выполним их отдельно.def dop_plot_diagnostics(lags=None):\n    \"\"\"ЧАКФ и коробчатая диаграмма остатков модели\"\"\"\n    fig, axs = pyplot.subplots(2, 1)\n    plot_pacf(residuals, lags=lags, ax=axs[0])\n    axs[0].set_title('Частная автокорреляционная функция')\n    axs[1].boxplot(residuals, vert=False)\n    axs[1].set_title('Коробчатая диаграмма')\n    pyplot.suptitle('Дополнительная графическая диагностика')\n    pyplot.show()\n\n\ndop_plot_diagnostics(lags=48)Пускай и не так заметно, но на графике с ЧАКФ выделяется 24-й шаг запаздывания, что указывает на необходимость пересмотра параметров модели применительно к увеличению шагов процесса авторегрессии. Согласно коробчатой диаграмме в остатках модели присутствует множество значительно удалённых точек (значений), преимущественно отрицательных, обуславливающих отрицательную асимметрию. Принимать эти значения за выбросы или нет, держа в уме тот аспект, что анализируется временной ряд с температурой, — это задача, требующая дополнительного изучения и применения специальных критериев. В данном исследовании мы не будем преобразовывать эти данные, а оставим как есть, считая, что они содержат важную информацию о динамике изменения температуры со временем.Таким образом, на основании вышеприведённых элементов оценки параметров модели и диагностической проверки её остатков можно сделать вывод, что модель ARIMA(23, 1, 24), сформулированная в качестве отправной точки, неэффективно подогнана к обучающим данным. Есть признаки автокорреляции и гетероскедастичности в остатках, а также их несоответствие нормальному распределению. Поэтому остатки модели не ведут себя как белый шум. Следовательно, необходимо пересмотреть модель.Что ж, если приведение распределения данных к нормальному мы вынесли за рамки нашего исследования и работаем с тем, что есть, то давайте попробуем избавиться от автокорреляции в остатках. Для этого воспользуемся подсказкой графика с ЧАКФ остатков предыдущей модели и увеличим количество шагов запаздывания процесса авторегрессии до 24. Затем обучим модель ARIMA(24, 1, 24) (потребуется 14.3 ГБ памяти согласно мониторингу TPU) и проверим, лишена ли эта модель автокорреляции в остатках или нет.По итоговой таблице видно, что коэффициент 24-го шага запаздывания процесса авторегрессии имеет низкое значение стандартной ошибки и статистически значим. Его применение в новой модели немного улучшило все три информационных критерия, а полученное p-значение для теста Льюнга-Бокса (0.26), которое больше уровня значимости 0.05, позволяет сделать вывод, что в остатках модели отсутствует автокорреляция. Таким образом, с допущениями мы можем говорить, что модель ARIMA(24, 1, 24) адекватно описывает данные. Однако необходимо помнить о результатах выполнения предсказаний на валидационных данных, которые показывали, что предыдущая модель плохо справлялась с их воспроизводством. Поэтому нам следует сконцентрироваться на внесении глобальных изменений в модель, а не заниматься тонкой настройкой параметров модели после диагностической проверки.Ничего не поделаешь, давайте приступим к определению сезонного порядка (P, D, Q, m) для модели SARIMAX... Но уже в самом начале нас поджидает неприятность в виде длительной отрисовки графиков АКФ и ЧАКФ с количеством шагов запаздывания, равным 8760*2 и более. (Прошу поберечь помидоры: хоть я и держал интригу до последнего, планируя представить хотя бы код для выполнения расчётов с данным годовым периодом, но длительность отрисовки (более 3-х часов ожидания) вконец «добила» эту затею.) Поэтому я настойчиво предлагаю не издеваться над компьютером, выполняя расчёты для сезонного параметра m=8760, и ограничиться теоретическими правилами (алгоритмом) нахождения параметров, на изучение и систематизацию которых я вложил немало времени.Не берусь останавливать отчаянных последователей, которые захотят испытать свой компьютер; только предупрежу, что m=200 и более считается непосильной задачей для любого компьютера [ссылка] из-за обработки и временного хранения больших массивов данных (форма состояния пространства и применение фильтра Калмана для оценивания модели, матрицы ковариаций и пр.), требующих значительных ресурсов. Эта проблема известна, и разработчики библиотеки statsmodels предлагают решения для сокращения потребления памяти [ссылка], допуская различные ограничения, чтобы ускорить обработку. Но даже с этими настройками наш с вами сезонный годовой период в 8760 шагов не по зубам массовым компьютерам. Однако не стоит опускать руки: дальше мы применим общеизвестный метод, который позволяет моделировать сезонность любой длины и их любое количество, но об этом сильно далее и в своё время.Итак, алгоритм определения порядков модели SARIMAX(p, d, q)(P, D, Q, m) сводится к следующему. Сначала начинают с определения параметров сезонного порядка. И первым делом, как по методологии АПРСС, рассмотренной в первой части главы 2, идентифицируют сезонный порядок разности D, чтобы устранить сезонность в данных. Поскольку у временного ряда температуры имеется явная сезонность, то выполняя дифференцирование временного ряда на период, равным годовой сезонности (8760), мы тем самым определяем D=1. Далее определим несезонный порядок разности d. Поскольку D\u003e0, то выполним тест КПСС на дифференцированном (период=8760) временном ряду. Так как годовая сезонность устраняется с помощью параметра разности D, то тест КПСС проверяет стационарность временного ряда относительно линейной тенденции (regression='ct') и, если она имеется, устраняем её с помощью параметра d=1. При этом часто отмечается, что сумма порядков разности d и D не должна превышать значение 2.Давайте запустим тест КПСС для определения значения параметра d:def seasonal_trend_kpss_verification(series, periods=None):\n    \"\"\"Проверка ряда с сезонной разностью на стационарность вокруг тенденции\"\"\"\n    if periods:\n        name = series.name\n        series = series.diff(periods=periods).dropna()\n        series.rename(name + ' diff=' + str(periods), inplace=True)\n\n    msg = \"Тест КФШШ для проверки стационарности вокруг тенденции (ct):\"\n    print('\\n' + msg)\n    print('-' * len(msg))\n\n    kpss_result = kpss(series, regression='ct')\n    if kpss_result[1] \u003e 0.05:\n        h_text = colored('стационарен', 'grey', 'on_green')\n        print(f\"Ряд {series.name} {h_text}: p-value = {kpss_result[1]:.5f}\")\n    else:\n        h_text = colored('нестационарен', 'grey', 'on_red')\n        print(f\"Ряд {series.name} {h_text}: p-value = {kpss_result[1]:.5f} \u003c 0.05\")\n\n\nseasonal_trend_kpss_verification(temperature, periods=8760)Результат представлен ниже.По результатам теста видно, что предыдущее взятие сезонной разности не устранило тенденцию в данных. Необходимо устранить её с помощью параметра разности d, установив ему значение 1. Следовательно, мы принимаем параметры разности равными D=1 и d=1.Для определения оставшихся параметров воспользуемся следующей функцией, которая построит график временного ряда температуры с разностью в годовую сезонность (период=8760), а также графики с АКФ и ЧАКФ с временным запаздыванием, равным 240 шагам. Такое количество шагов не требует длительного времени построения графиков и позволит понять «природу» как сезонных порядков P, Q, так и несезонных p, q. При этом я дополнительно отмечу на графике возможное поведение частной автокорреляционной функции на шагах 8760, 17520 для понимания того, как определяются сезонные параметры P и Q.def plot_seasonal_order_analysis(series, periods=8760, lags=None):\n    \"\"\"График для определения параметров (P, Q), (p, q)\"\"\"\n    fig, axs = pyplot.subplots(3, 1)\n\n    series = series.diff(periods=periods).dropna()\n    axs[0].plot(series)\n    axs[0].set_title(f\"Ряд с разностью {periods}-го порядка\")\n    plot_acf(series, ax=axs[1], lags=lags)\n    axs[1].set_title(f\"АКФ ({periods}-й порядок)\")\n    plot_pacf(series, ax=axs[2], lags=lags)\n    axs[2].set_title(f\"ЧАКФ ({periods}-й порядок)\")\n\n    pyplot.show()\n\n\nplot_seasonal_order_analysis(temperature, lags=240)Итак, согласно этим графикам и таблице 3.2 (из книги Дж.Бокса и Г.Дженкинса) мы можем охарактеризовать ряд с разностью в годовой порядок как процесс авторегрессии: автокорреляционная функция бесконечна и представлена в виде затухающей экспоненты, а частная автокорреляционная функция конечна и обрывается после 2-х шагов запаздывания с появлением всплесков на шагах 8760 и 17520. Следовательно, сезонный параметр авторегрессии P может иметь значения 2 и 1 (по всплескам). Сезонный параметр скользящего среднего Q принимаем равным 0: никакие предыдущие сезонные остатки в модели не учитываются.На том же графике с ЧАКФ видно, что два начальных шага представлены в виде обособленной пики: эти шаги (2 и 1) мы примем в качестве значений параметра авторегрессии p. Поскольку ранее мы обнаружили, что процесс скользящего среднего оказывает влияние на результат предсказаний, то примем q равным 1.Таким образом, по результатам анализа вышерассмотренных графиков АКФ и ЧАКФ для временного ряда температуры с сезонной годовой разницей в 8760 шагов мы определили следующие возможные конфигурации модели SARIMAX:(2-1, 1, 1)(2-1, 1, 0, 8760)Интерпретация данной модели следующая. Текущее часовое значение температуры зависит от двух (либо одного - в зависимости от установленного значения p) предыдущих часов, учитывая один предыдущий остаток (ошибку), и от двух (либо одного - в зависимости от установленного значения P) значений температуры в аналогичные часы из позапрошлого (или прошлого) года.Не будь мы ограничены в вычислительных ресурсах, то проверить как сильно влияет годовая сезонность на прогнозирование не составило бы труда: «нажми на кнопку — получишь результат».И тем не менее существуют известные методы, среди которых анализ Фурье, являющийся частью спектрального анализа, которые позволяют обойти ограничение длины сезонной компоненты. Это направление очень интересное и заслуживает отдельного рассмотрения в третьей, завершающей части второй главы. А пока разработаем модель SARIMAX с «адекватным» сезонным параметром применительно к вычислительным ресурсам, значение которого установим равным дневной сезонности (m=24). Тем самым мы учтём кратковременные зависимости с помощью сезонного параметра. А годовую сезонность попробуем учесть с помощью имеющихся экзогенных переменных.Чтобы усилить интерес, предлагаю подбор оптимальной конфигурации модели SARIMAX выполнить с помощью мощного инструмента под названием авто-АРПСС. Наиболее известной python-библиотекой, предоставляющей возможность автоматического перебора параметров модели АРПСС, является pmdarima [ссылка]. Однако в этом исследовании я предлагаю воспользоваться библиотекой под названием statsforecast, которая, как заявляют её разработчики, в 20 раза быстрее, чем pmdarima (скомпилирована для выполнения высокопроизводительных расчётов с использованием numba [ссылка]).Методы с автоматическим перебором параметров АРПСС обеих библиотек заимствованы из пакета forecast языка программирования R. Этот пакет, в свою очередь, опирается на систематизацию более ранних исследований и решений, направленных на автоматизацию процесса перебора с использованием программного обеспечения TRAMO, SEATS, Autobox и других подобных инструментов. Об этом справедливо упоминают авторы статьи «Автоматическое прогнозирование временных рядов: пакет forecast для языка R» (2008) [ссылка].Рекомендую ознакомиться с этой работой, чтобы уверенно ориентироваться в разделах с инструментом auto-ARIMA различных python-библиотек. Я лишь кратко отмечу отличия от «ручного» метода определения параметров, который мы с вами применяли ранее для определения несезонных и сезонных порядков модели АРПСС/САРПСС.Итак, если выбор как несезонных, так и сезонных параметров процессов АР и СС (p, q, P и Q) осуществляется путём минимизации информационного критерия Акаике (AIC) (либо информационного критерия Байеса, BIC), то определение параметров разности выполняется с помощью различных статистических тестов. В отличие от python, в языке R эти тесты давно реализованы и широко используются. В частности, auto.arima() для определения параметра сезонной разницы D использует функцию nsdiffs(), которая охватывает несколько тестов [ссылка]. После того, как параметр D установлен, с помощью другой функции — ndiffs() -  определяют параметр d. Данная функция использует тест КПСС по умолчанию [ссылка] и применяется к ряду либо с сезонной разницей (если D=1), либо к исходному (если D=0).С помощью следующего кода мы запустим автоматический перебор параметров для поиска наилучшей модели SARIMA с длиной сезона в 24 шага (season_length=24):from statsforecast.models import AutoARIMA\n\ntrain_data = temperature['2009-01-01':'2011-12-31'].copy()\nval_data = temperature['2012-01-01':'2012-01-07'].copy()\n\nmodel = AutoARIMA(season_length=24, seasonal=True, trace=True)\nmodel = model.fit(y=train_data.to_numpy())Результаты представлены ниже.ARIMA(2,0,2)(1,1,1)[24]                   :inf\n\nARIMA(0,0,0)(0,1,0)[24]                   :139845.93882574368\n\nARIMA(1,0,0)(1,1,0)[24]                   :62793.959059700705\n\nARIMA(0,0,1)(0,1,1)[24]                   :107428.13870839613\n\nARIMA(1,0,0)(0,1,0)[24]                   :68380.80916728784\n\nARIMA(1,0,0)(2,1,0)[24]                   :60514.528469509656\n\nARIMA(1,0,0)(2,1,1)[24]                   :inf\n\nARIMA(1,0,0)(1,1,1)[24]                   :inf\n\nARIMA(0,0,0)(2,1,0)[24]                   :138470.59089873798\n\nARIMA(2,0,0)(2,1,0)[24]                   :54144.152318747736\n\nARIMA(2,0,0)(1,1,0)[24]                   :56682.81810435849\n\nARIMA(2,0,0)(2,1,1)[24]                   :inf\n\nARIMA(2,0,0)(1,1,1)[24]                   :inf\n\nARIMA(3,0,0)(2,1,0)[24]                   :inf\n\nARIMA(2,0,1)(2,1,0)[24]                   :54143.24675833847\n\nARIMA(2,0,1)(1,1,0)[24]                   :56682.01262490523\n\nARIMA(2,0,1)(2,1,1)[24]                   :inf\n\nARIMA(2,0,1)(1,1,1)[24]                   :inf\n\nARIMA(1,0,1)(2,1,0)[24]                   :55008.69589543988\n\nARIMA(3,0,1)(2,1,0)[24]                   :54102.41679797225\n\nARIMA(3,0,1)(1,1,0)[24]                   :56643.717496704645\n\nARIMA(3,0,1)(2,1,1)[24]                   :inf\n\nARIMA(3,0,1)(1,1,1)[24]                   :inf\n\nARIMA(4,0,1)(2,1,0)[24]                   :54055.97547555753\n\nARIMA(4,0,1)(1,1,0)[24]                   :56595.9640338018\n\nARIMA(4,0,1)(2,1,1)[24]                   :inf\n\nARIMA(4,0,1)(1,1,1)[24]                   :inf\n\nARIMA(4,0,0)(2,1,0)[24]                   :54056.11095450652\n\nARIMA(5,0,1)(2,1,0)[24]                   :53941.59162502506\n\nARIMA(5,0,1)(1,1,0)[24]                   :56441.562388514416\n\nARIMA(5,0,1)(2,1,1)[24]                   :inf\n\nARIMA(5,0,1)(1,1,1)[24]                   :139861.94553157385\n\nARIMA(5,0,0)(2,1,0)[24]                   :inf\n\nARIMA(5,0,2)(2,1,0)[24]                   :inf\n\nARIMA(4,0,2)(2,1,0)[24]                   :54057.93584322888\n\nNow re-fitting the best model(s) without approximations...\n\n\nARIMA(5,0,1)(2,1,0)[24]                   :53941.59162502506 В результате автоматического перебора параметров с помощью AutoARIMA лучшей моделью по минимизации AIC оказалась модель SARIMA(5, 0, 1)(2, 1, 0, 24). Согласно документации [ссылка] параметры авторегрессии p и P достигли своих максимальных значений, установленных по умолчанию, — max_p=5 и max_P=2 соответственно. Данное обстоятельство должно подвергнуть критике подобранную модель: в наших данных действительно имеется сильный авторегрессионный процесс, но ограничивается ли он теми максимальными значениями по авто-АРПСС?Постройте для временного ряда температуры графики с АКФ и ЧАКФ с сезонной разностью m=24, а затем проверьте, можно ли включить большее число шагов запаздывания в процессы авторегрессии. Возможно, кто-нибудь из вас пойдёт дальше — и обучит модель SARIMA подобранной (возможно, более подходящей) конфигурации с помощью библиотеки statsmodels на TPU google.colab (потребуется более 20 ГБ памяти) и проанализирует её результаты. Делитесь ими в комментариях, с интересом обсудим!А мы тем временем попробуем учесть годовую сезонность с помощью имеющихся экзогенных переменных.Напоминаю, экзогенные переменные используются для учёта внешних воздействий (внешних факторов) с целью более точного и адекватного описания поведения эндогенного временного ряда. В моделях ARIMAX/SARIMAX экзогенные переменные рассматриваются как ковариаты, которые добавляются в модель наряду с эндогенной переменной. В отличие от эндогенной переменной к ним не применяют операторы разности, и они не подвергаются преобразованию, а используются в исходном виде. В этом можно убедиться по формулам моделей ARIMAX/SARIMAX [ссылка].Предлагаю провести интересный эксперимент, в котором одна и та же сезонная модель будет тестироваться с разной экзогенной переменной. В первом случае будем использовать температуру в исходном виде (без преобразований), а во втором — температуру с разностью первого порядка (как мы обнаружили ранее, временной ряд с температурой нестационарен по тесту КПСС). Выполняя предсказания на валидационных данных, мы проверим, какая их этих двух экзогенных переменных способна улучшить результат и, следовательно, наиболее значима. На основании полученных результатов сделаем вывод о целесообразности применения оператора разности к нестационарной экзогенной переменной.С помощью нижеприведённого кода мы сначала выделим обучающую и валидационную части для эндогенного параметра модели, определим отдельную переменную с дифференцированной температурой, а затем на их основе сформируем экзогенный параметр, который будет состоять из двух вышерассмотренных элементов (столбцов). Далее для каждого экзогенного столбца обучим одну и ту же модель SARIMAX, выполним предсказания по меткам валидационных данных на 168 шагов с учётом валидационной части экзогенной переменной, рассчитаем ошибку mape и сохраним результаты в словарь results. Этот словарь как результат функции build_sarimax_model() вложим в общий для обеих сезонных моделей словарь sarimax_sum_results, по ключам и элементам которого с помощью функции show_sarimax_temp_plot_prediction() будем оценивать эффективность выполнения предсказания моделей.Так как при использовании экзогенных переменных в модели SARIMAX библиотеки statsmodels возникает ошибка при обучении (экзогенные параметры фактически игнорируются), мы будем применять модель ARIMA. С учётом использования сезонного порядка и экзогенных переменных эта модель будет эквивалентна SARIMAX.#from statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima.model import ARIMA\n\nhorizont = 168\n\nendog_train = temperature['2009-01-01':'2011-12-31'].copy()\nendog_val = temperature['2012-01-01':'2012-01-07'].copy()\n\ntemperature_diff = temperature.diff().fillna(0)\n\nexog_train = endog_train.to_frame().copy()\nexog_val = endog_val.to_frame().copy()\n\nexog_train[f\"{exog_train.columns[0]}_diff\"] = temperature_diff['2009-01-01':'2011-12-31'].copy()\nexog_val[f\"{exog_val.columns[0]}_diff\"] = temperature_diff['2012-01-01':'2012-01-07'].copy()\nsarimax_sum_results = {}\n\ndef build_sarimax_model(colls):\n    \"\"\"\n    Обучение модели SARIMAX с экзогенными переменными и\n    сохранение её результатов\n    \"\"\"\n    order=(5, 0, 1)\n    seasonal_order=(2, 1, 0, 24)\n\n    results = {}\n\n    sarimax_model = ARIMA(endog=endog_train,\n                          exog=exog_train[colls],\n                          order=order,\n                          seasonal_order=seasonal_order,\n                          trend=None,\n                          enforce_stationarity=False,\n                          enforce_invertibility=False)\n\n    sarimax_model = sarimax_model.fit()\n\n    preds = sarimax_model.get_prediction(start=endog_val.index.min(),\n                                         end=endog_val.index.max(),\n                                         exog=exog_val[colls])\n\n    mape = mean_absolute_percentage_error(y_true=endog_val,\n                                          y_pred=preds.predicted_mean)\n\n    results['AIC'] = sarimax_model.aic\n    results['BIC'] = sarimax_model.bic\n    results['residuals'] = sarimax_model.resid\n    results['preds_mean'] = preds.predicted_mean\n    results['MAPE'] = mape\n    return results\n\nfor exoq_col in exog_train:\n    key = f\"\\'{exoq_col}\\'\"\n    print('\\n' f\"Обучение модели SARIMAX с экзоген.переменной {key}\")\n    sarimax_sum_results['SARIMAX[24] ' + key] = build_sarimax_model(exoq_col)После завершения расчётов выведем результаты обеих моделей на общем графике с помощью следующей функции:def show_sarimax_temp_plot_prediction(model_dict):\n    \"\"\"График сравнения истинных значений с предсказанными\"\"\"\n    pyplot.figure()\n\n    pyplot.plot(endog_val, label='Валидационные данные')\n    for key, model_data in model_dict.items():\n        predictions = model_data['preds_mean']\n        label_model = f\"{key} \"\n        label_mape = f\"[mape: {model_data['MAPE']*100:.2f}%] \"\n        label_aic = f\"[AIC: {int(model_data['AIC'])}]\"\n        pyplot.plot(predictions, dashes=[4, 3],\n                    label=(label_model + label_mape + label_aic))\n\n    pyplot.xlabel('Дата наблюдения')\n    pyplot.ylabel('Температура (град C)')\n    pyplot.title('Предсказания на валидационных данных: сравнение exoq и exoq_diff')\n    pyplot.grid(True)\n    pyplot.legend(loc='best')\n    pyplot.show()\n\n\nshow_sarimax_temp_plot_prediction(model_dict=sarimax_sum_results)График показывает, что модель SARIMAX с нестационарной экзогенной переменной идеально точно воспроизвела валидационную выборку, ошибка mape равна нулю. Напротив, модель SARIMAX с предварительно продифференцированной экзогенной переменной показала значительно худшие результаты, ошибка mape подскочила до 56%.Таким образом, предсказания на валидационных данных показали, что предварительная обработка экзогенной переменной привела к снижению точности. Это подтверждает, что экзогенные переменные следует использовать в исходном виде.Давайте рассмотрим коэффициенты обеих экзогенных переменных, которые мы получим благодаря дополнительно выполненной модели SARIMAX.Оба коэффициента имеют p-значение, равное 0, что подтверждает их статистическую значимость при любых стандартных уровнях значимости. Коэффициент первого параметра, выделенный зелёным цветом, равен 1, что указывает на его положительное влияние на модель, которое подтверждается высокой точностью и уверенностью в значениях, поскольку доверительный интервал совпадает в верхней и нижней границах и равен 1. Как отмечается в литературе по прогнозированию, это делает его интерпретацию «интуитивно понятной и предпочтительной».Коэффициент второго параметра обладает отрицательным «околонулевым» значением (если быть точным, его значение в стандартной форме равно -0.00000188), что говорит о его незначительном отрицательном влиянии, которое подтверждается менее точным доверительным интервалом с отрицательными границами, что менее удобно для интерпретации.Какому из этих коэффициентов вы бы отдали предпочтение? Вероятнее всего первому из-за его положительного влияния и понятной интерпретации. Таким образом, рассмотрение коэффициентов также подтверждает, что экзогенный фактор лучше оставлять без изменений.А теперь давайте ещё раз посмотрим на график, на котором модель SARIMAX с нестационарной экзогенной переменной идеально точно воспроизвела валидационную выборку. Использование экзогенных параметров в прогнозных моделях накладывает важное ограничение, связанное с необходимостью предоставления соответствующей экзогенной информации во время выполнения предсказаний или прогнозирования. Другими словами, применение экзогенных данных подразумевает, что для выполнения предсказаний или прогнозов с помощью обученной модели также необходимо использовать соответствующие экзогенные данные (валидационные или тестовые). Это важно учитывать.А если у нас нет валидационных экзогенных данных и мы «просто хотим выполнить прогноз»? Для решения этой проблемы наиболее распространённым подходом является использование последних n-строк обучающих экзогенных данных. Однако нужно учитывать, что в этом случае нарушается временная структура (последние наблюдения могут сильно отличаться от предыдущих) и это может привести к снижению точности предсказаний (прогнозов).В этом легко убедиться благодаря следующему эксперименту. Обратите внимание, мы будем использовать несколько временных рядов в качестве экзогенных параметров (переменная colls), а их валидационная часть будет соответствовать последним 168 наблюдениям обучающей части (переменная exog_val).from statsmodels.tsa.arima.model import ARIMA\n\nhorizont = 168\n\ncolls = [\n    'p (mbar)',\n    'T (degC)',\n    'VPmax (mbar)',\n    'sh (g/kg)',\n    'rho (g/m**3)',\n    'wv (m/s)'\n    ]\n\nendog_train = temperature['2009-01-01':'2011-12-31'].copy()\nendog_val = temperature['2012-01-01':'2012-01-07'].copy()\n\nexog_train = df[colls].loc['2009-01-01':'2011-12-31'].copy()\nexog_val = exog_train.iloc[-horizont:, :].copy()\n\norder=(5, 0, 1)\nseasonal_order=(2, 1, 0, 24)\n\nsarimax_model = ARIMA(endog=endog_train,\n                      exog=exog_train,\n                      order=order,\n                      seasonal_order=seasonal_order,\n                      trend=None,\n                      enforce_stationarity=False,\n                      enforce_invertibility=False)\n\nsarimax_model = sarimax_model.fit()\n\npreds = sarimax_model.get_prediction(start=endog_val.index.min(),\n                                     end=endog_val.index.max(),\n                                     exog=exog_val)\n\nmape = mean_absolute_percentage_error(y_true=endog_val,\n                                      y_pred=preds.predicted_mean)\n\ndef show_sarimax_plot_exog_train_prediction():\n    \"\"\"График сравнения истинных значений с предсказанными\"\"\"\n    pyplot.figure()\n\n    pyplot.plot(endog_val, label='Валидационные данные')\n\n    label_mape = f\" [mape: {mape*100:.2f}%]\"\n    pyplot.plot(preds.predicted_mean, dashes=[4, 3],\n                label='Предсказанные данные '+ label_mape)\n\n    pyplot.xlabel('Дата наблюдения')\n    pyplot.ylabel('Температура (град C)')\n    pyplot.title('Предсказания на валидационных данных с exog_train[-history:, :]')\n    pyplot.grid(True)\n    pyplot.legend(loc='best')\n    pyplot.show()\n\nshow_sarimax_plot_exog_train_prediction()График показывает, что применение данного подхода позволило отразить годовые изменения температуры, однако точность предсказаний существенно снизилась: ошибка mape на валидационных данных увеличилась до 78%.Существуют ли методы, которые позволили бы нам улучшить точность предсказаний? Да, такие методы существуют, и одним из них является преобразование Фурье. В третьей, завершающей, части мы с вами, дорогие друзья, перейдём к более глубокому анализу целевого временного ряда - и применим некоторые методы спектрального анализа, чтобы точнее предсказать будущие значения температуры, минимизируя влияние шумов и случайных колебаний.",
  "О хакерах из Shedding Zmiy из первых рук. Интервью с Геннадием Сазоновым и Антоном Каргиным из Solar 4RAYS ГК «Солар»": "В преддверии SOC Forum 2024 давайте вспомним другое значимое событие — Positive Hack Day 2. Ранее уже выходило на Хабре интервью о первых шагах в обнаружении атаки, тоже посвящённое выступлению с этого мероприятия. На этот раз хочу представить интервью с инженером группы расследования инцидентов Solar 4RAYS Геннадием Сазоновым и экспертом группы анализа ВПО Solar 4RAYS Антоном Каргиным. Мне было интересно узнать из первых рук об одной из обнаруженных группировок. На PHD 2 Антон и Геннадий выступали с докладом, посвящённым группировке Shedding Zmiy, так что для полноты картины его стоит посмотреть. Приятного чтения!Как вы впервые нашли следы этой группировки?Антон: Когда мы начали расследование инцидентов, у нас не было четкого понимания ситуации. Мы находили различные индикаторы, и первым из них стал вредонос CobIint, ассоциирующийся с группировкой Cobalt, которая известна как минимум с 2016 года. Мы выделили Shedding Zmey как отдельный кластер, не связывая его напрямую с Cobalt или ex(Cobalt).Геннадий: В процессе исследования мы обнаружили уникальные вредоносные программы и техники, характерные только для этой группировки. Двигаясь от кейса к кейсу и собирая новые артефакты — IP‑адреса, сетевую инфраструктуру, используемые техники и особенности, — мы смогли объединить всю информацию в одну историю, которую назвали Shedding Zmiy.Это был какой‑то рядовой случай у кого‑то из ваших заказчиков, правильно?Геннадий: Да, первый случай оказался достаточно обычным, но он привлек внимание общественности, так как злоумышленники опубликовали данные, украденные у жертвы. Именно поэтому нас пригласили на расследование. Это было наше первое столкновение с этой группировкой, и мы ещё не знали всех деталей о ней.А потом у вас было несколько инцидентов, верно?Геннадий: Однозначно. У нас было несколько инцидентов, которые в итоге мы связали в одну цепочку. Первый произошел в декабре 2022 года, а последний, о котором мы рассказывали, был в феврале 2024 года. Таким образом, мы следили за этой группировкой в течение полутора лет. Кроме того, данные, обнаруженные Антоном, указывают на кастомный загрузчик и вредонос, использовавшиеся в фишинговых письмах и уходящие корнями в март 2022 года. Мы видим следы активности группировки более чем за два года.А коллеги из других компаний вам помогали в поисках этой группировки?Геннадий: Мы не обменивались данными с коллегами из других компаний, но внимательно следили за их отчетами и исследованиями. Мы изучали, как они описывают схожие группировки, упомянутые на одном из наших слайдов, и фиксировали связи с нашей группировкой. Читая их материалы, мы обязательно учитывали общие моменты в своей работе.Получается, у коллег из других ИБ‑компаний не было смежных следов этой группировки?Геннадий: Все коллеги относили наблюдаемые инциденты к своим старым группировкам. Некоторые связывали следы с ex-(Cobalt), другие указывали на такие группировки, как Shadow\\Comet, Twelve. Важно отметить, что каждый интерпретировал ситуацию по‑своему. Некоторые наши коллеги фиксировали больше случаев шифрования и деструктивного воздействия, в то время как мы заметили изменение мотивации в сторону кибершпионажа.Правильно я понимаю, что это больше хактивисты, а не группировка, заточенная на кибершпионаж и финансовое обогащение?Антон: Мы не стали бы называть их хактивистами. Важно отметить, что по инструментам, которые мы анализировали, было обнаружено несколько уникальных образцов, о которых не писали наши коллеги. Уровень их активности достаточно высок. Некоторые экземпляры вредоносного ПО эксплуатировали уязвимость десятилетней давности, были впервые замечены в атаках на Россию и были заточены на максимальную скрытность. Это нас удивило. Вряд ли обычные хактивисты демонстрировали бы такой высокий уровень подготовки.Получается, Shedding Zmiy занимается кибершпионажем, но неизвестно с какой целью?Геннадий: Вопросы атрибуции всегда сложны, и мы подходим к ним с осторожностью. Мы изложили свое видение, и один из зрителей подошел к нам, шепотом спросив о конкретной стране. Он был достаточно близок к истине. Хотя мы не будем озвучивать название страны, думаю, тем, кто разбирается в теме, станет понятно, о чем идет речь. Антон: Мы знаем о некоторых атаках на подрядчиков, работающих с крупными государственными компаниями. Вероятно, целью таких атак могло быть проникновение в инфраструктуры государственных учреждений для получения конфиденциальной информации. Однако в нашей практике таких случаев зафиксировано не было.В Даркнете вы пытались проследить эту группировку?Геннадий: Мониторинг даркнет‑каналов не совсем наша область. Этим занимается наше отдельное подразделение Solar AURA, специализирующееся на отслеживании подобных угроз. Пока от них не поступало конкретной информации, но мы постоянно обмениваемся данными. Мы отмечали, что группировка Shedding Zmiy часто приобретает специализированное программное обеспечение на подпольных форумах и использует его в своих атаках, особенно на начальных стадиях.Антон: Прямых контактов с ними у нас не было. Единственное упоминание, связанное с угрозами в наш адрес, было удалено вскоре после публикации. Активных действий против нас не предпринималось, и их цели, вероятно, лежат в другой плоскости.Мы еще не раз услышим об этом кластере? Каков ваш прогноз? Они продолжат действовать или уйдут в тень после вашего отчёта?Геннадий: С высокой долей вероятности они не изменят своей мотивации и стремления. Группировка атакует множество организаций, и семь случаев, которые мы описали, — это лишь те инциденты, в которых мы участвовали. Вероятно, пострадавших компаний гораздо больше: кто‑то не знает о взломе, а кто‑то предпочитает не сообщать об этом. Антон: Мы также наблюдаем значительное развитие их инструментов. Они постепенно совершенствуют свои техники. Например, сначала мы видели загрузчики, написанные на одном языке, а затем появился более сложный загрузчик XDHijack. Это можно рассматривать как переход от NIM к XDHijack. Их сложный фреймворк Bad State также развивается — мы видели несколько его версий, что свидетельствует о его активной разработке. Исходя из этого, можно сделать вывод, что у них долгоиграющие планы.Геннадий: Что касается названия, они сами себя так не называют. У нас есть определённые правила обозначения группировок. В блоге 4RAYS вышел пост, где мы подробно разбераем, как формируются имена для кибергруппировок. Мы объясняем, почему выбираются определённые названия и показываем наш подход к классификации угроз и его соответствие международной практике.Антон: Кибергруппировки редко сами себе дают имена. Группировки Shadow и Twelve, например, открыто заявляли о своих атаках в записках о выкупе и в своих Telegram‑каналах. Но чаще их названия придумывают специалисты по кибербезопасности. В результате одни и те же группировки могут иметь разные названия у разных компаний. Возможно, у них есть собственные внутренние имена, но они не разглашают их для усложнения идентификации.Геннадий: Мы отметили, что группировка Shedding Zmiy связана с другими киберпреступными структурами. Некоторые из них даже используют собственные публичные каналы, например, группировка BlackJack с одноимённым Telegram‑каналом. А с какой долей вероятности эта группировка прогосударственная?Геннадий: Вероятность этого весьма низкая. Отдельные публично активные члены группировки сообщали о передаче некоторой выгруженной из сетей жертв информации для нужд государственных служб и ведомств конкретной страны, но полностью спонсируемые государством группировки держатся в тени и никак не комментируют свои активности публично. Некоторые группы, как показывает практика, могут быть самофинансируемыми, что объясняет их публичные сборы на оборудование и технические средства — такая открытость зачастую является отличием от деятельности спонсируемых государством групп, которые обычно обладают стабильным финансированием и ресурсами. Также мотивированная команда может формироваться вокруг идеологических целей, которые часто сравнимы с государственной повесткой, но не обязательно совпадают. Вот таким получился наш разговор. Думаю, у меня будут и другие материалы на тему обнаружения хакерских группировок и выявления хакерских атак. Кстати, у меня есть материал про первые шаги в поиске хакерского присутствия на предприятии. Так что вкупе с материалом про Shedding Zmiy получается интересная картина. Спасибо за прочтение!",
  "Подельники или подражатели? Подробности атак группировки PhaseShifters": "С весны 2023 года мы в отделе исследования киберугроз активно наблюдаем за одной любопытной группировкой — PhaseShifters. Мы решили назвать ее так, потому что она меняет свои техники вслед за другим группировками, например UAC-0050 и TA558. А phase shift (в переводе с англ. — «сдвиг фазы»), как известно, — разность между начальными фазами двух величин, изменяющихся во времени периодически с одинаковой частотой.В своих атаках PhaseShifters не отличаются оригинальностью: используют фишинг от имени официальных лиц с просьбой ознакомиться с документами и их подписать. А эти документы, в свою очередь, защищены паролем и (что предсказуемо) заражены.В качестве ВПО злоумышленники используют Rhadamanthys, DarkTrack RAT, Meta Stealer — часть этих инструментов мы заметили у UAC-0050. А использованные криптеры были те же, что и у группировок TA558 и Blind Eagle.Подробнее о том, кто у кого списал, читайте под катом, а полный текст исследования можно найти в нашем блоге на сайте. С чего все начиналосьВ конце июня 2024 года мы обнаружили архив с названием Копия трудовой.docx.rar. Он распространялся в качестве вложения в письмах различным компаниям на территории России. Архив был защищен паролем, который, предположительно, можно было найти в теле письма. Файл из архива (Копия трудовой.docx.exe) выгружал в систему несколько файлов, один из которых был с провокационным названием putin_***.exe, а также документ-приманку в виде фото паспорта гражданина Республики Беларусь.Документ-приманка с паспортомФайл с провокационным названием запускал PowerShell-скрипты, которые получали картинки с загрузчиком ВПО и полезную нагрузку последней стадии из репозиториев Bitbucket.По первой ссылке (bitbucket[.]org/hgdfhdfgd/test/downloads/new_image[.]jpg?14441723) можно было найти репозиторий с файлами и картинками со стеганографией.На картинках была пустыня Атакама в Чили.Изображение с пустыней, скрывающее вредоносный скрипт  Второй Bitbucket-репозиторий (bitbucket[.]org/fasf24124/fdgfytrj/downloads) до сих пор функционирует и хранит 46 файлов с разными названиями, но с одинаковым содержанием.Репозиторий создан 4 марта 2024 года пользователем с никнеймом siniy34240. В цепочке атак использовался болгарский С2-сервер, который связан с другими подобными атаками и такими файлами, как:ru***.exe***pen**.exevvp_***.exeПоследняя полезная нагрузка в цепочке атаки — троян удаленного доступа DarkTrack RAT, который использовался группировкой в атаках 2023 и 2024 годов. Помимо DarkTrack RAT, в репозиториях находились такие семейства ВПО, как AsyncRAT, RedLine, Remcos, njRAT, XWorm и другие. Такой набор троянов и стилеров использовался и другими группировками, а именно: TA558 и Blind Eagle. Количество скачиваний у некоторых образцов ВПО превышает 46 000 раз. Такое массовое использование файлов указывает скорее на распространение вредоносов через специализированные сервисы, нежели на активность традиционных APT-группировок.Один криптер на всехПри поиске схожих элементов в PowerShell-скриптах, а также в BAT- и VBS-файлах мы выделили несколько особенностей обфускации:Использование обфускации с применением функций SET, GOTO (BAT-файлы) и Call (VBS-файлы).Переменная $codigo («код» с португальского или испанского языков) в PowerShell-скрипте, получаемом после деобфускации.В последнем PowerShell-скрипте происходило скачивание полезной нагрузки в виде текстового файла, внутри которого находится закодированный в Base64 файл с ВПО. Проанализировав эти особенности скриптов, мы увидели пересечение с атаками группировки TA558, про которую мы рассказывали в апреле. Кроме того,  мы нашли сходство c деятельностью группировки Blind Eagle, описанной компанией eSentire. В отчете они рассказали про использование криптеров и обфускаторов, которые распространяются на теневых форумах в связке с загрузчиком Ande Loader. Его-то мы и встретили у PhaseShifters.Понять, что PhaseShifters использовала тот же криптер, что и TA558, также помогли картинки с репозиториев создателей обфусткаторов — они были одинаковыми.Самая первая картинка (июль 2023 года) с тестовой полезной нагрузкой внутри  Картинка из репозитория (август 2024 года), полезная нагрузка внутри — dnlibНа этом сходства PhaseShifters с другими APT-группировками не заканчиваются :)Не Bitbuket-ом единым: пересечения с UAC-0050UAC-0050 — хакерская группировка, атакующая государственные организации на территории Украины с 2020–2021 годов, а также компании из Польши, Беларуси, Молдовы, стран Балтии и России. Ранее мы сказали, что ход атак группировок PhaseShifters и UAC-0050 (отправка фишингового письма и заражение через документ-приманку или архив с паролем) был подозрительно похож. Более того, в 2023 году UAC-0050 использовала Remote Utilities для атаки на польские и украинские организации, а в январе 2024 года группировка начала маскировать это приложение под легитимный CCleaner. В этой же атаке группировка тоже использовала Bitbucket, но уже для хранения архива. Точно так же, как UAC-0050, PhaseShifters использовала ВПО, замаскированное под установщик утилиты CCleaner, в своих атаках на компании из Беларуси! Сам файл был SFX-архивом с обфусцированными AutoIt-скриптами. Обе группировки использовали AutoIt-скрипты на протяжении примерно полугода, и обе одновременно прекратили их использовать. Интересная особенность этих скриптов заключалась в том, что только три группировки замечены за использованием AutoIt-загрузчика, а именно: PhaseShifters, UAC-0050 и пропалестинская группировка Handala. Первые атаковали польские организации: PhaseShifters делала это в феврале 2024 года, а UAC-0050 атаковала Польшу в ноябре — декабре 2023 года. Основываясь на сходстве используемых техник фишинга, а также на абсолютно идентичной маскировке ВПО, мы сделали предположение о взаимосвязи этих двух группировок. Еще больше подозрительно точных сходствРассмотрим граф взаимосвязей, который нам удалось создать на основе обнаруженных атак. На схеме белым цветом отображено использование группировкой UAC-0050 некоторых репозиториев из общего списка, желтым — аналогичные связи только со стороны группировки PhaseShifters. Зеленым цветом в представлении графа показан анализ цепочки атаки, а фиолетовым — дополнительная информация. Как видно из графа, некоторые репозитории используются обеими группировками. И если взаимодействие с картинкой еще можно объяснить покупкой одного и того же криптера, о котором говорилось ранее, то использование одного и того же репозитория с полезной нагрузкой объяснить сложнее. Дело в том, что репозиторий с вредоносным ВПО указывается пользователем в обфускаторах как ссылка на полезную нагрузку. Получается, что обе группировки вручную указали эти ссылки.А теперь рассмотрим другие атаки группировок PhaseShifters и UAC-0050, в которых также использовался один репозиторий: bitbucket[.]org/sdgw/sdge/downloads/ (более недействителен).Со стороны группировки UAC-0050 такой репозиторий использовался в цепочке атаки с файлом Копія з позначкою банку.vbs, загружая в качестве полезной нагрузки файл bitbucket.org/sdgw/sdge/downloads/meduza[.]txt, после декодирования Base64 становящийся стилером Meduza. Группировка PhaseShifters использовала этот репозиторий в цепочке атаки с файлом Проект распоряжения правительства Курской области.pdf.zip. Полезная нагрузка скачивалась по ссылке bitbucket[.]org/sdgw/sdge/downloads/mbFgnhd[.]txt, что является DarkTrack RAT — характерным для них ВПО.Таким образом, в одном репозитории находились и стилер Meduza, использование которого запрещено правилами теневого форума для атак на российские организации и российских пользователей в целом, и DarkTrack RAT, который был обнаружен в атаках на территории России. Позже, 15 октября 2024 года, вышла новость с упоминанием этих репозиториев. Странная получается ситуация.Все точки над iМы убедились, что PhaseShifters копирует технику UAC-0050. Кроме того,  мы уже в который раз наблюдаем, что это копирование начинается с небольшим промежутком в несколько недель. Может, PhaseShifters — просто искусные подражатели, а может, PhaseShifters и UAC-0050 — это одна и та же группировка, атакующая как Россию, так и Украину. Мы склоняемся ко второму варианту, но не можем утверждать наверняка.Сетевые индикаторы и матрица MITRE ATT\u0026CKСетевые индикаторыИндикаторТип индикатораНазначениеraw.githubusercontent.com/santomalo/audit/main/img_test.jpg?\\d+Общий. Картинка из репозитория используется в атаках нескольких группировокGitHub-репозиторий с картинкой, внутри которой — полезная нагрузкаbitbucket.org/hgdfhdfgd/test/downloads/new_image.jpg?\\d+Общий. Картинка из репозитория используется в атаках нескольких группировокBitbucket-репозиторий с картинкой, внутри которой — полезная нагрузка с Ande Loaderbitbucket.org/shieldadas/gsdghjj/downloads/img_test.jpg?\\d+Общий. Картинка из репозитория используется в атаках нескольких группировокBitbucket-репозиторий с картинкой, внутри которой — полезная нагрузка с Ande Loaderbitbucket.org/rulmerurk/ertertqw/downloads/.*\\.txtИндивидуальный индикатор для PhaseShiftersBitbucket-репозиторий с закодированной полезной нагрузкойbitbucket.org/fasf24124/fdgfytrj/downloads/.*\\.txtИндивидуальный индикатор для PhaseShiftersBitbucket-репозиторий с закодированной полезной нагрузкойbitbucket.org/fwfsfw/fwf/downloads/.*\\.txtОбщий индикатор. Репозиторий используется несколькими группировками по всему мируBitbucket-репозиторий с закодированной полезной нагрузкойbitbucket.org/sdgw/sdge/downloads/.*\\.txtОбщий индикатор. Репозиторий используется несколькими группировками по всему мируBitbucket-репозиторий с закодированной полезной нагрузкой45.143.166.100Индивидуальный. Используется только группировкой PhaseShiftersDarkTrack C294.156.79.57DarkTrack C2Матрица MITRE ATT\u0026CKReconnaissanceT1135Network Share DiscoveryГруппа PhaseShifters сканировала сетевые папки, доступные через протокол SMB Resource DevelopmentT1588.001Obtain Capabilities: MalwareГруппа PhaseShifters предположительно приобретала криптер или подписку на криптерT1608.001Stage Capabilities: Upload MalwareГруппа PhaseShifters сама или с помощью третьих лиц загружала вредоносное ПО в Bitbucket-репозиторийInitial AccessT1566.001Phishing: Spearphishing AttachmentГруппа PhaseShifters посылала фишинговые письма в различные компании и прикрепляла запрошенные архивы с паролем, который находился в теле письмаExecutionT1059.001Command and Scripting Interpreter: PowerShellГруппа PhaseShifters запускала обфусцированные PowerShell-скриптыT1059.003Command and Scripting Interpreter: Windows Command ShellГруппа PhaseShifters запускала обфусцированные файлы с помощью cmd.exeT1059.005Command and Scripting Interpreter: Visual BasicГруппа PhaseShifters использовала обфусцированные VBS-скриптыPersistenceT1547.001Boot or Logon Autostart Execution: Registry Run Keys / Startup FolderВредоносные файлы группировки PhaseShifters закреплялись в папке StartupDefense EvasionT1027Obfuscated Files or InformationГруппировка PhaseShifters использовала Base64 для шифрования полезной нагрузкиT1027.002Obfuscated Files or Information: Software PackingГруппировка PhaseShifters использовала UPX и ThemidaT1027.003Obfuscated Files or Information: SteganographyГруппировка PhaseShifters использовала технику стеганографии в картинках, находящихся в Bitbucket-репозиторииT1027.010Obfuscated Files or Information: Command ObfuscationКриптер, который использовала группа PhaseShifters, обфусцировал PowerShell-кодT1036MasqueradingГруппировка PhaseShifters маскировала EXE-файлы, используя иконки легитимных программ и других расширенийT1036.007Masquerading: Double File ExtensionГруппировка PhaseShifters маскировала EXE-, DOCX- или LNK-файлы, используя двойные расширения, например .docx.exeT1036.008Masquerading: Masquerade File TypeНа Bitbucket, которые использовала PhaseShifters, вредоносные файлы хранились в закодированном виде с расширением .txtT1140Deobfuscate/Decode Files or InformationВредоносное ПО группировки PhaseShifters декодировало полезную нагрузку, полученную в процессе зараженияT1564.003Hide Artifacts: Hidden WindowВ криптере, который использовала группировка PhaseShifters, использовался флаг -hidden для сокрытия выполнения PowerShell-скриптаDiscoveryT1057Process DiscoveryГруппировка PhaseShifters использовала PowerShell-команды для поиска и завершения определенных процессовT1012Query RegistryГруппировка PhaseShifters собирала информацию о реестреCommand And ControlT1102Web ServiceГруппировка PhaseShifters использовала Bitbucket и GitHub для загрузки вредоносного ПО T1105Ingress Tool TransferГруппировка PhaseShifters использовала Ande Loader для загрузки дополнительного вредоносного ПОT1571Non-Standard PortГруппировка PhaseShifters использовала в атаках нестандартные порты, например 1443 и 49162, для коммуникации через TCP-протоколT1132.001Data Encoding: Standard EncodingГруппировка PhaseShifters использовала стандартный протокол TLS 1.2, а также закодированные в Base64 строки для шифрования передаваемых данныхАлександр БадаевСпециалист группы киберразведки Positive Technologies  Климентий ГалкинСпециалист группы киберразведки Positive Technologies",
  "Поиск жулика: Как понять, что перед вами ChatGPT 4?": "С момента появления ChatGPT 4, вопрос о том, как отличить ее на практике от старой-доброй 3.5 , волнует многих пользователей, в том числе и пользователей нашего сервиса. Простой запрос “какая версия ChatGPT передо мной?” не всегда даст правильный ответ — модели могут сообщить, что они не те, кем должны быть. И конечно же это насторожит.В этой статье мы покажем, как отличить ChatGPT 4o от ChatGPT 3.5, используя задачи, с которыми обе модели справляются по-разному. Эти тесты помогут вам быстро определить, с какой версией вы работаете, если вдруг появились сомнения.Приятного прочтения(:Небольшая вводнаяХотя OpenAI убрали ChatGPT 3.5 из своего интерфейса и заменили ее на ChatGPT 4o mini, но 3.5 пока также доступна через API. Так как же верить сервисам, которые предоставляют доступ к официальным моделям, используя API? Как убедиться, что вы платите именно за то, за чем пришли? Я здесь, чтобы показать вам парочку задач, которые помогут разобраться, что за модель перед глазами. Многих вводит в заблуждение вопрос, а-ля, “что ты за модель?”, адресованный одной из моделей. Почему же вводит в заблуждение? Все просто: модель может ответить неверно, сказав, что она более устаревшей версии, чем та, за которой вы пришли и за которую могли заплатить. Конечно же это посеет сомнения! Но это явление можно отнести к галлюцинациям, о которых мы когда-то уже говорили.Но не будем ходить вокруг да около. Статья написана посредством полученного негативного фидбэка от одного из пользователей Хабра (@progchip666), где как раз таки и столкнулись с сомнениями по поводу модели.Предлагаю задать пару вопросов моделям 4о и 3.5. Обращаться к 4о будем двумя способами: через оболочку BotHub и через официальное приложение ChatGPT, чтобы сравнить ответы. Сразу отмечу, что ответы через API и официальный UI могут разнится, почему? По причине системного промта, настроек и параметров. В то время как разработчик может сам себе все настроить, как ему угодно, прикупив API, – в официальном UI может быть сделано все за него. Насколько мне известно, то OpenAI не публиковала внутренние инструкции или системные промпты для ChatGPT, но они определенно есть, разве что нам с вами, простым обывателям, не дают в полной мере в этом покопаться, зато я помню одну из статей с Medium со способом вытащить этот самый системный промпт ChatGPT, где, кстати, указана модель, которую модель должна назвать, если ее спросят (сейчас этот способ уже не работает):Автор: Sawradip. Источник.Но нужно понимать, что это июльский материал, и команда OpenAI могла обновить системные промпты.Пример того, как системный промпт может выглядеть в официальном UI, есть в открытом доступе у Anthropic (модели Claude). То есть, например, Claude действует по принципу: чего не писано — того не ведаю, коль пусто в промпте — значит выдумаю.*С Claude просто хороший пример для восприятия разницы API и оф. UI, больше мы к нему обращаться не будемАктуальные данныеНачнем с данных. Обучающая выборка 4о модели - до октября 2023 года, а 3.5 до сентября 2021 года.Наш промт будет таким:что произошло 6 февраля 2023 годаChatGPT-4o (BotHub):ChatGPT-4o (OpenAI):ChatGPT-3.5:Итак, перед нашими глазами абсолютно разные ответы, наглядная демонстрация разницы в предоставлении актуальной информации (в условиях выборки, конечно). Чтобы убедиться, что перед вами ChatGPT 4o - просто задайте вопрос по той информации, которая появилась на просторах интернета уже после сентября 2021 года, но до октября 2023 года (это без функции поиска). ChatGPT 3.5 не сможет ответить на такой вопрос, либо ответит неверно, поскольку будет ограничена в своих возможностях.ЛогикаЗнаем, что версия 4o гораздо лучше 3.5 в логике, предлагаю это проверить на конкретном примере.Наш промт будет следующим:Две лодки плывут по реке параллельно друг другу. Каждая движется со скоростью 30 км/ч. С какой скоростью относительно берега движется их общий центр?ChatGPT-4o:ChatGPT-4o (OpenAI):ChatGPT-3.5:В задаче на логику, сравнивая модели, вы заметите, что ChatGPT 3.5 будет демонстрировать недостаточное понимание условия задачи, что приведет к неправильному выводу, в то время как ChatGPT 4o будет более точен в логических рассуждениях и физических задачах. ChatGPT 3.5, в силу того, что не понимает задачу, будет вдаваться (либо наоборот избегать, об этом далее) в подробности там, где того не требуется и только запутает вас, в то время как версия 4o сможет сразу установить, какие элементы важны для решения и применит правильный подход в решении.МатематикаИ давайте, например, возьмем задачу по математике, простую, но которая требует внимательности. Наш промт:Кирпич весит 1 кг и полкирпича. Сколько весит кирпич?ChatGPT-4o (BotHub):ChatGPT-4o (OpenAI):ChatGPT-3.5:Итак, как я указала ранее, ChatGPT 3.5 может наоборот избегать подробностей, как она пришла к такому выводу, и ответ, полученный сейчас – яркая демонстрация этого явления. Вы не увидите логики и рассуждений, а только неправильное интерпретирование задачи, что, как и в предыдущей задаче, только создаст путаницу. Также, и ответ вы получите неверный, поскольку у версии 3.5 большие пробелы в математике в сравнении с версией 4o. Таким образом, прогоняя модели через простые для человека, но порой непонятные для машины задачи, можно легко вычислить, где модель, за которой вы пришли, а где спрятался жулик. GPT 3.5 справляется исключительно с базовыми запросами, если пытаться копать дальше и давать ему задачки, связанные с рассуждениями, логикой и сложными (в сравнении с базовым уровнем) вычислениями – он посыпется и вы сможете понять, что перед вами ChatGPT 3.5. Но в то же время, нужно понимать, что и GPT 4/4o не так хороша, как могло показаться в ходе прочтения этой небольшой статьи. Статья предоставляет примеры задач, как понять, что перед вами ChatGPT 3.5, но не возносит GPT 4/4o к небесам: в нашем блоге есть много сравнений моделей (например, последний релиз API Grok 2), поиска галлюцинаций и когнитивных искажений, которые демонстрируют изъяны моделей, в том числе GPT-4o.Всем спасибо за внимание! И не верьте моделям, когда они говорят свою версию или размер контекста(:",
  "Проведение стратегической сессии. Пошаговое руководство": "Проведение стратегических сессий — один из наиболее недооценённых инструментов развития бизнеса. Реализация не требует существенных затрат, а возможные выгоды — выглядят привлекательно.Основная цель сессии — вынуть голову из операционки и оптимально распределить ресурсы за счет:Фиксации текущих приоритетных целейФормирования общей стратегии или бизнес-моделиКоординации миссии компании и ключевых ценностей командыФормирования общего плана действий по запуску нового проекта, развития направления или бизнеса в целомВыявления основных точек роста бизнеса и возможных сдерживающей факторовОпределения списка актуальных бизнес-гипотез и способов их проверкиДополнительный приятный бонус — сонастройка и усиление вовлеченности команды.В этой статье поделюсь своим опытом проведения сессий, который я получил, как в работе с основателями ещё тогда небольших стартап-проектов, так и с командами крупных корпоративных клиентов.Начнем с оргвопросовКогда проводить?Раз в год за 2-3 месяца до начала нового управленческого года в компании или “по нужде” (например, когда решили запустить новое направление бизнеса внутри компании, возникло непреодолимое желание оптимизировать или чувствуете, что у собственников стало расходится видение развития компании).В каком формате?От половины дня до 3х дней, в зависимости от целей, состава участников и размера бизнеса. Но, в любом случае, лучше добавить дополнительные поддерживающие встречи (трекшн-встречи) раз в 1-3 месяца для сохранения фокуса и дополнительной поддержке в борьбе приоритетов с рутиной.Где проводить?Лучше выйти за пределы офисного пространства. Как вариант — поехать в загородный отель и дополнить стратсессию вечерней программой.Каким составом?Тут нет универсального решения. Как правило, для крупного бизнеса — управленческая команда, малого / среднего бизнеса — собственники + управленческая команда, для стартапа (в тч внутрикорпоративного) — вся основная команда или отдельно фаундеры, для отдела (например отдела маркетинга) — руководитель компании, руководитель отдела и смежных отделов (для маркетинга — это могут быть продажи и производство) и руководители направлений в отделе (например, трафика, вебдизайна и др). В идеале, должны быть люди, отвечающие за результаты по всем основным бизнес-процессам проекта: финансы, маркетинг, продажи, закупки, производство, качество, HR…Иногда на сессии можно приглашать также внешних экспертов.Идеально — 7-9 человек. Больше 15 — не рекомендую.Кто должен проводить?Обычно проводит или руководитель / HR компании, или нанимают внешнего подрядчика (фасилитатор / модератор), или целое агентство. Я, как внешний подрядчик, выступаю за второй вариант. В целом, при должных навыках, можно провести и внутренними силами (в том числе, с использованием материалов, о которых расскажу далее), но тогда, помимо навыков и свободного времени, нужно ещё быть уверенным в относительной беспристрастности ведущего сессии.Какие документы получаются в результате?Список решенийПлан с целями, задачами, сроками, ресурсами, ответственными и ближайшими шагамиСписки идей, гипотез и открытых вопросовПодготовка к стратегической сессииБольшая часть работы делается до сессии. Более того, я убежден, что успех сессии намного в большей степени зависит именно от качества её подготовки.Начать стоит с понимания вводной информации: цель сессии, основной вопрос, ответ на который нужно найти на сессии, продукт сессии (что должно получиться / появиться в результате?), критерии успешности (как поймем, что сессия прошла успешно?), инициатор сессии, знания (что мы точно знаем, а где есть зоны неясности?), участники (кто должен быть и каковы их представления?), текущие и возможные проблемы, процесс (какие шаги нам нужно предпринять в рамках сессии?), место проведения, предыдущий опыт (проводились ли уже сессии и, если да, то какие были результаты и сложности?).Далее этап, который многие напрасно пропускают — предварительные интервью с участниками. Такие интервью позволят ориентировать сессию на правильные цели, заранее предусмотреть и сгладить возможные сложности (в том числе, потенциальные конфликты), а также наладить предварительный контакт с участниками и, в случае скепсиса, “продать” им выгоды от вовлеченного участия.Ниже несколько полезных вопросов для подобных интервью:— Как думаете, зачем мы проводим сессию? Ответы на какие вопросы вы бы хотели получить? С чем бы хотели уйти с неё? Почему это важно?— Как вы видите будущее организации? В чем сильные стороны вашей компании? А с чем вы справляетесь не так хорошо, как хотелось бы? Почему?— Что мешает реализации текущей стратегии? Что может создавать сложности при внедрении изменений? Какова ваша выгода при успешной реализации проекта?— Что я, как ведущий, должен знать до начала сессии?Результаты сессии зависят от поставленных целей и могут быть следующего характера: единое согласованное мнение; новые идеи; определенный документ, например, видение компании или наброски стратегии, отчет, план; календарный график определенных действий, с указанием ответственных лицПосле этого можно приступать к подготовке плана сессии с точным таймингом, ключевыми вопросами для обсуждения и целевыми результатами.В классическом виде план годовой стратсессии может быть таким, как на примере ниже, но каждый раз создавать план нужно индивидуально, исходя из целей и особенностей. Из инструментов, часто подходят (если без злоупотреблений) классические консалтинговые формы, типа SWOT (только в полноценном формате: не просто прописать сильные стороны, слабые стороны, угрозы и возможности, а оценить и сопоставить для выделения приоритетных действий), матрицы BCG, GE / McKinsey, Ансоффа, Хосин-Канри и др.Проведение стратегической сессии. Пошаговое руководствоПосле составления плана важно по каждому из пунктов понять, какую информацию (отчеты, материалы) следует участникам заранее узнать и какие материалы подготовить к сессии.В самом процессе подготовки имеет смысл помочь участникам сессии в создании их презентаций. Чаще всего, в рамках таких презентаций нужно будет рассказать о результатах руководимого направления за прошлый год: 2- 3 ключевых результата, 2-3 ключевые зоны роста, 2-3 идеи нелинейного роста, 2-3 вещи, которые нужно делать более активно, 2-3 вещи, которые стоит прекратить делать.За 1-2 недели до начала сессии я также создаю в ТГ общий чат участников, где делюсь планами, оргвопросами (программа, время, место, стиль одежды и пр.) и дополнительно собираю пожелания.Фасилитация стратегической сессииПеред началом стоит проверить все оборудование и повесить, как минимум, три листа флипчата (для фиксации решений, планов и вопросов на будущее), а также лист с правилами.В начале сессии вам нужно: проинформировать о происходящем, вовлечь участников и утвердить список правил сессии. Свой список я взял из книги М. Вилкинсон. Секреты фасилитации. SMART-руководство по работе с группами (конспект можно прочитать по ссылке):— право высказаться есть у каждого, в том числе с предложением ускорится или замедлиться— решения по предложениям участников будет решаться путём голосования— говорить в один момент времени может только один человек— нельзя использовать фразу “не получится” (в различных её вариациях), вместо этого нужно говорить конкретно, что именно может не сработать— необходимо соблюдать электронный этикет— в случае падения уровня энергии, мы будем использовать «подзарядку» (давайте вместе придумаем упражнение), каждый участник может в любой момент времени предложить совершить «подзарядку»— роль участников — активное участие и обмен опытом, моя роль — сделать процесс более эффективным и сфокусированным— необходимо соблюдать тайминг— ориентация на то, чтобы охватить 100% повестки и проработать каждый пункт не менее чем на 85%— дополнительные правила от участниковДалее идите по плану, каждый раз перед началом рассмотрения нового вопроса делая обзор пройденного и предстоящего и объясняя как новый пункт вписывается в цель сессии. А также перепроверять динамику сессии через соответствующие вопросы к аудитории: чувствуют ли прогресс? не устали? устраивает ли темп?Из инструментов фасилитации, стоит использовать:— Уточняющие вопросы: «Правильно я понял, что…?»— Продолжающие вопросы: «Что ещё? Какие ещё…?»— Работу в малых группах— Инструменты генерации идей— Различные инструменты принятия решений группой: голосование простым большинством, консенсусом (к которому нужно прийти не более, чем за 3 этапа голосования чередующегося с обсуждением) или с использованием балльных систем (наклеивание стикеров разных весов на предпочтительные варианты). В любом случае, принятие решений должно начинаться с совместного определения критериев, лоббирования (кто хочет, может высказывается за какой-то из вариантов) и, только потом, голосования.В конце сессии следует провести обзор достигнутого: результаты , решения, действия (можно разделить не категории: которые можно было бы реализовать, которые необходимо реализовать и которые точно будут реализованы), открытые вопросы и получить обратную связь от участников сессии.Поддержка после сессииПосле завершения сессии стоит обсудить итоги, подготовить резюме сессии — документ, содержащий список принятых решений, запланированных действий и выявленных вопросов для будущего обсуждения.И я всегда выступаю за дополнительные поддерживающие активности в виде трекшн-сессий по реализации / корректировки запланированного в рамках годовой стратсессии.Ссылка на меня. ",
  "Разработка бота для Telegram на платформе .NET": "ВведениеTelegram — один из самых популярных мессенджеров в мире, предлагающий такие функции, как групповые чаты, каналы, голосовые и видеозвонки, а также возможность создания ботов. В данной статье мы не будем ставить цель показать, как создать с нуля приложение a-la \"Hello, World!\", а изучим более сложный пример готовой реализации бота на платформе .NET с использованием современных технологий и практик разработки. Выбор библиотеки для создания ботаСуществует несколько способов создания Telegram-ботов на платформе .NET, включая использование сторонних библиотек и фреймворков. Рассмотрим самые популярные варианты, предлагаемые самим Telegram.Telegram.Bot от TelegramBots Наиболее популярная библиотека для создания Telegram-ботов на платформе .NET.Поддерживает все возможности Telegram Bot API.Регулярно обновляется и поддерживается сообществом.Обладает подробной документацией и примерами использования.Поддерживает .NET Standard 2.0 и .NET 6+.Telegram.BotAPI от EptagoneЕще одна популярная библиотека для создания Telegram-ботов на платформе .NET.Также поддерживает все возможности Telegram Bot API, регулярно обновляется и имеет хорошие примеры использования.Поддерживает .NET Standard 2.0, .NET Framework 4.6.2+ и .NET 6, 8.TelegramBotFramework от MajMcCloudБиблиотека с простым интерфейсом для создания ботов, напоминающая разработку приложений на Windows Forms.Имеет хорошую документацию и примеры использования.Обновляется реже, чем Telegram.Bot и Telegram.BotAPI, и не поддерживает все возможности Telegram Bot API.Поддерживает .NET Standard 2.0+ и .NET 6, 7.Для примера реализации была выбрана библиотека Telegram.BotAPI, так как она обладает актуальными возможностями и предоставляет лучшие примеры использования. Данный выбор — субъективный и не является строгой рекомендацией, так как первые две библиотеки одинаково хороши. TelegramBotFramework не был выбран из-за специфичного подхода к архитектуре и отсутствия поддержки новых возможностей Telegram Bot API.Практический пример реализацииРассмотрим GitHub-репозиторий автора с примером реализации бота, который предоставляет информацию о погоде в различных городах.Основные особенности рассматриваемой реализацииТехнические:Современная версия платформы .NET 8..NET Aspire для развёртывания проекта и управления зависимостями.PostgreSQL для хранения и Entity Framework Core 8 для работы с данными.MediatR для реализации паттерна CQS и уменьшения связанности.OpenTelemetry для логирования, трассировки и мониторинга.Unit-тесты с использованием xUnit, FluentAssertions и Moq.Архитектурные тесты для проверки соответствия кода чистой архитектуре.Функциональные:Получение обновлений через Polling (Webhook не рассматривается в целях простоты).Отправка пользователю информации о погоде в различных городах. Текущая температура генерируется случайным образом.Поддержка пожертвований в виде Telegram Stars.Ролевая система для управления доступом к командам.Поддержка пользовательских настроек.Локализация интерфейса и сообщений.Inline-функции для быстрого доступа к информации.Архитектура решенияОбщая структураРешение разделено на несколько слоев, каждый из которых отвечает за свою функциональность.Host/AppHost - отвечает за запуск приложения.Содержит точку входа и конфигурацию всех необходимых сервисов, таких как MediatR, Entity Framework и OpenTelemetry.Настраивает подключение к базе данных и инициализирует бота.AppHost также позволяет запустить приложение и все зависимости с помощью .NET Aspire.Application - содержит бизнес-логику приложения.Здесь находятся обработчики команд, которые обрабатывают запросы пользователей и выполняют соответствующие действия.Реализует паттерны CQS и Mediator для разделения команд и запросов. Все команды и их обработчики сгруппированы в отдельные директории по функциональности, что упрощает их поиск и поддержку и несколько напоминает подход Vertical slice:src\n├── Application\n│   ├── Features\n│   │   ├── Bot\n│   │   │   ├── StartBotCommand.cs\n│   │   │   ├── StartBotCommandHandler.cs\n...\n│   │   ├── Weather\n│   │   │   ├── WeatherBotCommand.cs\n│   │   │   ├── WeatherBotCommandHandler.cs\nData - отвечает за доступ к данным и взаимодействие с базой данных.Реализует репозитории, использующие Entity Framework для выполнения операций с базой данных.Содержит миграции базы данных и конфигурации сущностей.Domain - содержит основные сущности и интерфейсы, используемые в приложении.Определяет модели данных, интерфейсы репозиториев и другие абстракции, которые помогают отделить бизнес-логику от деталей реализации.Framework - содержит вспомогательные библиотеки и утилиты, используемые в проекте.Общие классы, расширения, обработчики исключений и другие компоненты, которые помогают упростить разработку и поддержку приложения.Взаимодействие компонентовЗапуск приложения: Проект Host инициализирует все необходимые сервисы и запускает приложение.Обработка команд: Когда пользователь отправляет команду боту, она попадает в слой Application, где соответствующий обработчик команды выполняет бизнес-логику.Доступ к данным: Если обработчику команды необходимо взаимодействовать с базой данных, он использует репозитории из слоя Data.Использование сущностей: Репозитории и обработчики команд работают с сущностями и интерфейсами из слоя Domain.Вспомогательные функции: В процессе работы приложения используются утилиты и библиотеки из слоя Framework.Данная архитектура позволяет легко масштабировать и поддерживать приложение, разделяя ответственность между различными слоями и обеспечивая гибкость и модульность кода.Зависимости между слоями выстроены по правилам чистой архитектуры. Например, слой Application зависит от слоя Domain, но не зависит от слоя Data.Инфраструктура ботаКонфигурированиеДля полноценной работы бота необходимо настроить токен API Telegram (получение самого токена опустим, т.к. данная тема хорошо раскрыта в официальной документации) и список основных команд. Этой цели служит класс TelegramBotSetup, исполняемый как hosted-сервис при запуске приложения.Для каждого поддерживаемого языка определяются команды, которые будут отображаться в списке доступных команд бота непосредственно в приложении Telegram.internal sealed class TelegramBotSetup : IHostedService\n{\n    // ctor\n\n    public async Task StartAsync(CancellationToken cancellationToken)\n    {\n        //...\n        await SetCommands(cancellationToken).ConfigureAwait(false);\n        //...\n    }\n\n    private async Task SetCommands(CancellationToken cancellationToken)\n    {\n        await _client.DeleteMyCommandsAsync(cancellationToken: cancellationToken).ConfigureAwait(false);\n\n        // default (en)\n        await _client.SetMyCommandsAsync(\n            [\n                new(WeatherBotCommand.CommandName,\n                    _botMessageLocalizer.GetLocalizedString(nameof(BotMessages.WeatherCommandDescription), BotLanguage.English)),\n                new(HelpBotCommand.CommandName,\n                    _botMessageLocalizer.GetLocalizedString(nameof(BotMessages.HelpCommandDescription), BotLanguage.English)),\n            ],\n            cancellationToken: cancellationToken).ConfigureAwait(false);\n\n        // other languages\n    }\n}Получение обновлений от TelegramДля получения обновлений от Telegram используется подход Polling, который позволяет боту регулярно проверять наличие новых сообщений и обновлений. Такой подход удобен для небольших проектов и не требует настройки веб-хуков. Тем не менее, реализовать полноценный веб-хук тоже не составит труда (см пример).За получение обновлений от Telegram отвечает hosted-сервис UpdateReceiver, который регулярно запрашивает обновления через API Telegram, инициализирует экземпляр класса WeatherBot и передает ему полученные данные.Обработка запросовЦентральной точкой функционирования бота является класс WeatherBot, наследующий библиотечный класс SimpleTelegramBotBase.Именно он отвечает за обработку команд, callback'ов и биллинга. Для простоты восприятия, данный класс разбит на несколько частей, каждая из которых отвечает за определенный функционал:Преобразование обновления или callback'а из Telegram в команду и её отправка в MediatRОбработка ошибокОбработка платежейт.д.Обработка командОсновная задача бота - это обработка сообщений/команд, которые пользователи отправляют боту для выполнения определенных действий. В рассматриваемом примере команды обрабатываются с использованием паттерна CQS и MediatR.Диаграмма последовательности обработки командКаждая команда Telegram или её callback имеют соответствующий класс, реализующий интерфейс IBotCommand или ICallbackCommand. Например, StartBotCommand:public sealed record StartBotCommand(Message Message, UserInfo UserInfo) : IBotCommand\n{\n    public static string CommandName =\u003e \"start\";\n}\n\npublic interface IBotCommand : IRequest\u003cUnit\u003e\n{\n    static abstract string CommandName { get; }\n\n    static virtual bool AllowGroups =\u003e true;\n\n    static virtual IReadOnlyList\u003cstring\u003e Roles { get; } = Array.Empty\u003cstring\u003e();\n\n    public Message Message { get; init; }\n\n    public UserInfo UserInfo { get; init; }\n}Имя команды определяется статическим свойством CommandName.Соответствие имени и самой команды автоматически кешируется приложением для обеспечения быстрой инициализации команд.Дополнительно можно переопределить свойства AllowGroups и Roles для управления доступом к команде в разрезе групповых чатов и ролей пользователей.Обработка команды происходит в соответствующем MediatR-обработчике, который выполняет некоторые вычисления и отправляет готовый результат пользователю. Например, StartBotCommandHandler:internal sealed class StartBotCommandHandler : IRequestHandler\u003cStartBotCommand, Unit\u003e\n{\n    private readonly ITelegramBotClient _telegramBotClient;\n    private readonly IBotMessageLocalizer _botMessageLocalizer;\n\n    public StartBotCommandHandler(\n        ITelegramBotClient telegramBotClient,\n        IBotMessageLocalizer botMessageLocalizer)\n    {\n        _telegramBotClient = telegramBotClient;\n        _botMessageLocalizer = botMessageLocalizer;\n    }\n\n    public async Task\u003cUnit\u003e Handle(StartBotCommand request, CancellationToken cancellationToken)\n    {\n        var message = request.Message;\n        var text = _botMessageLocalizer.GetLocalizedString(nameof(BotMessages.HelpCommand), request.UserInfo.Language);\n\n        await _telegramBotClient.SendMessageAsync(\n                message.Chat.Id,\n                text,\n                parseMode: FormatStyles.HTML,\n                linkPreviewOptions: DefaultLinkPreviewOptions.Value,\n                cancellationToken: cancellationToken)\n            .ConfigureAwait(false);\n\n        return Unit.Value;\n    }\n}Также можно заметить, что в обработчике реализована поддержка локализации сообщений (через стандартные возможности .NET и ресурсные файлы)), что позволяет отправлять сообщения на разных языках в зависимости от настроек пользователя. Информация о пользователе, его языковых предпочтениях и ролях также является частью команды - эти данные заполняются автоматически при создании экземпляра класса.Опустим в данной статье ролевую систему, вопрос локализации и платежей, так как это не является ключевым аспектом рассматриваемого примера. При желании, вы можете изучить соответствующие классы и интерфейсы в репозитории и самостоятельно запустить приложение.ЗаключениеМы рассмотрели создание Telegram-бота на платформе .NET с использованием стека современных библиотек и технологий. Пример реализации демонстрирует архитектурные подходы, обеспечивающие модульность, гибкость и расширяемость решения. Это позволяет разработчикам сосредоточиться на бизнес-логике и легко добавлять новые команды и функции, минимизируя связанные изменения кода.Если у вас есть вопросы или предложения по улучшению решения, не стесняйтесь обращаться к автору или создавать issue в репозитории.Дополнительные материалыОфициальная документация Telegram Bot APIGitHub-репозиторий с примером реализации.",
  "Рекомендую поиграть: Deponia": "Эта заметка — сразу о четырёх играх: обо всей серии Deponia. Проходил я их разом, запойно, и представить себе их отдельно друг от друга не могу. Тем более, что первые две игры заканчивались и вовсе клиффхэнгерами.Что сказать, в целом... Это шедевр. Удивительно, как создатели вдохнули жизнь в столь старомодный жанр, как point-and-click квест. Сюжет мог бы быть вторичным, если бы не высмеивал все штампы и клише, до которых мог дотянуться. Юмор блестящий, часто грубый, но порой очень тонкий и совершенно нецензурный — не всем понравится. Иногда кажется, что сценаристы перегибают палку: то приходится из милых дельфинчиков сделать консервы, то содействовать тому, чтобы милых детишек сожрал монстр — но оторваться от этого действа невозможно.А действо безумное. Главный герой, стопроцентный бедоносец, полуграмотный выскочка, искренне считающий себя гениальным изобретателем, из кожи вон лезет, чтобы сбежать с превращённой в одну большую свалку планеты (Депонии) на огромный корабль, парящий в верхних слоях атмосферы — Элизиум. Жители Элизиума, не знающие голода и проблем, понятия не имеют в большинстве своём, что мир внизу обитаем — не может же быть, чтобы на этой свалке ещё теплилась жизнь! Но на всякий случай они отправляют вниз комиссию, дабы проверить, нет ли там кого, среди отходов и мусора. Надо ведь наверняка убедиться, прежде чем уничтожать планету...Жители Депонии знают про Элизиум и зачастую пытаются туда попасть — вот только никому ещё это не удавалось. И никто, понятно, не верит, что такой неудачник и бестолочь, как Руфус, сможет обосноваться на Элизиуме. Раз за разом он что-то изобретает, чтобы долететь до Элизиума, и каждый раз его идеи терпят крах, после чего он возвращается к исходной точке. Однако упорства ему не занимать: едва потерпев неудачу, он тут же принимается за очередной гениальный план побега с Депонии.Но дуракам везёт (или почти везёт), поэтому в начале первой игры Руфус добирается до патрульного военного крейсера, нарезающего круги по рельсам надземки, высоко стоящей над землями Депонии. И на этом крейсере он случайно становится свидетелем странной ссоры между ослепительной рыжеволосой красоткой-элизианкой и офицером-органонцем. А потом, желая героически спасти красотку, случайно сбрасывает её с крейсера вниз, в трущобы Депонии. После этого солдаты выбрасывают за борт и самого путешественника, и всё бы вернулось на круги своя, если бы не Гоал — та самая девушка, которая теперь, как считает Руфус, может сыграть ему роль пропуска на Элизиум.А дальше начинается чистейшее безумие, но безумие восхитительное. Какой только ерунды не приходится натворить, преследуя низменные меркантильные цели протагониста! Руфусу поначалу даже сочувствуешь, однако через какое-то время, особенно в третьей части игры, понимаешь, что таких, как он, надо отстреливать, едва его фигура замаячит на горизонте. Подпустишь ближе — и оглянуться не успеешь, как твой город сгорит дотла, твою девушку отдадут в рабство, а сам ты окажешься в плену у монстра, или того хуже.Разработчики регулярно ломают четвёртую стену. Например, в одном эпизоде второй части главный герой жалуется, что ему не получается подслушать секретный стук-пароль из-за музыки, громыхающей неподалёку. Знаете, что нужно сделать? Залезть в настройки игры и отключить музыку! Только тогда Руфусу удаётся всё услышать. Кажется, больше возможностей из квестов ещё никто не выжимал.Более того, игра пестрит мини-играми, не похожими друг на друга. То надо драться на арене в духе примитивного файтинга, то ловить червя-паразита в чужом животе, доставая органы, а потом по памяти возвращать органы обратно, то чистить зубы, притворяясь отражением своего двойника, то уворачиваться на скользящей по склону холма доске от открывающихся порталов, то решать головоломки на импровизированном танцполе... И здесь тоже создатели издеваются над игроком, как могут. Например, нужно собрать разбитую мозаику. Вот дыра, вот осколки — бери и вставляй. В лучших традициях старых игр каждый осколок можно вставлять только на одно место, остальные просто неактивны. И вот заскучавший игрок тычет осколками на все места, вставляя их один за другим, берёт последний... и тут Руфус понимает, что собрал мозаику неправильно, осколок не вставляется. Но не начинать же всё заново — куда проще поднажать на осколок, чтобы он запихался наверняка! Секунда — и вся мозаика, на которую вы потратили несколько минут жизни, разбивается вдребезги, уже необратимо.Разработчики постоянно обманывают ожидания игрока, выворачивая каждую знакомую по другим играм ситуацию наизнанку. И это вызывает только восхищение.Забавно, но даже главный герой здесь играет ту же роль. Помните GTA 5? В играх серии GTA мы частенько пускаемся в отрыв, делая то, что по сюжету главный герой не делает и делать не может. Понятно, что главный герой не отстреляет половину города из чистого веселья — поэтому сюжет сделает вид, что этого не было. Но только не GTA 5! Там сценаристы специально ввели такого персонажа, любые действия игрока с которым будут полностью вписываться в сюжет. Да, да, я про Тревора. Какую бы дичь вы ни творили — это всё легко впишется в рамки сюжета, ибо персонаж под это заточен.В Deponia — то же самое. Зачастую в квестах мы понятия не имеем, что делать, потому что задачки бывают просто-напросто нелогичны, и тогда мы просто пытаемся соединить в инвентаре все предметы друг с другом, авось что получится, или тыкать случайным предметом на все активные поверхности. Так вот, герой именно это и делает. Он считает себя гениальным механиком именно потому, что пытается присобачить всё друг к другу, и только так, по злой иронии, у него получаются совершенно безумные и непредсказуемые результаты — такое ощущение, что на место главного героя поставили случайного, при этом не очень опытного игрока в квесты, который ещё толком не разобрался, но уже что-то делает. Авось что выйдет.Всё остальное в игре тоже хорошо — рисовка (простая, но душевная), музыка (чего стоят только песни \"Huzzah!\" между главами!), интересный сеттинг и яркие, совершенно незабываемые персонажи. Есть даже мораль — ведь ровно один раз в каждой игре Руфус будет делать правильный выбор. Приводить это будет, увы, не к тем последствиям, которых можно ожидать.Как ни странно, в этом абсурде и пародии нашлось место и неплохой романтической линии — история Руфуса и Гоал заняла прочное место в моём сердце как одна из лучших фантастических историй идиотской любви. А финалы (да, во множественном числе) оставляют послевкусие искренней печали, несмотря на абсурдность ситуаций.Есть и отсылки к другим играм, порой довольно резкие. Например, в одном эпизоде герой в кромешной темноте пытается проложить себе путь наощупь и на что-то натыкается.— Хм... Длинный, сухой и тощий... — задумчиво говорит Руфус, ощупывая предмет. — Сомнений быть не может! Это сюжет игры \"Тунгуска: Секретные материалы\"!Серия Deponia — редкий образчик по-настоящему качественной и душевно сделанной игры, штучный продукт, неповторимый игровой опыт. Любителям квестов мимо не проходить.Автор текста: Михаил Гречанников. Написано при поддержке Timeweb Cloud специально для CatGeek и читателей Хабра.  Разрабатывайте и развивайте свою игру (и не только) с помощью облачного хостинга для GameDev ↩Перейти ↩🎲 Читайте также:➤ Contra – игра своего времени;➤ Под покровом арабской ночи: Disney's Aladdin;➤ О ненадежных рассказчиках, апокрифистике и нестандартном взгляде на «Вархаммер»;➤ Их место в музее: игры об Индиане Джонсе;➤ Полная история игровой вселенной «Гарри Поттера» — «Золотое поколение».  ",
  "Созданный в МФТИ функциональный узел радиомодуля для системы связи 5G успешно прошел испытания": "Инженеры МФТИ совместно с индустриальным партнером ООО “Телепорт” испытали блок дуплексора для приемопередающего радиомодуля системы мобильной связи 5G. Изделие полностью выполнено на отечественной компонентной базе и является частью проекта по созданию российской телеком-инфраструктуры. Она  значительно повысит скорость и качество передачи данных и откроет новые горизонты для технологий будущего и цифрового суверенитета России.Изделие полностью выполнено на отечественной компонентной базеВ рамках этой работы на Физтехе создают высокочастотные фильтры на объемных резонаторах  - специальные устройства, предназначенные для использования в базовых станциях сетей четвертого и пятого поколений (4/5G). Они отвечают за подавление паразитных составляющих сигнала на входе радиомодуля и улучшение электромагнитной совместимости и играют ключевую роль в обеспечении эффективной работы систем связи.Схематичное изображение разработанного в МФТИ узла - высокочастотного фильтраПринцип работы объёмных резонаторов основан на использовании металлических или диэлектрических элементов определенной формы и размеров. Они создают полости с частотными характеристиками, позволяющими эффективно подавлять нежелательные составляющие сигналов.К основным компонентам конструкции высокочастотных фильтров на объемных резонаторах относятся:металлический или диэлектрический корпус, обеспечивающий механическую прочность и защиту от внешних воздействий;набор объемных резонаторов с собственной частотой настройки;система возбуждения и согласования, обеспечивающая передачу сигналов между резонаторами и внешними устройствами.По словам разработчиков, применение таких фильтров позволяет улучшить характеристики базовых станций 4/5G, обеспечивая более надежное и стабильное соединение, а также снижая уровень помех и взаимного влияния между различными системами связи.В результате испытаний на базе ООО “Телепорт” (Ижевск) и в пилотной зоне МФТИ фильтры дуплексера (устройство, предназначенное для организации дуплексной радиосвязи с использованием одной общей антенны для приёма и передачи сигналов. Содержит два радиочастотных фильтра с непересекающимися диапазонами частот с высоким коэффициентом передачи амплитудно-частотных характеристик и работает на основе частотной селекции сигналов) показали уровень заграждения в перекрестных диапазонах частот более 120 дБ, что полностью соответствует техническому заданию. Набор высокочастотных фильтров - составных частей дуплексера“Фильтры показали свою готовность к работе. Это лишь часть большого проекта по созданию собственной сети 5G в России. Наша команда также занимается разработкой активных антенных устройств на базе технологии фазировоанных антенных решеток и других узлов для базовых станций. В дальнейшем наши наработки будут использованы в различном отечественном оборудовании. И речь идет не просто о скорости доступа к сети интернет. Наши технологии будут применяться в робототехнике, беспилотном управлении автомобилями, развитии искусственного интеллекта и других высокотехнологичных сервисах”, - рассказал начальник Отдела прикладных исследований и разработок перспективных решений сотовой связи МФТИ, директор Научно-исследовательского центра компонентов мобильной связи и технологий их производства МФТИ Григорий Серегин.На следующем этапе планируется комплексирование разработанной конструкции мультиплексоров в корпус отечественного приемопередающего радиомодуля и расширение продуктовой линейки готовых фильтров на новые частотные диапазоны.Специалисты Московского физико-технического института также занимаются разработкой технологического решения для подключения к 5G через спутники. Технология позволит обеспечить доступность связи пятого поколения для жителей отдаленных регионов России без развертывания на местности полноценной инфраструктуры сотовых операторов. Решение будет востребовано прежде всего предприятиями, добывающими природные ископаемые, например, на рудниках, газовых месторождениях или в рабочих посёлках.«Разработка спутниковых сервисов на базе 5G открывает новые горизонты в области связи и интернета вещей. Благодаря интеграции спутниковых технологий и сетей пятого поколения мы сможем обеспечить функционирование робототехнических комплексов и технологического оборудования в автономном режиме без увеличения нагрузки на спутниковый канал связи в удаленных регионах. В настоящий момент данные технологии отрабатываются в пилотной зоне МФТИ», - отметил заместитель начальника НИЦ телекоммуникаций МФТИ Антон Худыкин.Фильтры показали свою готовность к работеазработка 5G в МФТИ является частью глобальной гонки за лидерство в области телекоммуникаций. Согласно планам Минцифры по развитию мобильной связи в России, разворачивание сетей 5G в России будет проходить постепенно, начиная с 2028 года. Одно из ключевых условий для успешной реализации этого проекта –  серийное производство отечественного оборудования.",
  "Тактические паттерны DDD": "Привет, Хабр! В предыдущей статье мы обсудили стратегические паттерны, а теперь давайте углубимся в тактические. Важно помнить: в DDD тактика без стратегии теряет смысл! Если вы не знаете, как правильно разделить систему, отдел или предприятие на контексты и поддомены, ваши усилия, направленные на тактические паттерны, вряд ли принесут плоды. Стратегическое мышление в сочетании с тактическими подходами поможет создать эффективную и гибкую архитектуру, способную справляться с изменениями и требованиями бизнеса.В этой статье мы рассмотрим реализацию шаблонов на основе вымышленного домена, что позволит лучше понять их применение в реальных сценариях. Вы можете ознакомиться с примерами реализации на GitHub (ссылки для TypeScript и Go).Использование DDD на практикеЕсли вы столкнулись с большой и неповоротливой системой, следуйте этому плану:Проводим Event StormingПригласите бизнес-заказчиков на встречу, чтобы прояснить требования и разбить систему на контексты. Рекомендую тщательно ознакомиться с процессом Event Storming, так как в этой статье я не буду углубляться в детали.Event StormingРезультатом встречи станет полное понимание системы и её контекстов (подробнее о контекстах можно прочитать здесь).Для нашего вымышленного домена контексты могут выглядеть следующим образом:Warehouse - контекст склада внутри маркетплейсаAccounting - контекст бухгалтерии внутри маркетплейсаDelivery - контекст доставки внутри маркетплейсаИщем поддоменыТеперь, когда контексты определены, важно понимать, что они могут быть весьма обширными. Например, контекст складских операций может охватывать множество внутренних систем, каждая из которых может иметь свою сложную структуру.Предлагаю в каждом контексте выделить поддомены и определить их типы, что станет основой для использования различных тактических паттернов, о которых мы поговорим в этой статье.Для контекста Warehouse:OrderManagement(Core)  - управление заказами на складеLocation(Supporting) - управление расположением товаров на складеКонтекст Accounting включает:Reports(Core) - генерация отчетов по финансамVerification(Supporting) - проверка заказов и выставление накладныхКонтекст Delivery представлен следующими поддоменами:Core.Board(Core) - доска предложений заказовCore.Couriers(Core) - управление курьерамиSupporting.Tracking(Supporting) - отслеживание статуса доставкиContexts and SubdomainsВстраивание тактических паттернов в поддоменыКаждый поддомен внутри контекста имеет свою важность и уровень сложности, что требует применения соответствующих паттернов. Одни паттерны лучше подходят для простых поддоменов, другие — для более сложных. Важно использовать их по назначению и не привязываться к одному шаблону, чтобы избежать его применения в неподходящих ситуациях.Основные тактические паттерныTransaction ScriptПредставьте, что вы разрабатываете сервис авторизации. Насколько сложной может стать его бизнес-логика? Оправдано ли добавление архитектурно сложных решений в этот сервис? Рассмотрим следующий код:export const register = async (req: Request, res: Response) =\u003e {\n  const { email, password } = req.body;\n  try {\n    const existingUser = await User.findOne({ email });\n    if (existingUser) {\n      return res.status(400).json({ message: 'User already exists' });\n    }\n    const hashedPassword = await bcrypt.hash(password, 10);\n    const newUser = new User({ email, password: hashedPassword });\n    await newUser.save();\n    res.status(201).json({ message: 'User registered successfully' });\n  } catch (error) {\n    res.status(500).json({ message: 'Server error', error });\n  }\n};\n\nexport const login = async (req: Request, res: Response) =\u003e {\n  const { email, password } = req.body;\n  try {\n    const user = await User.findOne({ email });\n    if (!user) {\n      return res.status(400).json({ message: 'Invalid credentials' });\n    }\n    const isMatch = await bcrypt.compare(password, user.password);\n    if (!isMatch) {\n      return res.status(400).json({ message: 'Invalid credentials' });\n    }\n    const token = jwt.sign({ id: user._id }, JWT_SECRET, { expiresIn: '1h' });\n    res.json({ token });\n  } catch (error) {\n    res.status(500).json({ message: 'Server error', error });\n  }\n};Перед нами пример паттерна Transaction Script. Суть этого шаблона заключается в том, что мы организуем бизнес-логику с помощью процедур, каждая из которых обрабатывает один запрос из представления. Проще говоря, Transaction Script — это когда вся бизнес-логика сосредоточена в слое приложения (или сервисов). Хотя процедурный стиль может показаться устаревшим (адепты доменной модели могут критиковать его за анемию), он отлично подходит для простых задач, таких как авторизация.Сервис авторизации является отличным примером Generic поддомена, где использование паттерна Transaction Script вполне оправдано. Не стесняйтесь применять этот подход и в Supporting поддоменных, где сложность задач не требует излишней архитектурной нагрузки.Active RecordСледующий по сложности паттерн, который стоит рассмотреть, — это Active Record. Суть этого паттерна заключается в том, что бизнес-логика, подобно паттерну Transaction Script, располагается в сервисном слое, но значительная часть этой логики может быть интегрирована в модели ORM. При этом мы помещаем в ORM модели только ту логику, которая не содержит инфраструктурных зависимостей. Рассмотрим пример:export class VerificationService {\n  constructor(\n    private readonly verificationRepository: Repository\u003cVerification\u003e,\n  ) {}\n\n  async update(\n    updateVerificationDto: UpdateVerificationDto,\n  ): Promise\u003cVerification\u003e {\n    const verification = await this.verificationRepository.findOne({\n      where: {\n        id: updateVerificationDto.id,\n      },\n    });\n\n    if (verification === null) {\n      throw new BadRequestException(\n        `Verification with id ${updateVerificationDto.id} not found`,\n      );\n    }\n\n    if (updateVerificationDto.signed) {\n      verification.signReport();\n    }\n\n    if (updateVerificationDto.completed) {\n      verification.completeVerification();\n    }\n\n    return this.verificationRepository.save(verification);\n  }\n}\n\nexport class Verification {\n  @PrimaryGeneratedColumn('uuid')\n  id: string;\n\n  /// ... columns\n\n  signReport() {\n    if (this.completed) {\n      throw new Error('Cannot sign a report that has already been completed.');\n    }\n\n    this.signed = true;\n  }\n\n  completeVerification() {\n    if (!this.signed) {\n      throw new Error(\n        'Cannot complete verification without signing the report.',\n      );\n    }\n\n    if (this.reportNumber \u003c 0) {\n      throw new Error('Report number cannot be negative.');\n    }\n\n    this.completed = true;\n  }\n}В этом примере модель ORM избавляется от анемичности, и код становится более структурированным и выразительным. К сожалению, вокруг Active Record существует много незаслуженной критики. Некоторые считают его антипаттерном, однако важно отметить, что в методах модели должна находиться только чистая бизнес-логика. Пожалуйста, избегайте обращения к базе данных в этих бизнес-методах — тогда ваш Active Record никогда не превратится в антипаттерн.Active Record является отличным компромиссом между доменной моделью (о которой мы поговорим позже) и Transaction Script. Этот паттерн хорошо подходит как для Supporting, так и для Generic поддоменов. Не пренебрегайте им!Domain modelDomain Model — это ключевой аспект тактического DDD. Этот шаблон хорошо подходит для множества Сore поддоменов, где критически важно обеспечить качество и скорость внесения изменений.EntityОсновой шаблона доменной модели является использование Entity с чистой бизнес-логикой. В отличие от Active Record, бизнес-логика здесь не размещается в ORM моделях, а инкапсулируется в отдельных чистых классах (сущностях). Добавляя поведение в сущности, мы превращаем модель из анемичной в полноценную, а уход от ORM слоя помогает избавиться от ненужных инфраструктурных зависимостей. Рассмотрим пример кода:export class CurierEntity {\n   id: string\n   name: string\n   orders: OrderEntity[]\n   \n   addOrder(newOrder) {\n       if (this.isActicve === true) {\n          if (this.rating \u003e 4) {\n\t\t         this.order.push(order)\n             const totalRating = this.rating * this.orders.length;\n\t\t\t       const updatedRating = (totalRating + 0.1) / (this.orders.length + 1);\n\t\t\t       this.rating = updatedRating;\n          }\n        }\n   }\n}\n\nexport class OrderEntity {\n   id: string\n   name: string\n   curier: CurierEntity;\n   \n   create(newOrder) {\n       ///\n   }\n}Сервисный слой при этом будет тонким, так как основная часть логики сосредоточена в сущностях. Мы извлекаем сущности из базы данных и сохраняем их целиком:export class CurierService {\n    async addOrder(id, order) {\n        curier = await this.repository.findById(id)\n        \n        curier.addOrder(new OrderEntity({...order}))\n        \n        await this.repository.save(curier)\n       \n    }\n}Репозиторий представлен ниже. Как видите, вся \"магия\" с ORM и маппингом на сущности происходит именно здесь:export class CurierRepository {\n    findById(curierId): CurierEntity {\n        const curierOrm = await this.prisma.cureir.findById(curierId)\n        return CurierMapper.mapToDomain(curierOrm)\n    }\n    save(curier: CurierEntity): CurierEntity {\n        const curierOrm = CurierMapper.mapToORM(curier)\n        const updatedCurier = await this.prisma.curier.save(curierOrm)\n        return curierMapper.mapToDomain(updatedCurier)\n    }\n}Используя доменные сущности, мы получаем множество преимуществ:Не зависим от хранения данных: Нам не важно, как данные хранятся в базе.Четкая ответственность: Ответственность за управление информацией лежит на тех, кто владеет всей необходимой информацией.Реляционное представление: Мы можем строить наши сущности в соответствии с реляционными принципами.Упрощенное тестирование: Меньше необходимости в моках, что делает тесты проще и надежнее.Отказ от database-driven development: Мы переходим к более умному моделированию, сфокусированному на бизнес-логике.Аккуратный сервисный слой: Получаем ясный и понятный сервисный слой, что упрощает сопровождение кода.АгрегатОдних Entity недостаточно. Хотя сущности отлично инкапсулируют бизнес-логику, возникает вопрос: как их связывать между собой? Как установить четкие модульные границы и обеспечить транзакционную согласованность?Если сущности неправильно объединены, бизнес-правила могут теряться. Рассмотрим наглядный пример. Представим, что из-за ограничения по лимиту мы не можем добавить новый заказ на склад.export class WarehouseEntity {\n  addOrder(order: OrderEntity) {\n    if (this.orders.length \u003e 500) {\n      throw new Error('Limit 500');\n    }\n    this.orders.push(order);\n  }\n}\n\nexport class Curier {\n  addOrder(curierId, newOrder) {\n    const curier = curierRepository.findById(warehouseId)\n\t\tcurier.addOrder(new OrderEntity(...newOrder))\n\t\treturn curierRepository.save(curier)\n  }\n}Этот код должен работать, но что если в другой части системы кто-то решил добавить заказ, минуя WarehouseEntity?export class OrdersService {\n  reorder(curierId, oldOrder) {\n    const order = new OrderEntity({...oldOrder, curierId})\n\treturn ordersRepository.save(order)\n  }\n}Вуаля! Такой баг сложно отловить — полагаться придется на тестирование или, что хуже, на отличную память коллег. В худшем случае функционал одной части системы может повредить другую. Люди должны постоянно помнить о всех проверках внутри сущностей и учитывать их при разработке новых функций. Чтобы чтобы избежать такой несогласованности, нам необходимы абстракции для управления границами сущностей. К счастью, такая абстракция существует.Агрегат — это иерархия сущностей, помогающая сохранить бизнес-правила и обеспечить транзакционную согласованность. Если выделен агрегат Courier, изменяйте его только через корень. Никаких манипуляций с Order напрямую — только через родителя.Преимущества аггрегатов:Легкость тестирования: В агрегате сосредоточена чистая бизнес-логика, что упрощает процесс тестирования.Простой интерфейс: Правильные агрегаты предоставляют четкий и ясный интерфейс, скрывая сложность под капотом.Целостность: Агрегат является цельным блоком, который мы можем извлечь, изменить и сохранить. Можно сказать, что агрегат — это основа вашего будущего модуля.Таким образом, использование агрегатов помогает избежать многих проблем, связанных с согласованностью и управлением бизнес-правилами, обеспечивая при этом четкость и структурированность кода.Агрегат и модульностьДовольно часто встречаются советы о том, что следует группировать сущности в агрегаты по принципу 1 к 1 (одна сущность — один агрегат). Аргументируют это тем, что сложно правильно разделить систему на агрегаты, поэтому проще сразу достичь максимальной гранулярности. Это вредно и неправильно; никогда так не делайте.Важно осознать ценность агрегатов. Как я упоминал ранее, агрегат может стать основой модульности, и само понятие агрегата во многом пересекается с модульностью. Агрегат — это самостоятельная единица, обладающая глубиной и относительной независимостью от внешних компонентов. Один агрегат \"владеет\" и управляет данными, которые находятся под его контролем. Ничто не дает вам права изменять эти данные каким-либо образом извне; только агрегат отвечает за это.Модуль, в свою очередь, также должен инкапсулировать определённый функционал таким образом, чтобы в будущем его можно было легко отделить и превратить в самостоятельную единицу развертывания. Важно достичь хорошей инкапсуляции внутри модуля, чтобы он был по-настоящему качественным и полезным. Я вижу много общего между понятиями агрегата и модуля!Пожалуйста, старайтесь проектировать агрегаты с учетом глубины и обеспечивать узкий и простой интерфейс для взаимодействия с ними. Это не только улучшит структуру вашего кода, но и облегчит его понимание и поддержку.Чересчур большие агрегатыПредставьте, что у нас есть курьеры, у которых есть заказы, в заказах содержатся товары, а в позициях — еще что-то. Цепочка может тянуться бесконечно.Извлекать из базы огромные массивы данных для выполнения небольших обновлений крайне неэффективно. Разделять агрегаты по одной сущности, как иногда советуют, — это больше похоже на инженерную катастрофу. Важно найти баланс.Для этого необходимо проанализировать бизнес-процессы, задавать вопросы экспертам и искать eventual consistency между сущностями, которые могут указать на слабую согласованность. Если строгая согласованность (ACID) не критична для операций между сущностями и таких взаимодействий не так уж много, это может указывать на слабую связность.Например, как часто вам нужно обновлять документацию по заказу при работе с курьерами? Скорее всего, это происходит не так часто. Так зачем же тянуть документы в агрегат курьера?Для таких редких взаимодействий лучше всего подойдет обмен сообщениями. При выполнении бизнес-операции просто добавляйте новое сообщение в массив messages внутри агрегата:crashOrder(orderId: string) {\n    const order = this.orders.find((el) =\u003e el.Id === orderId);\n    order.changeStatus(false);\n\n    this.messages.push(\n      new OrderCrashedEvent({\n        aggregateId: this.id,\n        payload: {\n          orderId: order.Id,\n        },\n      }),\n    );\n  }На стороне репозитория вы можете извлекать добавленные сообщения из сущности и отправлять их в брокер сообщений (или в базу данных, а затем в брокер, если используете паттерн transactional outbox):async saveCurier(curier: CurierEntity): Promise\u003cCurierEntity\u003e {\n    const curierORM = CurierMapper.mapToORM(curier);\n    const outboxORM = warehouse.pullMessages()\n    const crOrm = await this.dataSource.transaction(\n      async (transactionalEntityManager) =\u003e {\n        await transactionalEntityManager.save(outboxORM);\n        return await transactionalEntityManager.save(curierORM);\n      },\n    );\n    return CurierMapper.mapToDomain(crOrm);\n}Однако никто не запрещает вам реализовать обмен сообщениями другим способом. Универсальных решений не существует. Главное — понимать принципы и мотивы, стоящие за теми или иными решениями.Старайтесь избегать ситуаций, когда необходимо обновлять несколько агрегатов в одной ACID-транзакции. Если это становится частым случаем, пересмотрите границы агрегатов. Возможно, они у вас проведены неправильно.ТриллемаDDD TrilemmaВ использовании шаблона с доменной моделью мы неизбежно сталкиваемся с триллемой, которая заключается в том, что невозможно одновременно удовлетворить три ключевых атрибута: полноту модели домена, чистоту модели домена и производительность. Приходится выбирать два из трех.Рассмотрим задачу смены номера телефона у пользователя с предварительной проверкой его уникальности. Как грамотно ее решить?Представим ситуацию, в которой нам необходимо сменить пользователю номер телефона, предварительно убедившись в его уникальности. Как решить эту задачу грамотно? Сохранение полноты и чистоты модели домена, жертвуя производительностью. В этом случае мы можем реализовать проверку уникальности номера непосредственно в модели данных. Однако такой подход требует выгрузки всех пользователей для проверки, что негативно скажется на производительности системы.export class UserService {\n    \n    async changeEmail(id, email) {\n        user = await this.repository.findById(id)\n        allUsers = await this.repository.findAll()\n        \n        user.changeEmail(email, allUsers)\n       \n        await this.repository.save(user)\n       \n    }\n}\n\nexport class User {\n\tchangeEmail(email, allUsers) {\n\t  const userWithEmail = allUsers.find(u =\u003e u.email === email);\n\t  if (userWithEmail) {\n\t    throw\n\t  }\n\t  this.email = email;\n\t}\n}Сохранение производительности и полноты модели, но за счет чистоты. Можно внедрить инфраструктурную зависимость прямо в модель для проверки уникальности номера телефона. Хотя этот вариант может обеспечить хорошую производительность, он нарушает чистоту модели, смешивая бизнес-логику с инфраструктурными деталями. В результате код может выглядеть заманчиво, но его сложнее поддерживать и развивать.export class UserService {\n    \n    async changeEmail(id, email) {\n        user = await this.repository.findById(id)\n   \n        user.changeEmail(email, this.repository)\n       \n        await this.repository.save(user)\n       \n    }\n}\n\nexport class User {\n\n\tasync changeEmail(email, repository) {\n\t  const userWithEmail = await repository.findByEmail(email);\n\t  if (userWithEmail) {\n\t    throw\n\t  }\n\t  this.email = email;\n\t}\n}Сохранение производительности и чистоты модели, но с жертвой полноты. В этом варианте мы разделяем процесс принятия решений между уровнем домена и сервисом. Бизнес-логика проверки уникальности номера реализуется на уровне сервиса, что позволяет сохранить чистоту модели и высокую производительность. Этот подход требует четкого определения точек принятия решений и взаимодействия между слоями, но, как правило, является наиболее оптимальным для большинства приложений.export class UserService {\n    \n    async changeEmail(id, email) {\n        userWithEmail = await this.repository.findByEmail(email);\n        if (userWithEmail) {\n         throw\n        }\n        user = await this.repository.findById(id)\n        \n        user.changeEmail(email)\n       \n        await this.repository.save(user)\n       \n    }\n}\n\nexport class User {\n\tchangeEmail(email) {\n\t  this.email = email;\n\t}\n}Таким образом, в реальных проектах приходится искать баланс между этими тремя атрибутами. Выбор подхода зависит от приоритетов системы и требований к архитектуре.Value ObjectsВ Domain-Driven Design (DDD) Value Objects представляют собой концепцию, которая добавляет ценность за счёт акцента на сущностных характеристиках объекта, а не на его уникальной идентичности. Эти объекты не имеют идентификаторов, но могут инкапсулировать данные и поведение, связанное с ними. Value Objects неизменяемы и определяются исключительно своими атрибутами, что делает их идеальными для моделирования понятий, таких как деньги, даты или адреса.export class AmountObjectValue {\n  public amount: number;\n  public rate: number;\n  constructor(attributes: Attributes) {\n    this.amount = attributes.amount;\n    this.rate = attributes.rate;\n  }\n\n  applyDiscount(discount: number): number {\n    return this.amount * discount;\n  }\n\n  getAmoutWithoutTax(): number {\n    return this.amount * (100 - this.rate);\n  }\n\n  differenceAfterTax(): number {\n    return this.amount - this.getAmoutWithoutTax();\n  }\n}Чаще используйте Value Objects, особенно когда в них можно скрыть значительную бизнес-логику, обеспечить необходимую инкапсуляцию и снизить избыточную сложность в Entity.Read ModelПри разработке агрегатов следует помнить, что они необходимы только для операций изменения данных. Если требуется только чтение без изменений, лучше использовать паттерн Read Model.export class ReportReadModel {\n  readonly id: string;\n  readonly isValid: boolean;\n  readonly orderId: string;\n  readonly reportNumber: number;\n  readonly positions: ReportPositionReadModel[];\n\n  constructor(attributes) {\n    this.id = attributes.id;\n    this.isValid = attributes.isValid;\n    this.orderId = attributes.orderId;\n    this.reportNumber = attributes.reportNumber;\n    this.positions = attributes.positions;\n  }\n}У вас может быть множество моделей чтения для различных сценариев — не бойтесь создавать их по необходимости. Главное правило: не используйте их для внесения изменений, так как за все изменения отвечают агрегаты.Вполне вероятно (и, скорее всего, так и будет), что модель чтения будет содержать данные из разных модулей. Это не проблема, поскольку она используется исключительно для чтения. Когда мы говорим о модульности и границах модуля, ключевое внимание уделяется именно операциям изменения данных.Некоторый итогИтак, мы прошлись по основным шаблонам применили тактические паттерны, целевая картинка нашей системы выглядит так.Для контекста Warehouse:OrderManagement(Core)  - Domain ModelLocation(Supporting) - Transaction scriptКонтекст Accounting состоит из:Reports(Core) - Domain ModelVerification(Supporting) - Active recordКонтекст Delivery представлен тремя контекстами:Core.Board(Core) - Domain ModelCore.Couriers(Core) - Domain ModelSupporting.Tracking(Supporting) - Transaction scriptВесь код нашего вымышленного домена можете посмотреть на github (Typescript, Golang). Не забывайте про стратегические паттерны, побольше внимания уделите прежде всего им. Используйте тактические паттерны там, где они действительно помогут, а не добавят лишней головной боли.",
  "Топ самых интересных CVE за октябрь 2024 года": "⚠ Внимание ⚠Вся информация, представленная ниже, предназначена только для ознакомительных целей. Автор не несет ответственности за вред, который может быть причинен с помощью предоставленной им информации.В этой подборке представлены самые интересные уязвимости за октябрь 2024 года.Подведем вместе итоги этого осеннего месяца, поехали!🟣 RCE-уязвимость в Zimbra▶ CVE-2024-45519Об уязвимости:В почтовом сервере Zimbra Collaboration (ZCS) обнаружена RCE-уязвимость, связанная с некорректной обработкой службой Zimbra postjournal пользовательского ввода в поле CC входящих писем.Опубликована проверка концепции, которая доступна по ссылке.Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, выполнять произвольный код на уязвимом сервере Zimbra, отправляя на целевое устройство специально подготовленные письма с командами для выполнения в поле CC.Исправление:Всем пользователям рекомендуется как можно скорее обновить продукты до исправленных версий 9.0.0 Patch 41, 10.0.9, 10.1.1 и версии 8.8.15 Patch 46 (подробнее).Оценка уязвимости по шкале CVSS 3.1 — 10.0 баллов.Подробнее:CVE-2024-45519🟣 Доступ к виртуальным машинам в Kubernetes Image Builder▶ CVE-2024-9486Об уязвимости:В Kubernetes Image Builder обнаружена уязвимость, возникающая по причине использования учетных данных по умолчанию, которые были включены в процессе сборки образа виртуальной машины (ВМ), созданной с помощью провайдера Proxmox. Затронуты все версии до 0.1.37 включительно.Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, используя SSH-соединение, получить несанкционированный доступ к ВМ с запущенными образами с привилегиями суперпользователя, тем самым получить полный контроль над уязвимой ВМ.Исправление:Всем пользователям рекомендуется как можно скорее обновить продукт до исправленной версии 0.1.38 (подробнее).Оценка уязвимости по шкале CVSS 3.1 — 9.8 баллов.Подробнее:CVE-2024-9486🟣 Отсутствие аутентификации в FortiManager ▶ CVE-2024-47575Об уязвимости:В системе централизованного управления устройствами FortiManager обнаружена уязвимость, возникающая по причине отсутствия аутентификации в API FGFM, отвечающего за связь между устройствами FortiManager и FortiGate.Затронуты следующие версии:FortiManager 7.6 (версии до 7.6.1)FortiManager 7.4 (версии с 7.4.0 по 7.4.4)FortiManager 7.2 (версии с 7.2.0 по 7.2.7)FortiManager 7.0 (версии с 7.0.0 по 7.0.12)FortiManager 6.4 (версии с 6.4.0 по 6.4.14)FortiManager 6.2 (версии с 6.2.0 по 6.2.12)Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, выполнять произвольные команды в системе, а также выполнять произвольный код на уязвимом устройстве, используя специально подготовленные запросы.Исправление:Всем пользователям рекомендуется как можно скорее обновить продукт до исправленной версии (подробнее).Оценка уязвимости по шкале CVSS 3.1 — 9.8 баллов.Подробнее:CVE-2024-47575🟣 Компрометация кода в GitLab EE▶ CVE-2024-9164Об уязвимости:В GitLab Enterprise Edition (EE) обнаружена уязвимость, связанная с небезопасным запуском pipeline jobs на произвольных ветках репозиториев. Затронуты версии продукта с 12.15 по 17.2.9, с 17.3 по 17.3.5, с 17.4 по 17.4.2.Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, запускать pipeline на произвольных ветках репозиториев, потенциально получая несанкционированный доступ к конфиденциальным данным и системам, тем самым позволяя скомпрометировать код.Исправление:Всем пользователям рекомендуется как можно скорее провести обновление системы до исправленных версий.Оценка уязвимости по шкале CVSS 3.1 — 9.6 баллов.Подробнее:CVE-2024-9164🟣 Выполнение команд ОС в Linear eMerge E3-Series▶ CVE-2024-9441Об уязвимости:В системах управления доступом Linear eMerge E3-Series обнаружена уязвимость внедрения команд ОС, связанная с небезопасным использованием параметра login_id в функции восстановления пароля.Затронутые версии Linear eMerge E3-Series Access Control:0.32-03i0.32-04m0.32-05p0.32-05z0.32-07p0.32-07e0.32-08e0.32-08f0.32-09c1.00.051.00.07Опубликована проверка концепции, которая доступна по ссылке.Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, выполнять произвольные команды ОС на уязвимом устройстве.  Исправление:Компания пока не выпустила исправлений для данной уязвимости, поэтому всем пользователям рекомендуется отключить устройства, использующие серию Linear Emerge E3, или изолировать их.Оценка уязвимости по шкале CVSS 3.1 — 9.8 баллов.Подробнее:CVE-2024-9441🟣 Критическая уязвимость в Apache Avro▶ CVE-2024-47561Об уязвимости:В фреймворке для сериализации данных Apache Avro обнаружена уязвимость, возникающая по причине ошибки в функции синтаксического анализа схемы Java SDK, что позволяет создавать данные Avro, которые при синтаксическом анализе уязвимой системой запускают выполнение произвольного кода. Затронуты все версии продукта до 1.11.4 версии.Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, выполнять произвольный код в целевых системах.Исправление:Всем пользователям рекомендуется как можно скорее обновить ПО продукта до исправленных версий 1.11.4 и 1.12.0 (подробнее).Оценка уязвимости по шкале CVSS 3.1 — 9.2 балла.Подробнее:CVE-2024-47561🟣 Выполнение произвольного кода в Firefox▶ CVE-2024-9680Об уязвимости:В браузере Firefox обнаружена уязвимость типа Use-After-Free, связанная с некорректной работой механизма управления анимацией на веб-страницах.Затронуты следующие версии Firefox:Firefox до 131.0.2 версииFirefox ESR до 128.3.1 версииFirefox ESR до 115.16.1 версииЭксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, выполнять произвольный код в процессе обработки контента.Исправление:Всем пользователям рекомендуется как можно скорее обновить браузер до исправленной версии Firefox 131.0.2, Firefox ESR 115.16.1 и Firefox ESR 128.3.1.Оценка уязвимости по шкале CVSS 3.1 — 9.8 баллов.Подробнее:CVE-2024-9680🟣 Выполнение произвольного кода в Trend Micro Cloud Edge▶ CVE-2024-48904Об уязвимости:В Trend Micro Cloud Edge обнаружена RCE-уязвимость, возникающая в REST API, который по умолчанию прослушивает TCP-порт 8443, по причине отсутствия надлежащей проверки вводимой пользователем строки перед её использованием для выполнения системного вызова. Затронут Cloud Edge версий 5.6SP2 и 7.0.  Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, выполнять произвольный код на уязвимом устройстве.Исправление:Всем пользователям рекомендуется как можно скорее обновить свои устройства до исправленной версии (подробнее).Оценка уязвимости по шкале CVSS 3.1 — 9.8 баллов.Подробнее:CVE-2024-48904🟣 Исправленная уязвимость нулевого дня в драйвере Samsung▶ CVE-2024-44068Об уязвимости:В мобильных процессорах Samsung в драйвере m2m1shot_scaler0 устройств на базе Exynos обнаружена уязвимость типа Use-After-Free, связанная с некорректной работой драйвера при освобождении участков памяти, что позволяет впоследствии повторно их использовать.Затронуты следующие серии процессоров:Exynos 9820Exynos 9825Exynos 980Exynos 990Exynos 850Exynos W920Эксплуатация:Уязвимость позволяет злоумышленнику захватывать освобожденные области памяти и выполнять вредоносный код с привилегиями root.Исправление:Всем пользователям рекомендуется как можно скорее установить обновление SMR-Oct-2024 на свои устройства (подробнее).Оценка уязвимости по шкале CVSS 3.1 — 8.1 балла.Подробнее:CVE-2024-44068🟣 Многочисленные уязвимости в MicrosoftВ традиционный Patch Tuesday компанией Microsoft было исправлено 118 уязвимостей, из которых две уязвимости использовались в реальных атаках, а три RCE-уязвимости оценены как критические. Всем пользователям остается как можно скорее провести обновление своих продуктов до исправленных версий.▶ Исправленные эксплуатируемые уязвимостиCVE-2024-43572 — RCE-уязвимость, возникающая в консоле управления Microsoft Management Console (MMC) по причине некорректной обработки входных данных. Успех эксплуатации зависит от пользователя, который должен загрузить и открыть вредоносный MSC-файл, и в таком случае это приводит к удаленному выполнению кода от имени локального пользователя на уязвимом устройстве.Оценка уязвимости по шкале CVSS 3.1 — 7.8 баллов.CVE-2024-43573 — уязвимость, возникающая в платформе Windows MSHTML по причине недостаточных мер безопасности структуры веб-страницы, позволяет осуществлять спуфинг-атаки.Оценка уязвимости по шкале CVSS 3.1 — 6.5 баллов.▶ Исправленные критические уязвимостиCVE-2024-43468 — RCE-уязвимость, возникающая в Microsoft Configuration Manager по причине небезопасной обработки входных данных. Успех эксплуатации позволяет удаленному злоумышленнику выполнять произвольные команды на сервере, используя специально сформированные запросы.Оценка уязвимости по шкале CVSS 3.1 — 9.8 баллов.CVE-2024-43488 — RCE-уязвимость, возникающая в расширении Visual Studio Code для Arduino по причине отсутствия аутентификации для критичной функции. Успех эксплуатации позволяет удаленному злоумышленнику выполнять произвольный код.Оценка уязвимости по шкале CVSS 3.1 — 9.8 баллов.CVE-2024-43582 — RCE-уязвимость, возникающая в протоколе удаленного рабочего стола (RDP) и связанная с использованием памяти после ее освобождения. Успех эксплуатации позволяет удаленному злоумышленнику при отправке специально сформированных запросов на сервер выполнять произвольный код.Оценка уязвимости по шкале CVSS 3.1 — 8.1 балла.🟣 Исправленная 0-day уязвимость в чипах Qualcomm▶ CVE-2024-43047Об уязвимости:В процессоре Digital Signal Processor (DSP) компании Qualcomm обнаружена уязвимость типа Use-After-Free, используемая в реальных атаках.Опубликована проверка концепции, которая доступна по ссылке.Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, прошедшему проверку подлинности и обладающему низкими привилегиями, при определенных условиях использовать ошибку повреждения памяти для последующих атак.Исправление:Компанией Qualcomm был выпущен патч для исправления данной уязвимости, а всем пользователям остается только ожидать обновлений на своих конечных устройствах (подробнее).Оценка уязвимости по шкале CVSS 3.1 — 7.8 баллов.Подробнее:CVE-2024-43047🟣 Уязвимости в плагинах для WordPress▶ CVE-2024-47374Об уязвимости:В плагине LiteSpeed Cache для WordPress обнаружена уязвимость межсайтового скриптинга (XSS), возникающая по причине небезопасной обработки значений HTTP-заголовка X-LSCACHE-VARY-VALUE. Затронуты все версии плагина до 6.5.0.2 включительно.Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, не прошедшему проверку подлинности, при определенных условиях получить несанкционированный доступ к конфиденциальным данным, а также, выполнив один HTTP-запрос, повысить свои привилегии в WordPress.Исправление:Всем пользователям рекомендуется как можно скорее обновить плагин до исправленной версии 6.5.1.Оценка уязвимости по шкале CVSS 3.1 — 7.2 балла.Подробнее:CVE-2024-47374▶ Доступ к формам других пользователей в плагине JetpackОб уязвимости:В плагине Jetpack для WordPress обнаружена уязвимость, возникающая в функции Contact Form. Затронуты все версии плагина до 13.9.1.Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, прошедшему проверку подлинности, читать формы, отправленные другими посетителями сайта.Исправление:Всем пользователям рекомендуется как можно скорее обновить плагин до исправленной версии 13.9.1 (подробнее).🟣 Внедрение команд ОС в принтерах Xerox ▶ CVE-2024-6333Об уязвимости:В принтерах Xerox обнаружена уязвимость внедрения команд ОС, возникающая по причине некорректной проверки вводимых значений IPv4 при использовании инструмента tcpdump.Затронуты следующие модели принтеров:EC80xxAltaLinkVersaLinkWorkCentreОпубликована проверка концепции, которая доступна по ссылке.Эксплуатация:Уязвимость позволяет злоумышленнику, обладающему привилегиями администратора,  получить доступ к операционной системе принтера через веб-интерфейс и выполнять произвольные команды с привилегиями суперпользователя.Исправление:Всем пользователям рекомендуется как можно скорее обновить прошивку своих устройств (подробнее).Оценка уязвимости по шкале CVSS 3.1 — 7.2 балла.Подробнее:CVE-2024-6333🟣 Уязвимость в OATH Toolkit▶ CVE-2024-47191Об уязвимости:В OATH Toolkit, ПО для аутентификации по одноразовому паролю (OTP), обнаружена уязвимость локального повышения привилегий, возникающая по причине небезопасной обработки файлов в домашнем каталоге пользователей при использовании PAM-модуля pam_oath . Затронуто программное обеспечение версий от 2.6.7 до 2.6.11.Эксплуатация:Уязвимость позволяет удаленному злоумышленнику, прошедшему проверку подлинности, повышать свои привилегии до root.Исправление:Всем пользователям рекомендуется как можно скорее обновить ПО до исправленной версии 2.6.12.Оценка уязвимости по шкале CVSS 3.1 — 7.1 балла.Подробнее:CVE-2024-47191"
}